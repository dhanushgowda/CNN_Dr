Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
2016-10-15 01:34:18.610808: step 0, loss = 4.67 (5.9 examples/sec; 21.874 sec/batch)
2016-10-15 01:34:21.054785: step 10, loss = 4.61 (751.1 examples/sec; 0.170 sec/batch)
2016-10-15 01:34:22.835321: step 20, loss = 4.57 (720.8 examples/sec; 0.178 sec/batch)
2016-10-15 01:34:24.621741: step 30, loss = 4.37 (736.9 examples/sec; 0.174 sec/batch)
2016-10-15 01:34:26.390990: step 40, loss = 4.32 (727.2 examples/sec; 0.176 sec/batch)
2016-10-15 01:34:28.168118: step 50, loss = 4.22 (740.5 examples/sec; 0.173 sec/batch)
2016-10-15 01:34:29.952607: step 60, loss = 4.19 (745.6 examples/sec; 0.172 sec/batch)
2016-10-15 01:34:31.728494: step 70, loss = 4.41 (716.2 examples/sec; 0.179 sec/batch)
2016-10-15 01:34:33.504632: step 80, loss = 4.27 (706.1 examples/sec; 0.181 sec/batch)
2016-10-15 01:34:35.278946: step 90, loss = 4.20 (729.7 examples/sec; 0.175 sec/batch)
2016-10-15 01:34:37.058628: step 100, loss = 4.12 (742.5 examples/sec; 0.172 sec/batch)
2016-10-15 01:34:39.012238: step 110, loss = 4.13 (713.8 examples/sec; 0.179 sec/batch)
2016-10-15 01:34:40.788833: step 120, loss = 4.04 (711.2 examples/sec; 0.180 sec/batch)
2016-10-15 01:34:42.556765: step 130, loss = 3.88 (726.8 examples/sec; 0.176 sec/batch)
2016-10-15 01:34:44.335482: step 140, loss = 4.08 (723.2 examples/sec; 0.177 sec/batch)
2016-10-15 01:34:46.117547: step 150, loss = 3.95 (707.4 examples/sec; 0.181 sec/batch)
2016-10-15 01:34:47.894651: step 160, loss = 4.14 (721.6 examples/sec; 0.177 sec/batch)
2016-10-15 01:34:49.679959: step 170, loss = 4.04 (699.6 examples/sec; 0.183 sec/batch)
2016-10-15 01:34:51.450792: step 180, loss = 3.87 (738.0 examples/sec; 0.173 sec/batch)
2016-10-15 01:34:53.239915: step 190, loss = 3.87 (724.5 examples/sec; 0.177 sec/batch)
2016-10-15 01:34:55.024436: step 200, loss = 3.86 (725.1 examples/sec; 0.177 sec/batch)
2016-10-15 01:34:56.979065: step 210, loss = 3.83 (717.2 examples/sec; 0.178 sec/batch)
2016-10-15 01:34:58.751108: step 220, loss = 3.75 (720.5 examples/sec; 0.178 sec/batch)
2016-10-15 01:35:00.528856: step 230, loss = 3.83 (729.9 examples/sec; 0.175 sec/batch)
2016-10-15 01:35:02.328010: step 240, loss = 3.70 (716.1 examples/sec; 0.179 sec/batch)
2016-10-15 01:35:04.107271: step 250, loss = 3.69 (726.1 examples/sec; 0.176 sec/batch)
2016-10-15 01:35:05.875852: step 260, loss = 3.69 (720.4 examples/sec; 0.178 sec/batch)
2016-10-15 01:35:07.648432: step 270, loss = 3.58 (723.8 examples/sec; 0.177 sec/batch)
2016-10-15 01:35:09.425679: step 280, loss = 3.57 (727.1 examples/sec; 0.176 sec/batch)
2016-10-15 01:35:11.192631: step 290, loss = 3.57 (740.8 examples/sec; 0.173 sec/batch)
2016-10-15 01:35:12.977152: step 300, loss = 3.58 (695.5 examples/sec; 0.184 sec/batch)
2016-10-15 01:35:14.929992: step 310, loss = 3.65 (744.5 examples/sec; 0.172 sec/batch)
2016-10-15 01:35:16.703616: step 320, loss = 3.63 (700.0 examples/sec; 0.183 sec/batch)
2016-10-15 01:35:18.475760: step 330, loss = 3.50 (724.6 examples/sec; 0.177 sec/batch)
2016-10-15 01:35:20.244446: step 340, loss = 3.41 (726.9 examples/sec; 0.176 sec/batch)
2016-10-15 01:35:22.019312: step 350, loss = 3.48 (737.1 examples/sec; 0.174 sec/batch)
2016-10-15 01:35:23.793526: step 360, loss = 3.64 (736.7 examples/sec; 0.174 sec/batch)
2016-10-15 01:35:25.575495: step 370, loss = 3.31 (713.9 examples/sec; 0.179 sec/batch)
2016-10-15 01:35:27.347404: step 380, loss = 3.46 (718.2 examples/sec; 0.178 sec/batch)
2016-10-15 01:35:29.136666: step 390, loss = 3.63 (688.9 examples/sec; 0.186 sec/batch)
2016-10-15 01:35:30.916339: step 400, loss = 3.83 (661.2 examples/sec; 0.194 sec/batch)
2016-10-15 01:35:32.870842: step 410, loss = 3.47 (714.9 examples/sec; 0.179 sec/batch)
2016-10-15 01:35:34.643254: step 420, loss = 3.30 (707.2 examples/sec; 0.181 sec/batch)
2016-10-15 01:35:36.412799: step 430, loss = 3.34 (726.9 examples/sec; 0.176 sec/batch)
2016-10-15 01:35:38.193041: step 440, loss = 3.36 (709.2 examples/sec; 0.180 sec/batch)
2016-10-15 01:35:39.967921: step 450, loss = 3.37 (722.1 examples/sec; 0.177 sec/batch)
2016-10-15 01:35:41.748794: step 460, loss = 3.08 (734.5 examples/sec; 0.174 sec/batch)
2016-10-15 01:35:43.522034: step 470, loss = 3.40 (730.4 examples/sec; 0.175 sec/batch)
2016-10-15 01:35:45.292209: step 480, loss = 3.24 (753.9 examples/sec; 0.170 sec/batch)
2016-10-15 01:35:47.067831: step 490, loss = 3.13 (746.0 examples/sec; 0.172 sec/batch)
2016-10-15 01:35:48.851015: step 500, loss = 3.14 (710.2 examples/sec; 0.180 sec/batch)
2016-10-15 01:35:50.805144: step 510, loss = 3.18 (713.7 examples/sec; 0.179 sec/batch)
2016-10-15 01:35:52.581229: step 520, loss = 3.20 (726.8 examples/sec; 0.176 sec/batch)
2016-10-15 01:35:54.352386: step 530, loss = 3.02 (742.4 examples/sec; 0.172 sec/batch)
2016-10-15 01:35:56.131355: step 540, loss = 3.04 (726.3 examples/sec; 0.176 sec/batch)
2016-10-15 01:35:57.910569: step 550, loss = 3.16 (733.9 examples/sec; 0.174 sec/batch)
2016-10-15 01:35:59.683372: step 560, loss = 3.01 (714.5 examples/sec; 0.179 sec/batch)
2016-10-15 01:36:01.449857: step 570, loss = 3.16 (727.8 examples/sec; 0.176 sec/batch)
2016-10-15 01:36:03.226657: step 580, loss = 2.95 (716.9 examples/sec; 0.179 sec/batch)
2016-10-15 01:36:04.997456: step 590, loss = 3.15 (734.1 examples/sec; 0.174 sec/batch)
2016-10-15 01:36:06.775310: step 600, loss = 2.92 (714.2 examples/sec; 0.179 sec/batch)
2016-10-15 01:36:08.735298: step 610, loss = 2.88 (706.2 examples/sec; 0.181 sec/batch)
2016-10-15 01:36:10.507166: step 620, loss = 2.82 (715.3 examples/sec; 0.179 sec/batch)
2016-10-15 01:36:12.282170: step 630, loss = 3.05 (713.2 examples/sec; 0.179 sec/batch)
2016-10-15 01:36:14.051409: step 640, loss = 3.06 (732.6 examples/sec; 0.175 sec/batch)
2016-10-15 01:36:15.832671: step 650, loss = 2.95 (696.6 examples/sec; 0.184 sec/batch)
2016-10-15 01:36:17.605225: step 660, loss = 3.01 (730.4 examples/sec; 0.175 sec/batch)
2016-10-15 01:36:19.387620: step 670, loss = 2.86 (684.5 examples/sec; 0.187 sec/batch)
2016-10-15 01:36:21.161506: step 680, loss = 2.84 (715.5 examples/sec; 0.179 sec/batch)
2016-10-15 01:36:22.933319: step 690, loss = 3.11 (748.3 examples/sec; 0.171 sec/batch)
2016-10-15 01:36:24.706780: step 700, loss = 2.90 (736.2 examples/sec; 0.174 sec/batch)
2016-10-15 01:36:26.661915: step 710, loss = 2.95 (725.5 examples/sec; 0.176 sec/batch)
2016-10-15 01:36:28.433041: step 720, loss = 3.01 (729.6 examples/sec; 0.175 sec/batch)
2016-10-15 01:36:30.207692: step 730, loss = 2.71 (716.9 examples/sec; 0.179 sec/batch)
2016-10-15 01:36:31.988240: step 740, loss = 2.69 (690.8 examples/sec; 0.185 sec/batch)
2016-10-15 01:36:33.752642: step 750, loss = 2.80 (732.1 examples/sec; 0.175 sec/batch)
2016-10-15 01:36:35.535272: step 760, loss = 2.76 (730.0 examples/sec; 0.175 sec/batch)
2016-10-15 01:36:37.307126: step 770, loss = 2.74 (720.3 examples/sec; 0.178 sec/batch)
2016-10-15 01:36:39.077367: step 780, loss = 2.80 (721.7 examples/sec; 0.177 sec/batch)
2016-10-15 01:36:40.847842: step 790, loss = 2.65 (743.9 examples/sec; 0.172 sec/batch)
2016-10-15 01:36:42.625323: step 800, loss = 2.66 (727.8 examples/sec; 0.176 sec/batch)
2016-10-15 01:36:44.588340: step 810, loss = 2.83 (714.8 examples/sec; 0.179 sec/batch)
2016-10-15 01:36:46.361873: step 820, loss = 2.73 (725.1 examples/sec; 0.177 sec/batch)
2016-10-15 01:36:48.141755: step 830, loss = 2.62 (697.0 examples/sec; 0.184 sec/batch)
2016-10-15 01:36:49.905283: step 840, loss = 2.73 (710.9 examples/sec; 0.180 sec/batch)
2016-10-15 01:36:51.674769: step 850, loss = 2.67 (714.8 examples/sec; 0.179 sec/batch)
2016-10-15 01:36:53.447876: step 860, loss = 2.57 (722.2 examples/sec; 0.177 sec/batch)
2016-10-15 01:36:55.222687: step 870, loss = 2.58 (723.7 examples/sec; 0.177 sec/batch)
2016-10-15 01:36:56.994331: step 880, loss = 2.55 (714.9 examples/sec; 0.179 sec/batch)
2016-10-15 01:36:58.770679: step 890, loss = 2.51 (720.2 examples/sec; 0.178 sec/batch)
2016-10-15 01:37:00.543163: step 900, loss = 2.57 (730.2 examples/sec; 0.175 sec/batch)
2016-10-15 01:37:02.509109: step 910, loss = 2.66 (724.9 examples/sec; 0.177 sec/batch)
2016-10-15 01:37:04.282024: step 920, loss = 2.55 (730.1 examples/sec; 0.175 sec/batch)
2016-10-15 01:37:06.062391: step 930, loss = 2.62 (712.8 examples/sec; 0.180 sec/batch)
2016-10-15 01:37:07.837429: step 940, loss = 2.40 (709.0 examples/sec; 0.181 sec/batch)
2016-10-15 01:37:09.622950: step 950, loss = 2.72 (712.2 examples/sec; 0.180 sec/batch)
2016-10-15 01:37:11.404211: step 960, loss = 2.24 (707.2 examples/sec; 0.181 sec/batch)
2016-10-15 01:37:13.188128: step 970, loss = 2.54 (710.1 examples/sec; 0.180 sec/batch)
2016-10-15 01:37:14.968851: step 980, loss = 2.49 (723.2 examples/sec; 0.177 sec/batch)
2016-10-15 01:37:16.742682: step 990, loss = 2.54 (739.3 examples/sec; 0.173 sec/batch)
2016-10-15 01:37:18.514493: step 1000, loss = 2.45 (756.7 examples/sec; 0.169 sec/batch)
2016-10-15 01:37:20.982482: step 1010, loss = 2.35 (719.6 examples/sec; 0.178 sec/batch)
2016-10-15 01:37:22.760090: step 1020, loss = 2.27 (736.7 examples/sec; 0.174 sec/batch)
2016-10-15 01:37:24.544508: step 1030, loss = 2.32 (713.4 examples/sec; 0.179 sec/batch)
2016-10-15 01:37:26.318286: step 1040, loss = 2.54 (716.2 examples/sec; 0.179 sec/batch)
2016-10-15 01:37:28.097345: step 1050, loss = 2.62 (701.4 examples/sec; 0.182 sec/batch)
2016-10-15 01:37:29.882366: step 1060, loss = 2.43 (729.7 examples/sec; 0.175 sec/batch)
2016-10-15 01:37:31.659794: step 1070, loss = 2.18 (725.0 examples/sec; 0.177 sec/batch)
2016-10-15 01:37:33.438727: step 1080, loss = 2.23 (704.6 examples/sec; 0.182 sec/batch)
2016-10-15 01:37:35.227681: step 1090, loss = 2.32 (719.5 examples/sec; 0.178 sec/batch)
2016-10-15 01:37:37.010434: step 1100, loss = 2.31 (711.0 examples/sec; 0.180 sec/batch)
2016-10-15 01:37:38.986698: step 1110, loss = 2.36 (750.2 examples/sec; 0.171 sec/batch)
2016-10-15 01:37:40.768080: step 1120, loss = 2.19 (720.7 examples/sec; 0.178 sec/batch)
2016-10-15 01:37:42.543259: step 1130, loss = 2.33 (728.2 examples/sec; 0.176 sec/batch)
2016-10-15 01:37:44.328633: step 1140, loss = 2.45 (699.4 examples/sec; 0.183 sec/batch)
2016-10-15 01:37:46.101486: step 1150, loss = 2.27 (717.8 examples/sec; 0.178 sec/batch)
2016-10-15 01:37:47.870937: step 1160, loss = 2.38 (720.2 examples/sec; 0.178 sec/batch)
2016-10-15 01:37:49.653930: step 1170, loss = 2.29 (710.9 examples/sec; 0.180 sec/batch)
2016-10-15 01:37:51.425627: step 1180, loss = 2.22 (710.4 examples/sec; 0.180 sec/batch)
2016-10-15 01:37:53.207640: step 1190, loss = 2.16 (694.0 examples/sec; 0.184 sec/batch)
2016-10-15 01:37:54.980122: step 1200, loss = 2.36 (722.3 examples/sec; 0.177 sec/batch)
2016-10-15 01:37:56.934850: step 1210, loss = 2.17 (727.2 examples/sec; 0.176 sec/batch)
2016-10-15 01:37:58.707857: step 1220, loss = 2.27 (760.2 examples/sec; 0.168 sec/batch)
2016-10-15 01:38:00.470324: step 1230, loss = 2.42 (768.1 examples/sec; 0.167 sec/batch)
2016-10-15 01:38:02.249806: step 1240, loss = 2.23 (701.4 examples/sec; 0.183 sec/batch)
2016-10-15 01:38:04.022795: step 1250, loss = 2.21 (735.4 examples/sec; 0.174 sec/batch)
2016-10-15 01:38:05.805150: step 1260, loss = 2.07 (709.0 examples/sec; 0.181 sec/batch)
2016-10-15 01:38:07.572150: step 1270, loss = 2.20 (733.7 examples/sec; 0.174 sec/batch)
2016-10-15 01:38:09.343902: step 1280, loss = 2.04 (717.9 examples/sec; 0.178 sec/batch)
2016-10-15 01:38:11.111189: step 1290, loss = 2.07 (725.0 examples/sec; 0.177 sec/batch)
2016-10-15 01:38:12.879480: step 1300, loss = 2.00 (722.4 examples/sec; 0.177 sec/batch)
2016-10-15 01:38:14.834057: step 1310, loss = 2.19 (719.9 examples/sec; 0.178 sec/batch)
2016-10-15 01:38:16.609116: step 1320, loss = 1.98 (714.8 examples/sec; 0.179 sec/batch)
2016-10-15 01:38:18.369777: step 1330, loss = 2.26 (733.8 examples/sec; 0.174 sec/batch)
2016-10-15 01:38:20.145248: step 1340, loss = 1.93 (748.6 examples/sec; 0.171 sec/batch)
2016-10-15 01:38:21.915378: step 1350, loss = 2.14 (724.2 examples/sec; 0.177 sec/batch)
2016-10-15 01:38:23.686794: step 1360, loss = 2.07 (710.5 examples/sec; 0.180 sec/batch)
2016-10-15 01:38:25.468090: step 1370, loss = 2.19 (738.7 examples/sec; 0.173 sec/batch)
2016-10-15 01:38:27.256427: step 1380, loss = 2.13 (691.4 examples/sec; 0.185 sec/batch)
2016-10-15 01:38:29.017945: step 1390, loss = 2.21 (742.0 examples/sec; 0.173 sec/batch)
2016-10-15 01:38:30.786809: step 1400, loss = 1.91 (731.6 examples/sec; 0.175 sec/batch)
2016-10-15 01:38:32.742784: step 1410, loss = 2.11 (739.3 examples/sec; 0.173 sec/batch)
2016-10-15 01:38:34.509184: step 1420, loss = 2.11 (717.6 examples/sec; 0.178 sec/batch)
2016-10-15 01:38:36.290961: step 1430, loss = 1.92 (718.2 examples/sec; 0.178 sec/batch)
2016-10-15 01:38:38.055031: step 1440, loss = 2.08 (740.2 examples/sec; 0.173 sec/batch)
2016-10-15 01:38:39.826157: step 1450, loss = 2.00 (748.7 examples/sec; 0.171 sec/batch)
2016-10-15 01:38:41.588595: step 1460, loss = 1.99 (723.8 examples/sec; 0.177 sec/batch)
2016-10-15 01:38:43.352630: step 1470, loss = 2.07 (718.3 examples/sec; 0.178 sec/batch)
2016-10-15 01:38:45.125414: step 1480, loss = 2.10 (722.1 examples/sec; 0.177 sec/batch)
2016-10-15 01:38:46.875223: step 1490, loss = 2.04 (727.4 examples/sec; 0.176 sec/batch)
2016-10-15 01:38:48.640191: step 1500, loss = 1.93 (718.1 examples/sec; 0.178 sec/batch)
2016-10-15 01:38:50.608658: step 1510, loss = 1.95 (715.6 examples/sec; 0.179 sec/batch)
2016-10-15 01:38:52.368566: step 1520, loss = 1.89 (717.5 examples/sec; 0.178 sec/batch)
2016-10-15 01:38:54.153251: step 1530, loss = 1.99 (698.7 examples/sec; 0.183 sec/batch)
2016-10-15 01:38:55.908075: step 1540, loss = 1.98 (731.1 examples/sec; 0.175 sec/batch)
2016-10-15 01:38:57.676717: step 1550, loss = 2.09 (735.5 examples/sec; 0.174 sec/batch)
2016-10-15 01:38:59.450876: step 1560, loss = 1.90 (705.3 examples/sec; 0.181 sec/batch)
2016-10-15 01:39:01.215547: step 1570, loss = 1.86 (717.3 examples/sec; 0.178 sec/batch)
2016-10-15 01:39:02.985037: step 1580, loss = 2.08 (743.6 examples/sec; 0.172 sec/batch)
2016-10-15 01:39:04.752951: step 1590, loss = 1.85 (743.2 examples/sec; 0.172 sec/batch)
2016-10-15 01:39:06.516396: step 1600, loss = 1.80 (721.6 examples/sec; 0.177 sec/batch)
2016-10-15 01:39:08.463056: step 1610, loss = 1.81 (734.5 examples/sec; 0.174 sec/batch)
2016-10-15 01:39:10.232027: step 1620, loss = 1.66 (752.5 examples/sec; 0.170 sec/batch)
2016-10-15 01:39:11.993766: step 1630, loss = 1.91 (710.0 examples/sec; 0.180 sec/batch)
2016-10-15 01:39:13.767571: step 1640, loss = 1.74 (729.9 examples/sec; 0.175 sec/batch)
2016-10-15 01:39:15.517990: step 1650, loss = 1.76 (717.3 examples/sec; 0.178 sec/batch)
2016-10-15 01:39:17.290284: step 1660, loss = 2.02 (704.2 examples/sec; 0.182 sec/batch)
2016-10-15 01:39:19.054850: step 1670, loss = 1.91 (736.1 examples/sec; 0.174 sec/batch)
2016-10-15 01:39:20.816843: step 1680, loss = 1.80 (725.6 examples/sec; 0.176 sec/batch)
2016-10-15 01:39:22.579498: step 1690, loss = 1.75 (724.8 examples/sec; 0.177 sec/batch)
2016-10-15 01:39:24.342183: step 1700, loss = 1.86 (732.4 examples/sec; 0.175 sec/batch)
2016-10-15 01:39:26.307519: step 1710, loss = 1.72 (721.8 examples/sec; 0.177 sec/batch)
2016-10-15 01:39:28.075002: step 1720, loss = 1.92 (733.2 examples/sec; 0.175 sec/batch)
2016-10-15 01:39:29.842624: step 1730, loss = 1.96 (752.6 examples/sec; 0.170 sec/batch)
2016-10-15 01:39:31.608648: step 1740, loss = 1.86 (683.7 examples/sec; 0.187 sec/batch)
2016-10-15 01:39:33.371490: step 1750, loss = 1.69 (718.7 examples/sec; 0.178 sec/batch)
2016-10-15 01:39:35.132086: step 1760, loss = 1.58 (719.2 examples/sec; 0.178 sec/batch)
2016-10-15 01:39:36.891728: step 1770, loss = 1.74 (746.4 examples/sec; 0.171 sec/batch)
2016-10-15 01:39:38.653591: step 1780, loss = 1.70 (735.9 examples/sec; 0.174 sec/batch)
2016-10-15 01:39:40.421125: step 1790, loss = 1.76 (715.5 examples/sec; 0.179 sec/batch)
2016-10-15 01:39:42.188690: step 1800, loss = 1.91 (719.0 examples/sec; 0.178 sec/batch)
2016-10-15 01:39:44.147588: step 1810, loss = 1.62 (744.5 examples/sec; 0.172 sec/batch)
2016-10-15 01:39:45.904749: step 1820, loss = 1.78 (737.1 examples/sec; 0.174 sec/batch)
2016-10-15 01:39:47.661879: step 1830, loss = 1.70 (728.4 examples/sec; 0.176 sec/batch)
2016-10-15 01:39:49.421156: step 1840, loss = 1.62 (717.6 examples/sec; 0.178 sec/batch)
2016-10-15 01:39:51.173602: step 1850, loss = 1.82 (733.0 examples/sec; 0.175 sec/batch)
2016-10-15 01:39:52.944106: step 1860, loss = 1.59 (701.4 examples/sec; 0.182 sec/batch)
2016-10-15 01:39:54.703712: step 1870, loss = 1.69 (710.8 examples/sec; 0.180 sec/batch)
2016-10-15 01:39:56.460448: step 1880, loss = 1.76 (721.4 examples/sec; 0.177 sec/batch)
2016-10-15 01:39:58.212544: step 1890, loss = 1.75 (726.9 examples/sec; 0.176 sec/batch)
2016-10-15 01:39:59.974770: step 1900, loss = 1.75 (726.3 examples/sec; 0.176 sec/batch)
2016-10-15 01:40:01.946655: step 1910, loss = 1.84 (698.4 examples/sec; 0.183 sec/batch)
2016-10-15 01:40:03.704211: step 1920, loss = 1.50 (711.9 examples/sec; 0.180 sec/batch)
2016-10-15 01:40:05.474384: step 1930, loss = 1.60 (731.1 examples/sec; 0.175 sec/batch)
2016-10-15 01:40:07.228290: step 1940, loss = 1.76 (721.1 examples/sec; 0.178 sec/batch)
2016-10-15 01:40:08.989078: step 1950, loss = 1.77 (705.5 examples/sec; 0.181 sec/batch)
2016-10-15 01:40:10.748337: step 1960, loss = 1.73 (725.6 examples/sec; 0.176 sec/batch)
2016-10-15 01:40:12.499199: step 1970, loss = 1.83 (735.3 examples/sec; 0.174 sec/batch)
2016-10-15 01:40:14.277149: step 1980, loss = 1.59 (688.6 examples/sec; 0.186 sec/batch)
2016-10-15 01:40:16.025444: step 1990, loss = 1.56 (725.5 examples/sec; 0.176 sec/batch)
2016-10-15 01:40:17.783495: step 2000, loss = 1.74 (718.2 examples/sec; 0.178 sec/batch)
2016-10-15 01:40:20.289444: step 2010, loss = 1.85 (737.0 examples/sec; 0.174 sec/batch)
2016-10-15 01:40:22.050516: step 2020, loss = 1.49 (735.5 examples/sec; 0.174 sec/batch)
2016-10-15 01:40:23.805577: step 2030, loss = 1.63 (729.9 examples/sec; 0.175 sec/batch)
2016-10-15 01:40:25.567801: step 2040, loss = 1.56 (717.0 examples/sec; 0.179 sec/batch)
2016-10-15 01:40:27.325770: step 2050, loss = 1.52 (724.8 examples/sec; 0.177 sec/batch)
2016-10-15 01:40:29.082809: step 2060, loss = 1.63 (727.7 examples/sec; 0.176 sec/batch)
2016-10-15 01:40:30.841306: step 2070, loss = 1.56 (736.3 examples/sec; 0.174 sec/batch)
2016-10-15 01:40:32.607491: step 2080, loss = 1.77 (725.5 examples/sec; 0.176 sec/batch)
2016-10-15 01:40:34.366854: step 2090, loss = 1.49 (729.1 examples/sec; 0.176 sec/batch)
2016-10-15 01:40:36.130436: step 2100, loss = 1.47 (761.9 examples/sec; 0.168 sec/batch)
2016-10-15 01:40:38.072242: step 2110, loss = 1.59 (733.5 examples/sec; 0.175 sec/batch)
2016-10-15 01:40:39.834844: step 2120, loss = 1.40 (738.9 examples/sec; 0.173 sec/batch)
2016-10-15 01:40:41.599839: step 2130, loss = 1.57 (704.2 examples/sec; 0.182 sec/batch)
2016-10-15 01:40:43.361360: step 2140, loss = 1.57 (725.4 examples/sec; 0.176 sec/batch)
2016-10-15 01:40:45.122669: step 2150, loss = 1.60 (740.9 examples/sec; 0.173 sec/batch)
2016-10-15 01:40:46.879708: step 2160, loss = 1.70 (725.5 examples/sec; 0.176 sec/batch)
2016-10-15 01:40:48.641067: step 2170, loss = 1.72 (709.7 examples/sec; 0.180 sec/batch)
2016-10-15 01:40:50.404243: step 2180, loss = 1.32 (721.0 examples/sec; 0.178 sec/batch)
2016-10-15 01:40:52.158060: step 2190, loss = 1.66 (715.2 examples/sec; 0.179 sec/batch)
2016-10-15 01:40:53.914399: step 2200, loss = 1.77 (725.0 examples/sec; 0.177 sec/batch)
2016-10-15 01:40:55.864949: step 2210, loss = 1.54 (738.1 examples/sec; 0.173 sec/batch)
2016-10-15 01:40:57.618984: step 2220, loss = 1.39 (756.0 examples/sec; 0.169 sec/batch)
2016-10-15 01:40:59.379134: step 2230, loss = 1.48 (705.6 examples/sec; 0.181 sec/batch)
2016-10-15 01:41:01.140023: step 2240, loss = 1.62 (717.8 examples/sec; 0.178 sec/batch)
2016-10-15 01:41:02.894050: step 2250, loss = 1.25 (740.9 examples/sec; 0.173 sec/batch)
2016-10-15 01:41:04.649814: step 2260, loss = 1.51 (796.5 examples/sec; 0.161 sec/batch)
2016-10-15 01:41:06.406271: step 2270, loss = 1.35 (735.3 examples/sec; 0.174 sec/batch)
2016-10-15 01:41:08.168519: step 2280, loss = 1.49 (706.2 examples/sec; 0.181 sec/batch)
2016-10-15 01:41:09.917681: step 2290, loss = 1.45 (749.1 examples/sec; 0.171 sec/batch)
2016-10-15 01:41:11.679637: step 2300, loss = 1.50 (748.6 examples/sec; 0.171 sec/batch)
2016-10-15 01:41:13.629206: step 2310, loss = 1.40 (711.5 examples/sec; 0.180 sec/batch)
2016-10-15 01:41:15.392608: step 2320, loss = 1.59 (728.4 examples/sec; 0.176 sec/batch)
2016-10-15 01:41:17.150814: step 2330, loss = 1.76 (715.9 examples/sec; 0.179 sec/batch)
2016-10-15 01:41:18.913065: step 2340, loss = 1.81 (731.3 examples/sec; 0.175 sec/batch)
2016-10-15 01:41:20.670095: step 2350, loss = 1.46 (735.9 examples/sec; 0.174 sec/batch)
2016-10-15 01:41:22.426035: step 2360, loss = 1.31 (717.2 examples/sec; 0.178 sec/batch)
2016-10-15 01:41:24.182383: step 2370, loss = 1.50 (715.7 examples/sec; 0.179 sec/batch)
2016-10-15 01:41:25.932914: step 2380, loss = 1.47 (729.0 examples/sec; 0.176 sec/batch)
2016-10-15 01:41:27.686114: step 2390, loss = 1.45 (711.2 examples/sec; 0.180 sec/batch)
2016-10-15 01:41:29.429876: step 2400, loss = 1.40 (740.9 examples/sec; 0.173 sec/batch)
2016-10-15 01:41:31.369415: step 2410, loss = 1.47 (743.1 examples/sec; 0.172 sec/batch)
2016-10-15 01:41:33.137232: step 2420, loss = 1.48 (710.9 examples/sec; 0.180 sec/batch)
2016-10-15 01:41:34.899103: step 2430, loss = 1.33 (712.3 examples/sec; 0.180 sec/batch)
2016-10-15 01:41:36.653905: step 2440, loss = 1.53 (735.6 examples/sec; 0.174 sec/batch)
2016-10-15 01:41:38.414766: step 2450, loss = 1.35 (726.0 examples/sec; 0.176 sec/batch)
2016-10-15 01:41:40.171695: step 2460, loss = 1.43 (743.2 examples/sec; 0.172 sec/batch)
2016-10-15 01:41:41.931978: step 2470, loss = 1.58 (723.4 examples/sec; 0.177 sec/batch)
2016-10-15 01:41:43.678681: step 2480, loss = 1.32 (744.5 examples/sec; 0.172 sec/batch)
2016-10-15 01:41:45.443689: step 2490, loss = 1.53 (741.4 examples/sec; 0.173 sec/batch)
2016-10-15 01:41:47.202193: step 2500, loss = 1.69 (740.6 examples/sec; 0.173 sec/batch)
2016-10-15 01:41:49.151568: step 2510, loss = 1.58 (721.4 examples/sec; 0.177 sec/batch)
2016-10-15 01:41:50.907675: step 2520, loss = 1.57 (757.3 examples/sec; 0.169 sec/batch)
2016-10-15 01:41:52.672806: step 2530, loss = 1.39 (745.1 examples/sec; 0.172 sec/batch)
2016-10-15 01:41:54.436477: step 2540, loss = 1.30 (708.1 examples/sec; 0.181 sec/batch)
2016-10-15 01:41:56.193188: step 2550, loss = 1.27 (723.0 examples/sec; 0.177 sec/batch)
2016-10-15 01:41:57.950383: step 2560, loss = 1.40 (731.3 examples/sec; 0.175 sec/batch)
2016-10-15 01:41:59.701391: step 2570, loss = 1.20 (722.5 examples/sec; 0.177 sec/batch)
2016-10-15 01:42:01.461404: step 2580, loss = 1.35 (737.9 examples/sec; 0.173 sec/batch)
2016-10-15 01:42:03.227698: step 2590, loss = 1.45 (708.0 examples/sec; 0.181 sec/batch)
2016-10-15 01:42:04.987449: step 2600, loss = 1.54 (776.3 examples/sec; 0.165 sec/batch)
2016-10-15 01:42:06.920158: step 2610, loss = 1.31 (709.7 examples/sec; 0.180 sec/batch)
2016-10-15 01:42:08.680719: step 2620, loss = 1.57 (744.0 examples/sec; 0.172 sec/batch)
2016-10-15 01:42:10.438268: step 2630, loss = 1.37 (720.7 examples/sec; 0.178 sec/batch)
2016-10-15 01:42:12.197921: step 2640, loss = 1.39 (738.9 examples/sec; 0.173 sec/batch)
2016-10-15 01:42:13.960055: step 2650, loss = 1.45 (774.8 examples/sec; 0.165 sec/batch)
2016-10-15 01:42:15.715413: step 2660, loss = 1.41 (717.2 examples/sec; 0.178 sec/batch)
2016-10-15 01:42:17.481581: step 2670, loss = 1.27 (726.5 examples/sec; 0.176 sec/batch)
2016-10-15 01:42:19.240971: step 2680, loss = 1.20 (748.4 examples/sec; 0.171 sec/batch)
2016-10-15 01:42:21.001079: step 2690, loss = 1.39 (721.9 examples/sec; 0.177 sec/batch)
2016-10-15 01:42:22.761430: step 2700, loss = 1.46 (751.1 examples/sec; 0.170 sec/batch)
2016-10-15 01:42:24.722937: step 2710, loss = 1.27 (762.1 examples/sec; 0.168 sec/batch)
2016-10-15 01:42:26.479084: step 2720, loss = 1.35 (727.4 examples/sec; 0.176 sec/batch)
2016-10-15 01:42:28.246633: step 2730, loss = 1.51 (713.6 examples/sec; 0.179 sec/batch)
2016-10-15 01:42:30.015564: step 2740, loss = 1.36 (713.2 examples/sec; 0.179 sec/batch)
2016-10-15 01:42:31.789787: step 2750, loss = 1.36 (696.6 examples/sec; 0.184 sec/batch)
2016-10-15 01:42:33.547250: step 2760, loss = 1.50 (732.3 examples/sec; 0.175 sec/batch)
2016-10-15 01:42:35.297289: step 2770, loss = 1.16 (751.2 examples/sec; 0.170 sec/batch)
2016-10-15 01:42:37.059961: step 2780, loss = 1.43 (713.0 examples/sec; 0.180 sec/batch)
2016-10-15 01:42:38.809814: step 2790, loss = 1.49 (747.8 examples/sec; 0.171 sec/batch)
2016-10-15 01:42:40.568172: step 2800, loss = 1.37 (744.7 examples/sec; 0.172 sec/batch)
2016-10-15 01:42:42.510535: step 2810, loss = 1.72 (727.9 examples/sec; 0.176 sec/batch)
2016-10-15 01:42:44.274931: step 2820, loss = 1.30 (710.1 examples/sec; 0.180 sec/batch)
2016-10-15 01:42:46.035754: step 2830, loss = 1.26 (718.6 examples/sec; 0.178 sec/batch)
2016-10-15 01:42:47.789983: step 2840, loss = 1.19 (728.3 examples/sec; 0.176 sec/batch)
2016-10-15 01:42:49.554813: step 2850, loss = 1.16 (715.6 examples/sec; 0.179 sec/batch)
2016-10-15 01:42:51.305655: step 2860, loss = 1.40 (737.6 examples/sec; 0.174 sec/batch)
2016-10-15 01:42:53.070838: step 2870, loss = 1.29 (699.9 examples/sec; 0.183 sec/batch)
2016-10-15 01:42:54.826295: step 2880, loss = 1.33 (726.2 examples/sec; 0.176 sec/batch)
2016-10-15 01:42:56.574127: step 2890, loss = 1.26 (770.2 examples/sec; 0.166 sec/batch)
2016-10-15 01:42:58.339358: step 2900, loss = 1.14 (714.7 examples/sec; 0.179 sec/batch)
2016-10-15 01:43:00.272186: step 2910, loss = 1.20 (743.1 examples/sec; 0.172 sec/batch)
2016-10-15 01:43:02.038014: step 2920, loss = 1.18 (719.2 examples/sec; 0.178 sec/batch)
2016-10-15 01:43:03.806106: step 2930, loss = 1.33 (721.1 examples/sec; 0.177 sec/batch)
2016-10-15 01:43:05.557273: step 2940, loss = 1.09 (718.5 examples/sec; 0.178 sec/batch)
2016-10-15 01:43:07.314254: step 2950, loss = 1.34 (710.2 examples/sec; 0.180 sec/batch)
2016-10-15 01:43:09.071078: step 2960, loss = 1.41 (725.9 examples/sec; 0.176 sec/batch)
2016-10-15 01:43:10.826313: step 2970, loss = 1.29 (739.2 examples/sec; 0.173 sec/batch)
2016-10-15 01:43:12.590241: step 2980, loss = 1.32 (735.7 examples/sec; 0.174 sec/batch)
2016-10-15 01:43:14.344396: step 2990, loss = 1.22 (730.3 examples/sec; 0.175 sec/batch)
2016-10-15 01:43:16.096741: step 3000, loss = 1.54 (731.5 examples/sec; 0.175 sec/batch)
2016-10-15 01:43:18.539002: step 3010, loss = 1.40 (703.1 examples/sec; 0.182 sec/batch)
2016-10-15 01:43:20.296741: step 3020, loss = 1.30 (702.4 examples/sec; 0.182 sec/batch)
2016-10-15 01:43:22.054168: step 3030, loss = 1.47 (753.8 examples/sec; 0.170 sec/batch)
2016-10-15 01:43:23.811034: step 3040, loss = 1.22 (715.7 examples/sec; 0.179 sec/batch)
2016-10-15 01:43:25.566044: step 3050, loss = 1.17 (724.0 examples/sec; 0.177 sec/batch)
2016-10-15 01:43:27.330084: step 3060, loss = 1.13 (727.5 examples/sec; 0.176 sec/batch)
2016-10-15 01:43:29.107145: step 3070, loss = 1.32 (705.5 examples/sec; 0.181 sec/batch)
2016-10-15 01:43:30.867865: step 3080, loss = 1.26 (726.7 examples/sec; 0.176 sec/batch)
2016-10-15 01:43:32.641528: step 3090, loss = 1.33 (686.1 examples/sec; 0.187 sec/batch)
2016-10-15 01:43:34.386311: step 3100, loss = 1.37 (760.0 examples/sec; 0.168 sec/batch)
2016-10-15 01:43:36.323642: step 3110, loss = 1.28 (717.9 examples/sec; 0.178 sec/batch)
2016-10-15 01:43:38.080088: step 3120, loss = 1.35 (723.1 examples/sec; 0.177 sec/batch)
2016-10-15 01:43:39.834034: step 3130, loss = 1.41 (693.3 examples/sec; 0.185 sec/batch)
2016-10-15 01:43:41.586239: step 3140, loss = 1.37 (732.3 examples/sec; 0.175 sec/batch)
2016-10-15 01:43:43.346759: step 3150, loss = 1.16 (738.4 examples/sec; 0.173 sec/batch)
2016-10-15 01:43:45.106425: step 3160, loss = 1.26 (750.4 examples/sec; 0.171 sec/batch)
2016-10-15 01:43:46.864303: step 3170, loss = 1.27 (713.8 examples/sec; 0.179 sec/batch)
2016-10-15 01:43:48.630291: step 3180, loss = 1.16 (714.2 examples/sec; 0.179 sec/batch)
2016-10-15 01:43:50.390443: step 3190, loss = 1.15 (729.9 examples/sec; 0.175 sec/batch)
2016-10-15 01:43:52.146721: step 3200, loss = 1.29 (761.3 examples/sec; 0.168 sec/batch)
2016-10-15 01:43:54.095509: step 3210, loss = 1.35 (733.0 examples/sec; 0.175 sec/batch)
2016-10-15 01:43:55.854760: step 3220, loss = 1.28 (736.8 examples/sec; 0.174 sec/batch)
2016-10-15 01:43:57.621112: step 3230, loss = 1.17 (728.4 examples/sec; 0.176 sec/batch)
2016-10-15 01:43:59.379430: step 3240, loss = 1.44 (726.3 examples/sec; 0.176 sec/batch)
2016-10-15 01:44:01.137564: step 3250, loss = 1.46 (717.2 examples/sec; 0.178 sec/batch)
2016-10-15 01:44:02.904240: step 3260, loss = 1.29 (748.7 examples/sec; 0.171 sec/batch)
2016-10-15 01:44:04.668976: step 3270, loss = 1.23 (711.9 examples/sec; 0.180 sec/batch)
2016-10-15 01:44:06.420983: step 3280, loss = 1.30 (734.0 examples/sec; 0.174 sec/batch)
2016-10-15 01:44:08.179074: step 3290, loss = 1.30 (730.0 examples/sec; 0.175 sec/batch)
2016-10-15 01:44:09.937793: step 3300, loss = 1.39 (744.8 examples/sec; 0.172 sec/batch)
2016-10-15 01:44:11.881633: step 3310, loss = 1.23 (746.5 examples/sec; 0.171 sec/batch)
2016-10-15 01:44:13.633387: step 3320, loss = 1.21 (728.5 examples/sec; 0.176 sec/batch)
2016-10-15 01:44:15.382530: step 3330, loss = 1.13 (728.4 examples/sec; 0.176 sec/batch)
2016-10-15 01:44:17.143875: step 3340, loss = 1.29 (700.0 examples/sec; 0.183 sec/batch)
2016-10-15 01:44:18.897359: step 3350, loss = 1.20 (712.9 examples/sec; 0.180 sec/batch)
2016-10-15 01:44:20.660321: step 3360, loss = 1.09 (713.7 examples/sec; 0.179 sec/batch)
2016-10-15 01:44:22.417315: step 3370, loss = 1.08 (727.3 examples/sec; 0.176 sec/batch)
2016-10-15 01:44:24.180641: step 3380, loss = 1.13 (725.9 examples/sec; 0.176 sec/batch)
2016-10-15 01:44:25.937359: step 3390, loss = 1.24 (726.4 examples/sec; 0.176 sec/batch)
2016-10-15 01:44:27.686885: step 3400, loss = 1.21 (751.4 examples/sec; 0.170 sec/batch)
2016-10-15 01:44:29.632835: step 3410, loss = 1.12 (735.4 examples/sec; 0.174 sec/batch)
2016-10-15 01:44:31.390976: step 3420, loss = 1.04 (713.7 examples/sec; 0.179 sec/batch)
2016-10-15 01:44:33.141777: step 3430, loss = 1.16 (734.1 examples/sec; 0.174 sec/batch)
2016-10-15 01:44:34.901939: step 3440, loss = 1.07 (718.1 examples/sec; 0.178 sec/batch)
2016-10-15 01:44:36.664486: step 3450, loss = 1.13 (709.5 examples/sec; 0.180 sec/batch)
2016-10-15 01:44:38.412644: step 3460, loss = 1.00 (714.9 examples/sec; 0.179 sec/batch)
2016-10-15 01:44:40.174281: step 3470, loss = 1.24 (739.3 examples/sec; 0.173 sec/batch)
2016-10-15 01:44:41.938901: step 3480, loss = 1.09 (687.2 examples/sec; 0.186 sec/batch)
2016-10-15 01:44:43.694851: step 3490, loss = 1.30 (711.8 examples/sec; 0.180 sec/batch)
2016-10-15 01:44:45.459390: step 3500, loss = 1.23 (691.1 examples/sec; 0.185 sec/batch)
2016-10-15 01:44:47.395750: step 3510, loss = 1.17 (729.5 examples/sec; 0.175 sec/batch)
2016-10-15 01:44:49.153980: step 3520, loss = 1.14 (726.7 examples/sec; 0.176 sec/batch)
2016-10-15 01:44:50.910478: step 3530, loss = 1.17 (755.1 examples/sec; 0.170 sec/batch)
2016-10-15 01:44:52.666940: step 3540, loss = 1.34 (726.9 examples/sec; 0.176 sec/batch)
2016-10-15 01:44:54.428625: step 3550, loss = 1.44 (733.3 examples/sec; 0.175 sec/batch)
2016-10-15 01:44:56.176708: step 3560, loss = 1.12 (755.2 examples/sec; 0.169 sec/batch)
2016-10-15 01:44:57.940992: step 3570, loss = 1.23 (739.6 examples/sec; 0.173 sec/batch)
2016-10-15 01:44:59.690687: step 3580, loss = 1.22 (738.3 examples/sec; 0.173 sec/batch)
2016-10-15 01:45:01.458976: step 3590, loss = 1.12 (690.6 examples/sec; 0.185 sec/batch)
2016-10-15 01:45:03.219793: step 3600, loss = 1.39 (734.5 examples/sec; 0.174 sec/batch)
2016-10-15 01:45:05.157736: step 3610, loss = 1.43 (733.7 examples/sec; 0.174 sec/batch)
2016-10-15 01:45:06.916503: step 3620, loss = 1.23 (711.7 examples/sec; 0.180 sec/batch)
2016-10-15 01:45:08.673643: step 3630, loss = 1.17 (728.3 examples/sec; 0.176 sec/batch)
2016-10-15 01:45:10.433244: step 3640, loss = 1.35 (715.0 examples/sec; 0.179 sec/batch)
2016-10-15 01:45:12.195677: step 3650, loss = 1.17 (714.3 examples/sec; 0.179 sec/batch)
2016-10-15 01:45:13.950663: step 3660, loss = 1.20 (754.7 examples/sec; 0.170 sec/batch)
2016-10-15 01:45:15.706642: step 3670, loss = 1.12 (746.4 examples/sec; 0.171 sec/batch)
2016-10-15 01:45:17.464547: step 3680, loss = 1.22 (735.5 examples/sec; 0.174 sec/batch)
2016-10-15 01:45:19.229631: step 3690, loss = 1.33 (726.0 examples/sec; 0.176 sec/batch)
2016-10-15 01:45:20.990016: step 3700, loss = 1.14 (733.9 examples/sec; 0.174 sec/batch)
2016-10-15 01:45:22.923777: step 3710, loss = 1.20 (730.3 examples/sec; 0.175 sec/batch)
2016-10-15 01:45:24.686714: step 3720, loss = 1.38 (736.6 examples/sec; 0.174 sec/batch)
2016-10-15 01:45:26.450662: step 3730, loss = 1.07 (714.6 examples/sec; 0.179 sec/batch)
2016-10-15 01:45:28.204472: step 3740, loss = 1.10 (744.7 examples/sec; 0.172 sec/batch)
2016-10-15 01:45:29.966838: step 3750, loss = 1.16 (746.1 examples/sec; 0.172 sec/batch)
2016-10-15 01:45:31.725109: step 3760, loss = 1.15 (761.0 examples/sec; 0.168 sec/batch)
2016-10-15 01:45:33.490109: step 3770, loss = 1.01 (740.0 examples/sec; 0.173 sec/batch)
2016-10-15 01:45:35.254066: step 3780, loss = 1.14 (718.3 examples/sec; 0.178 sec/batch)
2016-10-15 01:45:37.009309: step 3790, loss = 0.99 (746.3 examples/sec; 0.172 sec/batch)
2016-10-15 01:45:38.772169: step 3800, loss = 1.20 (730.5 examples/sec; 0.175 sec/batch)
2016-10-15 01:45:40.715375: step 3810, loss = 1.10 (789.2 examples/sec; 0.162 sec/batch)
2016-10-15 01:45:42.464937: step 3820, loss = 1.24 (725.9 examples/sec; 0.176 sec/batch)
2016-10-15 01:45:44.226124: step 3830, loss = 1.15 (725.4 examples/sec; 0.176 sec/batch)
2016-10-15 01:45:45.988188: step 3840, loss = 1.05 (723.2 examples/sec; 0.177 sec/batch)
2016-10-15 01:45:47.732594: step 3850, loss = 0.97 (760.8 examples/sec; 0.168 sec/batch)
2016-10-15 01:45:49.499402: step 3860, loss = 1.18 (734.3 examples/sec; 0.174 sec/batch)
2016-10-15 01:45:51.264844: step 3870, loss = 1.16 (690.8 examples/sec; 0.185 sec/batch)
2016-10-15 01:45:53.015262: step 3880, loss = 1.17 (738.3 examples/sec; 0.173 sec/batch)
2016-10-15 01:45:54.768683: step 3890, loss = 1.16 (728.0 examples/sec; 0.176 sec/batch)
2016-10-15 01:45:56.524684: step 3900, loss = 1.19 (726.0 examples/sec; 0.176 sec/batch)
2016-10-15 01:45:58.470643: step 3910, loss = 1.16 (736.8 examples/sec; 0.174 sec/batch)
2016-10-15 01:46:00.236814: step 3920, loss = 1.13 (701.2 examples/sec; 0.183 sec/batch)
2016-10-15 01:46:01.994755: step 3930, loss = 1.20 (718.8 examples/sec; 0.178 sec/batch)
2016-10-15 01:46:03.764894: step 3940, loss = 1.16 (717.8 examples/sec; 0.178 sec/batch)
2016-10-15 01:46:05.524807: step 3950, loss = 1.28 (708.8 examples/sec; 0.181 sec/batch)
2016-10-15 01:46:07.287520: step 3960, loss = 1.22 (706.3 examples/sec; 0.181 sec/batch)
2016-10-15 01:46:09.044806: step 3970, loss = 1.19 (745.5 examples/sec; 0.172 sec/batch)
2016-10-15 01:46:10.813028: step 3980, loss = 1.08 (714.3 examples/sec; 0.179 sec/batch)
2016-10-15 01:46:12.568192: step 3990, loss = 0.97 (732.8 examples/sec; 0.175 sec/batch)
2016-10-15 01:46:14.329135: step 4000, loss = 1.11 (738.5 examples/sec; 0.173 sec/batch)
2016-10-15 01:46:16.803622: step 4010, loss = 1.20 (749.0 examples/sec; 0.171 sec/batch)
2016-10-15 01:46:18.558668: step 4020, loss = 1.28 (729.9 examples/sec; 0.175 sec/batch)
2016-10-15 01:46:20.324354: step 4030, loss = 1.22 (734.3 examples/sec; 0.174 sec/batch)
2016-10-15 01:46:22.077543: step 4040, loss = 0.90 (737.8 examples/sec; 0.173 sec/batch)
2016-10-15 01:46:23.833144: step 4050, loss = 0.89 (739.0 examples/sec; 0.173 sec/batch)
2016-10-15 01:46:25.596292: step 4060, loss = 1.33 (732.6 examples/sec; 0.175 sec/batch)
2016-10-15 01:46:27.349790: step 4070, loss = 1.03 (717.0 examples/sec; 0.179 sec/batch)
2016-10-15 01:46:29.105669: step 4080, loss = 0.87 (746.0 examples/sec; 0.172 sec/batch)
2016-10-15 01:46:30.867077: step 4090, loss = 1.16 (721.3 examples/sec; 0.177 sec/batch)
2016-10-15 01:46:32.625854: step 4100, loss = 1.07 (730.8 examples/sec; 0.175 sec/batch)
2016-10-15 01:46:34.575251: step 4110, loss = 1.17 (723.4 examples/sec; 0.177 sec/batch)
2016-10-15 01:46:36.336100: step 4120, loss = 1.21 (763.6 examples/sec; 0.168 sec/batch)
2016-10-15 01:46:38.093245: step 4130, loss = 0.99 (720.1 examples/sec; 0.178 sec/batch)
2016-10-15 01:46:39.848958: step 4140, loss = 1.23 (724.2 examples/sec; 0.177 sec/batch)
2016-10-15 01:46:41.601477: step 4150, loss = 0.98 (719.3 examples/sec; 0.178 sec/batch)
2016-10-15 01:46:43.358735: step 4160, loss = 0.91 (723.0 examples/sec; 0.177 sec/batch)
2016-10-15 01:46:45.127360: step 4170, loss = 0.97 (724.8 examples/sec; 0.177 sec/batch)
2016-10-15 01:46:46.893311: step 4180, loss = 1.16 (752.1 examples/sec; 0.170 sec/batch)
2016-10-15 01:46:48.666818: step 4190, loss = 0.83 (720.1 examples/sec; 0.178 sec/batch)
2016-10-15 01:46:50.427304: step 4200, loss = 1.01 (746.9 examples/sec; 0.171 sec/batch)
2016-10-15 01:46:52.387470: step 4210, loss = 1.07 (727.5 examples/sec; 0.176 sec/batch)
2016-10-15 01:46:54.152816: step 4220, loss = 1.09 (732.4 examples/sec; 0.175 sec/batch)
2016-10-15 01:46:55.917852: step 4230, loss = 1.18 (706.4 examples/sec; 0.181 sec/batch)
2016-10-15 01:46:57.678809: step 4240, loss = 1.05 (739.4 examples/sec; 0.173 sec/batch)
2016-10-15 01:46:59.439316: step 4250, loss = 1.12 (726.5 examples/sec; 0.176 sec/batch)
2016-10-15 01:47:01.197604: step 4260, loss = 0.99 (738.1 examples/sec; 0.173 sec/batch)
2016-10-15 01:47:02.963519: step 4270, loss = 1.02 (725.5 examples/sec; 0.176 sec/batch)
2016-10-15 01:47:04.721299: step 4280, loss = 1.03 (714.1 examples/sec; 0.179 sec/batch)
2016-10-15 01:47:06.485175: step 4290, loss = 1.05 (728.2 examples/sec; 0.176 sec/batch)
2016-10-15 01:47:08.257805: step 4300, loss = 1.13 (693.4 examples/sec; 0.185 sec/batch)
2016-10-15 01:47:10.212629: step 4310, loss = 1.16 (703.9 examples/sec; 0.182 sec/batch)
2016-10-15 01:47:11.967647: step 4320, loss = 1.15 (734.3 examples/sec; 0.174 sec/batch)
2016-10-15 01:47:13.732060: step 4330, loss = 1.31 (732.8 examples/sec; 0.175 sec/batch)
2016-10-15 01:47:15.490710: step 4340, loss = 1.12 (752.6 examples/sec; 0.170 sec/batch)
2016-10-15 01:47:17.253428: step 4350, loss = 1.11 (711.9 examples/sec; 0.180 sec/batch)
2016-10-15 01:47:19.005097: step 4360, loss = 1.24 (725.3 examples/sec; 0.176 sec/batch)
2016-10-15 01:47:20.771650: step 4370, loss = 1.05 (727.7 examples/sec; 0.176 sec/batch)
2016-10-15 01:47:22.529010: step 4380, loss = 1.19 (731.0 examples/sec; 0.175 sec/batch)
2016-10-15 01:47:24.295701: step 4390, loss = 1.00 (686.3 examples/sec; 0.187 sec/batch)
2016-10-15 01:47:26.049935: step 4400, loss = 1.21 (720.1 examples/sec; 0.178 sec/batch)
2016-10-15 01:47:27.983073: step 4410, loss = 1.02 (738.8 examples/sec; 0.173 sec/batch)
2016-10-15 01:47:29.749937: step 4420, loss = 1.08 (721.3 examples/sec; 0.177 sec/batch)
2016-10-15 01:47:31.521101: step 4430, loss = 1.16 (727.4 examples/sec; 0.176 sec/batch)
2016-10-15 01:47:33.283626: step 4440, loss = 1.04 (730.8 examples/sec; 0.175 sec/batch)
2016-10-15 01:47:35.036271: step 4450, loss = 1.12 (719.9 examples/sec; 0.178 sec/batch)
2016-10-15 01:47:36.798896: step 4460, loss = 1.03 (736.4 examples/sec; 0.174 sec/batch)
2016-10-15 01:47:38.565617: step 4470, loss = 1.04 (751.8 examples/sec; 0.170 sec/batch)
2016-10-15 01:47:40.332897: step 4480, loss = 0.98 (714.7 examples/sec; 0.179 sec/batch)
2016-10-15 01:47:42.083364: step 4490, loss = 1.11 (728.7 examples/sec; 0.176 sec/batch)
2016-10-15 01:47:43.852974: step 4500, loss = 1.08 (726.5 examples/sec; 0.176 sec/batch)
2016-10-15 01:47:45.804348: step 4510, loss = 1.18 (726.5 examples/sec; 0.176 sec/batch)
2016-10-15 01:47:47.566806: step 4520, loss = 1.20 (731.0 examples/sec; 0.175 sec/batch)
2016-10-15 01:47:49.333198: step 4530, loss = 1.12 (723.1 examples/sec; 0.177 sec/batch)
2016-10-15 01:47:51.098221: step 4540, loss = 0.95 (730.5 examples/sec; 0.175 sec/batch)
2016-10-15 01:47:52.856694: step 4550, loss = 0.91 (739.1 examples/sec; 0.173 sec/batch)
2016-10-15 01:47:54.614860: step 4560, loss = 1.00 (747.4 examples/sec; 0.171 sec/batch)
2016-10-15 01:47:56.375023: step 4570, loss = 1.14 (730.9 examples/sec; 0.175 sec/batch)
2016-10-15 01:47:58.132451: step 4580, loss = 0.96 (727.5 examples/sec; 0.176 sec/batch)
2016-10-15 01:47:59.885883: step 4590, loss = 1.03 (718.3 examples/sec; 0.178 sec/batch)
2016-10-15 01:48:01.638837: step 4600, loss = 1.05 (733.3 examples/sec; 0.175 sec/batch)
2016-10-15 01:48:03.593240: step 4610, loss = 1.06 (707.6 examples/sec; 0.181 sec/batch)
2016-10-15 01:48:05.348972: step 4620, loss = 1.11 (742.5 examples/sec; 0.172 sec/batch)
2016-10-15 01:48:07.107122: step 4630, loss = 1.15 (746.3 examples/sec; 0.172 sec/batch)
2016-10-15 01:48:08.864323: step 4640, loss = 0.92 (753.0 examples/sec; 0.170 sec/batch)
2016-10-15 01:48:10.616550: step 4650, loss = 1.14 (723.6 examples/sec; 0.177 sec/batch)
2016-10-15 01:48:12.383106: step 4660, loss = 1.07 (706.5 examples/sec; 0.181 sec/batch)
2016-10-15 01:48:14.140739: step 4670, loss = 0.93 (720.3 examples/sec; 0.178 sec/batch)
2016-10-15 01:48:15.897379: step 4680, loss = 1.01 (716.5 examples/sec; 0.179 sec/batch)
2016-10-15 01:48:17.655429: step 4690, loss = 1.00 (745.8 examples/sec; 0.172 sec/batch)
2016-10-15 01:48:19.408217: step 4700, loss = 0.93 (726.8 examples/sec; 0.176 sec/batch)
2016-10-15 01:48:21.343993: step 4710, loss = 1.10 (749.0 examples/sec; 0.171 sec/batch)
2016-10-15 01:48:23.101144: step 4720, loss = 1.13 (707.7 examples/sec; 0.181 sec/batch)
2016-10-15 01:48:24.861469: step 4730, loss = 1.15 (746.1 examples/sec; 0.172 sec/batch)
2016-10-15 01:48:26.610646: step 4740, loss = 0.92 (748.8 examples/sec; 0.171 sec/batch)
2016-10-15 01:48:28.359846: step 4750, loss = 1.00 (741.9 examples/sec; 0.173 sec/batch)
2016-10-15 01:48:30.109718: step 4760, loss = 1.18 (727.5 examples/sec; 0.176 sec/batch)
2016-10-15 01:48:31.874271: step 4770, loss = 1.17 (718.4 examples/sec; 0.178 sec/batch)
2016-10-15 01:48:33.629938: step 4780, loss = 0.89 (743.9 examples/sec; 0.172 sec/batch)
2016-10-15 01:48:35.388196: step 4790, loss = 1.06 (725.3 examples/sec; 0.176 sec/batch)
2016-10-15 01:48:37.150402: step 4800, loss = 1.02 (762.1 examples/sec; 0.168 sec/batch)
2016-10-15 01:48:39.098523: step 4810, loss = 0.87 (726.6 examples/sec; 0.176 sec/batch)
2016-10-15 01:48:40.856101: step 4820, loss = 0.90 (752.2 examples/sec; 0.170 sec/batch)
2016-10-15 01:48:42.624718: step 4830, loss = 1.17 (725.5 examples/sec; 0.176 sec/batch)
2016-10-15 01:48:44.390764: step 4840, loss = 1.01 (710.0 examples/sec; 0.180 sec/batch)
2016-10-15 01:48:46.144216: step 4850, loss = 1.08 (731.7 examples/sec; 0.175 sec/batch)
2016-10-15 01:48:47.898580: step 4860, loss = 1.04 (717.3 examples/sec; 0.178 sec/batch)
2016-10-15 01:48:49.666842: step 4870, loss = 1.27 (713.9 examples/sec; 0.179 sec/batch)
2016-10-15 01:48:51.417909: step 4880, loss = 1.15 (728.1 examples/sec; 0.176 sec/batch)
2016-10-15 01:48:53.180770: step 4890, loss = 1.10 (715.2 examples/sec; 0.179 sec/batch)
2016-10-15 01:48:54.934609: step 4900, loss = 1.19 (748.6 examples/sec; 0.171 sec/batch)
2016-10-15 01:48:56.877020: step 4910, loss = 1.00 (711.1 examples/sec; 0.180 sec/batch)
2016-10-15 01:48:58.627440: step 4920, loss = 1.08 (734.1 examples/sec; 0.174 sec/batch)
2016-10-15 01:49:00.386028: step 4930, loss = 0.79 (699.8 examples/sec; 0.183 sec/batch)
2016-10-15 01:49:02.135887: step 4940, loss = 1.15 (726.6 examples/sec; 0.176 sec/batch)
2016-10-15 01:49:03.888290: step 4950, loss = 1.13 (729.1 examples/sec; 0.176 sec/batch)
2016-10-15 01:49:05.640877: step 4960, loss = 1.05 (732.3 examples/sec; 0.175 sec/batch)
2016-10-15 01:49:07.394603: step 4970, loss = 1.01 (756.9 examples/sec; 0.169 sec/batch)
2016-10-15 01:49:09.154514: step 4980, loss = 1.22 (773.9 examples/sec; 0.165 sec/batch)
2016-10-15 01:49:10.910408: step 4990, loss = 1.19 (741.6 examples/sec; 0.173 sec/batch)
2016-10-15 01:49:12.672847: step 5000, loss = 0.98 (708.1 examples/sec; 0.181 sec/batch)
2016-10-15 01:49:15.248468: step 5010, loss = 1.14 (725.8 examples/sec; 0.176 sec/batch)
2016-10-15 01:49:17.007107: step 5020, loss = 0.92 (745.5 examples/sec; 0.172 sec/batch)
2016-10-15 01:49:18.764177: step 5030, loss = 0.98 (730.9 examples/sec; 0.175 sec/batch)
2016-10-15 01:49:20.519535: step 5040, loss = 0.87 (759.4 examples/sec; 0.169 sec/batch)
2016-10-15 01:49:22.264302: step 5050, loss = 0.94 (742.4 examples/sec; 0.172 sec/batch)
2016-10-15 01:49:24.012990: step 5060, loss = 0.96 (734.1 examples/sec; 0.174 sec/batch)
2016-10-15 01:49:25.775634: step 5070, loss = 1.06 (748.3 examples/sec; 0.171 sec/batch)
2016-10-15 01:49:27.536168: step 5080, loss = 0.90 (712.8 examples/sec; 0.180 sec/batch)
2016-10-15 01:49:29.289245: step 5090, loss = 0.86 (728.1 examples/sec; 0.176 sec/batch)
2016-10-15 01:49:31.041102: step 5100, loss = 1.03 (733.2 examples/sec; 0.175 sec/batch)
2016-10-15 01:49:32.988690: step 5110, loss = 1.22 (709.2 examples/sec; 0.180 sec/batch)
2016-10-15 01:49:34.740826: step 5120, loss = 0.84 (750.3 examples/sec; 0.171 sec/batch)
2016-10-15 01:49:36.498505: step 5130, loss = 1.02 (715.5 examples/sec; 0.179 sec/batch)
2016-10-15 01:49:38.252096: step 5140, loss = 1.16 (750.1 examples/sec; 0.171 sec/batch)
2016-10-15 01:49:40.005460: step 5150, loss = 0.90 (708.4 examples/sec; 0.181 sec/batch)
2016-10-15 01:49:41.765142: step 5160, loss = 0.90 (720.7 examples/sec; 0.178 sec/batch)
2016-10-15 01:49:43.521496: step 5170, loss = 1.18 (731.1 examples/sec; 0.175 sec/batch)
2016-10-15 01:49:45.272992: step 5180, loss = 0.89 (734.5 examples/sec; 0.174 sec/batch)
2016-10-15 01:49:47.031295: step 5190, loss = 1.02 (714.1 examples/sec; 0.179 sec/batch)
2016-10-15 01:49:48.781902: step 5200, loss = 1.00 (734.4 examples/sec; 0.174 sec/batch)
2016-10-15 01:49:50.719921: step 5210, loss = 0.95 (741.7 examples/sec; 0.173 sec/batch)
2016-10-15 01:49:52.480249: step 5220, loss = 1.22 (755.2 examples/sec; 0.169 sec/batch)
2016-10-15 01:49:54.235523: step 5230, loss = 0.96 (747.8 examples/sec; 0.171 sec/batch)
2016-10-15 01:49:55.985651: step 5240, loss = 1.03 (750.8 examples/sec; 0.170 sec/batch)
2016-10-15 01:49:57.737723: step 5250, loss = 1.01 (727.9 examples/sec; 0.176 sec/batch)
2016-10-15 01:49:59.492784: step 5260, loss = 1.06 (716.9 examples/sec; 0.179 sec/batch)
2016-10-15 01:50:01.256446: step 5270, loss = 1.06 (706.9 examples/sec; 0.181 sec/batch)
2016-10-15 01:50:03.043709: step 5280, loss = 1.38 (709.5 examples/sec; 0.180 sec/batch)
2016-10-15 01:50:04.803574: step 5290, loss = 0.87 (716.6 examples/sec; 0.179 sec/batch)
2016-10-15 01:50:06.560597: step 5300, loss = 0.83 (728.5 examples/sec; 0.176 sec/batch)
2016-10-15 01:50:08.504654: step 5310, loss = 0.99 (708.4 examples/sec; 0.181 sec/batch)
2016-10-15 01:50:10.256834: step 5320, loss = 0.94 (721.2 examples/sec; 0.177 sec/batch)
2016-10-15 01:50:12.010678: step 5330, loss = 1.12 (725.2 examples/sec; 0.177 sec/batch)
2016-10-15 01:50:13.773233: step 5340, loss = 0.90 (740.5 examples/sec; 0.173 sec/batch)
2016-10-15 01:50:15.533360: step 5350, loss = 1.00 (717.4 examples/sec; 0.178 sec/batch)
2016-10-15 01:50:17.297217: step 5360, loss = 1.19 (724.4 examples/sec; 0.177 sec/batch)
2016-10-15 01:50:19.054379: step 5370, loss = 0.97 (709.8 examples/sec; 0.180 sec/batch)
2016-10-15 01:50:20.811426: step 5380, loss = 1.00 (715.0 examples/sec; 0.179 sec/batch)
2016-10-15 01:50:22.568474: step 5390, loss = 1.12 (731.0 examples/sec; 0.175 sec/batch)
2016-10-15 01:50:24.325560: step 5400, loss = 1.13 (757.0 examples/sec; 0.169 sec/batch)
2016-10-15 01:50:26.285486: step 5410, loss = 1.07 (729.3 examples/sec; 0.176 sec/batch)
2016-10-15 01:50:28.048325: step 5420, loss = 0.96 (738.6 examples/sec; 0.173 sec/batch)
2016-10-15 01:50:29.804254: step 5430, loss = 0.95 (733.2 examples/sec; 0.175 sec/batch)
2016-10-15 01:50:31.561562: step 5440, loss = 1.13 (723.2 examples/sec; 0.177 sec/batch)
2016-10-15 01:50:33.313191: step 5450, loss = 0.96 (718.5 examples/sec; 0.178 sec/batch)
2016-10-15 01:50:35.072035: step 5460, loss = 1.14 (730.2 examples/sec; 0.175 sec/batch)
2016-10-15 01:50:36.830313: step 5470, loss = 0.94 (710.6 examples/sec; 0.180 sec/batch)
2016-10-15 01:50:38.588029: step 5480, loss = 0.75 (713.0 examples/sec; 0.180 sec/batch)
2016-10-15 01:50:40.339947: step 5490, loss = 0.87 (725.1 examples/sec; 0.177 sec/batch)
2016-10-15 01:50:42.103951: step 5500, loss = 0.84 (713.7 examples/sec; 0.179 sec/batch)
2016-10-15 01:50:44.067078: step 5510, loss = 1.14 (702.2 examples/sec; 0.182 sec/batch)
2016-10-15 01:50:45.829225: step 5520, loss = 1.05 (698.8 examples/sec; 0.183 sec/batch)
2016-10-15 01:50:47.585444: step 5530, loss = 0.90 (725.3 examples/sec; 0.176 sec/batch)
2016-10-15 01:50:49.350500: step 5540, loss = 0.93 (730.7 examples/sec; 0.175 sec/batch)
2016-10-15 01:50:51.105716: step 5550, loss = 0.97 (749.3 examples/sec; 0.171 sec/batch)
2016-10-15 01:50:52.866764: step 5560, loss = 0.98 (746.0 examples/sec; 0.172 sec/batch)
2016-10-15 01:50:54.628091: step 5570, loss = 1.00 (734.0 examples/sec; 0.174 sec/batch)
2016-10-15 01:50:56.390089: step 5580, loss = 1.12 (699.8 examples/sec; 0.183 sec/batch)
2016-10-15 01:50:58.139795: step 5590, loss = 0.96 (720.7 examples/sec; 0.178 sec/batch)
2016-10-15 01:50:59.900877: step 5600, loss = 0.88 (697.3 examples/sec; 0.184 sec/batch)
2016-10-15 01:51:01.857150: step 5610, loss = 1.04 (715.1 examples/sec; 0.179 sec/batch)
2016-10-15 01:51:03.609912: step 5620, loss = 0.94 (704.0 examples/sec; 0.182 sec/batch)
2016-10-15 01:51:05.357926: step 5630, loss = 0.93 (732.7 examples/sec; 0.175 sec/batch)
2016-10-15 01:51:07.117644: step 5640, loss = 0.86 (716.1 examples/sec; 0.179 sec/batch)
2016-10-15 01:51:08.879345: step 5650, loss = 0.97 (742.6 examples/sec; 0.172 sec/batch)
2016-10-15 01:51:10.638595: step 5660, loss = 0.92 (724.2 examples/sec; 0.177 sec/batch)
2016-10-15 01:51:12.393328: step 5670, loss = 0.84 (741.6 examples/sec; 0.173 sec/batch)
2016-10-15 01:51:14.156234: step 5680, loss = 0.93 (739.9 examples/sec; 0.173 sec/batch)
2016-10-15 01:51:15.906275: step 5690, loss = 1.10 (727.7 examples/sec; 0.176 sec/batch)
2016-10-15 01:51:17.665380: step 5700, loss = 0.95 (709.2 examples/sec; 0.180 sec/batch)
2016-10-15 01:51:19.601975: step 5710, loss = 0.90 (703.8 examples/sec; 0.182 sec/batch)
2016-10-15 01:51:21.362020: step 5720, loss = 0.92 (760.0 examples/sec; 0.168 sec/batch)
2016-10-15 01:51:23.122276: step 5730, loss = 1.26 (734.1 examples/sec; 0.174 sec/batch)
2016-10-15 01:51:24.879415: step 5740, loss = 1.08 (736.2 examples/sec; 0.174 sec/batch)
2016-10-15 01:51:26.644783: step 5750, loss = 0.94 (717.3 examples/sec; 0.178 sec/batch)
2016-10-15 01:51:28.402658: step 5760, loss = 0.91 (718.4 examples/sec; 0.178 sec/batch)
2016-10-15 01:51:30.164289: step 5770, loss = 1.04 (732.9 examples/sec; 0.175 sec/batch)
2016-10-15 01:51:31.913050: step 5780, loss = 1.10 (722.4 examples/sec; 0.177 sec/batch)
2016-10-15 01:51:33.673014: step 5790, loss = 1.05 (743.1 examples/sec; 0.172 sec/batch)
2016-10-15 01:51:35.435491: step 5800, loss = 0.99 (705.3 examples/sec; 0.181 sec/batch)
2016-10-15 01:51:37.371716: step 5810, loss = 0.87 (740.0 examples/sec; 0.173 sec/batch)
2016-10-15 01:51:39.133285: step 5820, loss = 1.06 (744.3 examples/sec; 0.172 sec/batch)
2016-10-15 01:51:40.905337: step 5830, loss = 0.91 (716.5 examples/sec; 0.179 sec/batch)
2016-10-15 01:51:42.662111: step 5840, loss = 0.98 (747.3 examples/sec; 0.171 sec/batch)
2016-10-15 01:51:44.433458: step 5850, loss = 1.15 (714.4 examples/sec; 0.179 sec/batch)
2016-10-15 01:51:46.198039: step 5860, loss = 0.96 (707.3 examples/sec; 0.181 sec/batch)
2016-10-15 01:51:47.953141: step 5870, loss = 0.87 (737.4 examples/sec; 0.174 sec/batch)
2016-10-15 01:51:49.708472: step 5880, loss = 0.95 (736.4 examples/sec; 0.174 sec/batch)
2016-10-15 01:51:51.470936: step 5890, loss = 1.00 (713.8 examples/sec; 0.179 sec/batch)
2016-10-15 01:51:53.235561: step 5900, loss = 1.06 (708.8 examples/sec; 0.181 sec/batch)
2016-10-15 01:51:55.176404: step 5910, loss = 0.90 (719.7 examples/sec; 0.178 sec/batch)
2016-10-15 01:51:56.937895: step 5920, loss = 1.07 (733.3 examples/sec; 0.175 sec/batch)
2016-10-15 01:51:58.704270: step 5930, loss = 0.98 (710.1 examples/sec; 0.180 sec/batch)
2016-10-15 01:52:00.469191: step 5940, loss = 1.00 (711.9 examples/sec; 0.180 sec/batch)
2016-10-15 01:52:02.227385: step 5950, loss = 1.04 (756.4 examples/sec; 0.169 sec/batch)
2016-10-15 01:52:03.985835: step 5960, loss = 1.03 (737.3 examples/sec; 0.174 sec/batch)
2016-10-15 01:52:05.747723: step 5970, loss = 0.86 (718.8 examples/sec; 0.178 sec/batch)
2016-10-15 01:52:07.504450: step 5980, loss = 0.88 (738.3 examples/sec; 0.173 sec/batch)
2016-10-15 01:52:09.270093: step 5990, loss = 1.13 (712.0 examples/sec; 0.180 sec/batch)
2016-10-15 01:52:11.026643: step 6000, loss = 0.93 (718.8 examples/sec; 0.178 sec/batch)
2016-10-15 01:52:13.545505: step 6010, loss = 1.07 (731.4 examples/sec; 0.175 sec/batch)
2016-10-15 01:52:15.307185: step 6020, loss = 1.01 (725.7 examples/sec; 0.176 sec/batch)
2016-10-15 01:52:17.069942: step 6030, loss = 0.90 (723.6 examples/sec; 0.177 sec/batch)
2016-10-15 01:52:18.835349: step 6040, loss = 1.07 (697.3 examples/sec; 0.184 sec/batch)
2016-10-15 01:52:20.592359: step 6050, loss = 0.97 (712.1 examples/sec; 0.180 sec/batch)
2016-10-15 01:52:22.343680: step 6060, loss = 0.90 (732.5 examples/sec; 0.175 sec/batch)
2016-10-15 01:52:24.104280: step 6070, loss = 1.13 (721.3 examples/sec; 0.177 sec/batch)
2016-10-15 01:52:25.862980: step 6080, loss = 1.08 (717.6 examples/sec; 0.178 sec/batch)
2016-10-15 01:52:27.615693: step 6090, loss = 1.08 (729.2 examples/sec; 0.176 sec/batch)
2016-10-15 01:52:29.380572: step 6100, loss = 1.09 (723.4 examples/sec; 0.177 sec/batch)
2016-10-15 01:52:31.333692: step 6110, loss = 0.92 (707.0 examples/sec; 0.181 sec/batch)
2016-10-15 01:52:33.101538: step 6120, loss = 0.93 (748.3 examples/sec; 0.171 sec/batch)
2016-10-15 01:52:34.864720: step 6130, loss = 0.97 (726.7 examples/sec; 0.176 sec/batch)
2016-10-15 01:52:36.628113: step 6140, loss = 1.04 (738.4 examples/sec; 0.173 sec/batch)
2016-10-15 01:52:38.396952: step 6150, loss = 0.88 (718.3 examples/sec; 0.178 sec/batch)
2016-10-15 01:52:40.156863: step 6160, loss = 0.85 (744.4 examples/sec; 0.172 sec/batch)
2016-10-15 01:52:41.923804: step 6170, loss = 0.89 (719.7 examples/sec; 0.178 sec/batch)
2016-10-15 01:52:43.681724: step 6180, loss = 0.93 (720.4 examples/sec; 0.178 sec/batch)
2016-10-15 01:52:45.453414: step 6190, loss = 0.95 (750.7 examples/sec; 0.171 sec/batch)
2016-10-15 01:52:47.213872: step 6200, loss = 0.84 (728.3 examples/sec; 0.176 sec/batch)
2016-10-15 01:52:49.159335: step 6210, loss = 0.89 (743.2 examples/sec; 0.172 sec/batch)
2016-10-15 01:52:50.916639: step 6220, loss = 0.89 (724.4 examples/sec; 0.177 sec/batch)
2016-10-15 01:52:52.679043: step 6230, loss = 0.81 (728.7 examples/sec; 0.176 sec/batch)
2016-10-15 01:52:54.450033: step 6240, loss = 0.80 (732.5 examples/sec; 0.175 sec/batch)
2016-10-15 01:52:56.219738: step 6250, loss = 0.95 (741.8 examples/sec; 0.173 sec/batch)
2016-10-15 01:52:57.986230: step 6260, loss = 0.91 (722.3 examples/sec; 0.177 sec/batch)
2016-10-15 01:52:59.734209: step 6270, loss = 0.81 (738.0 examples/sec; 0.173 sec/batch)
2016-10-15 01:53:01.500247: step 6280, loss = 1.00 (731.9 examples/sec; 0.175 sec/batch)
2016-10-15 01:53:03.272685: step 6290, loss = 0.97 (709.9 examples/sec; 0.180 sec/batch)
2016-10-15 01:53:05.034004: step 6300, loss = 0.84 (738.5 examples/sec; 0.173 sec/batch)
2016-10-15 01:53:06.986523: step 6310, loss = 0.77 (717.2 examples/sec; 0.178 sec/batch)
2016-10-15 01:53:08.762774: step 6320, loss = 0.85 (716.2 examples/sec; 0.179 sec/batch)
2016-10-15 01:53:10.528791: step 6330, loss = 1.02 (724.7 examples/sec; 0.177 sec/batch)
2016-10-15 01:53:12.290697: step 6340, loss = 1.01 (712.0 examples/sec; 0.180 sec/batch)
2016-10-15 01:53:14.054230: step 6350, loss = 1.02 (736.7 examples/sec; 0.174 sec/batch)
2016-10-15 01:53:15.813338: step 6360, loss = 0.92 (722.4 examples/sec; 0.177 sec/batch)
2016-10-15 01:53:17.577333: step 6370, loss = 0.89 (723.3 examples/sec; 0.177 sec/batch)
2016-10-15 01:53:19.336753: step 6380, loss = 0.87 (742.4 examples/sec; 0.172 sec/batch)
2016-10-15 01:53:21.099560: step 6390, loss = 0.86 (729.3 examples/sec; 0.176 sec/batch)
2016-10-15 01:53:22.852685: step 6400, loss = 0.96 (728.9 examples/sec; 0.176 sec/batch)
2016-10-15 01:53:24.800356: step 6410, loss = 0.92 (709.2 examples/sec; 0.180 sec/batch)
2016-10-15 01:53:26.557095: step 6420, loss = 1.05 (741.7 examples/sec; 0.173 sec/batch)
2016-10-15 01:53:28.319129: step 6430, loss = 0.94 (741.5 examples/sec; 0.173 sec/batch)
2016-10-15 01:53:30.078508: step 6440, loss = 1.00 (734.0 examples/sec; 0.174 sec/batch)
2016-10-15 01:53:31.848548: step 6450, loss = 0.92 (708.0 examples/sec; 0.181 sec/batch)
2016-10-15 01:53:33.613616: step 6460, loss = 0.85 (746.4 examples/sec; 0.171 sec/batch)
2016-10-15 01:53:35.390603: step 6470, loss = 0.97 (707.9 examples/sec; 0.181 sec/batch)
2016-10-15 01:53:37.157294: step 6480, loss = 1.18 (705.0 examples/sec; 0.182 sec/batch)
2016-10-15 01:53:38.909643: step 6490, loss = 1.26 (752.0 examples/sec; 0.170 sec/batch)
2016-10-15 01:53:40.671410: step 6500, loss = 1.01 (728.5 examples/sec; 0.176 sec/batch)
2016-10-15 01:53:42.614199: step 6510, loss = 1.06 (754.6 examples/sec; 0.170 sec/batch)
2016-10-15 01:53:44.375677: step 6520, loss = 1.08 (717.8 examples/sec; 0.178 sec/batch)
2016-10-15 01:53:46.124329: step 6530, loss = 0.89 (734.6 examples/sec; 0.174 sec/batch)
2016-10-15 01:53:47.882947: step 6540, loss = 1.16 (743.9 examples/sec; 0.172 sec/batch)
2016-10-15 01:53:49.642391: step 6550, loss = 0.91 (710.7 examples/sec; 0.180 sec/batch)
2016-10-15 01:53:51.399501: step 6560, loss = 1.05 (734.0 examples/sec; 0.174 sec/batch)
2016-10-15 01:53:53.155613: step 6570, loss = 0.85 (725.4 examples/sec; 0.176 sec/batch)
2016-10-15 01:53:54.912367: step 6580, loss = 0.93 (739.7 examples/sec; 0.173 sec/batch)
2016-10-15 01:53:56.662832: step 6590, loss = 1.09 (718.2 examples/sec; 0.178 sec/batch)
2016-10-15 01:53:58.423190: step 6600, loss = 1.07 (725.9 examples/sec; 0.176 sec/batch)
2016-10-15 01:54:00.370629: step 6610, loss = 0.91 (714.7 examples/sec; 0.179 sec/batch)
2016-10-15 01:54:02.119221: step 6620, loss = 0.91 (740.2 examples/sec; 0.173 sec/batch)
2016-10-15 01:54:03.879829: step 6630, loss = 0.90 (736.1 examples/sec; 0.174 sec/batch)
2016-10-15 01:54:05.634736: step 6640, loss = 0.78 (728.5 examples/sec; 0.176 sec/batch)
2016-10-15 01:54:07.392002: step 6650, loss = 1.02 (748.1 examples/sec; 0.171 sec/batch)
2016-10-15 01:54:09.150424: step 6660, loss = 1.03 (723.1 examples/sec; 0.177 sec/batch)
2016-10-15 01:54:10.902139: step 6670, loss = 0.83 (727.7 examples/sec; 0.176 sec/batch)
2016-10-15 01:54:12.655789: step 6680, loss = 0.89 (733.7 examples/sec; 0.174 sec/batch)
2016-10-15 01:54:14.407922: step 6690, loss = 0.81 (752.2 examples/sec; 0.170 sec/batch)
2016-10-15 01:54:16.162679: step 6700, loss = 0.91 (750.7 examples/sec; 0.170 sec/batch)
2016-10-15 01:54:18.103540: step 6710, loss = 0.94 (713.6 examples/sec; 0.179 sec/batch)
2016-10-15 01:54:19.859891: step 6720, loss = 0.94 (713.7 examples/sec; 0.179 sec/batch)
2016-10-15 01:54:21.611318: step 6730, loss = 0.93 (755.0 examples/sec; 0.170 sec/batch)
2016-10-15 01:54:23.372632: step 6740, loss = 0.83 (729.5 examples/sec; 0.175 sec/batch)
2016-10-15 01:54:25.135022: step 6750, loss = 1.00 (735.0 examples/sec; 0.174 sec/batch)
2016-10-15 01:54:26.891279: step 6760, loss = 0.88 (741.0 examples/sec; 0.173 sec/batch)
2016-10-15 01:54:28.647746: step 6770, loss = 0.96 (701.0 examples/sec; 0.183 sec/batch)
2016-10-15 01:54:30.404525: step 6780, loss = 1.00 (714.3 examples/sec; 0.179 sec/batch)
2016-10-15 01:54:32.167008: step 6790, loss = 0.93 (713.7 examples/sec; 0.179 sec/batch)
2016-10-15 01:54:33.928691: step 6800, loss = 0.79 (717.0 examples/sec; 0.179 sec/batch)
2016-10-15 01:54:35.870607: step 6810, loss = 0.84 (716.7 examples/sec; 0.179 sec/batch)
2016-10-15 01:54:37.635652: step 6820, loss = 0.79 (742.9 examples/sec; 0.172 sec/batch)
2016-10-15 01:54:39.391675: step 6830, loss = 0.97 (743.8 examples/sec; 0.172 sec/batch)
2016-10-15 01:54:41.151493: step 6840, loss = 1.04 (741.6 examples/sec; 0.173 sec/batch)
2016-10-15 01:54:42.913477: step 6850, loss = 0.92 (737.2 examples/sec; 0.174 sec/batch)
2016-10-15 01:54:44.677810: step 6860, loss = 1.00 (699.5 examples/sec; 0.183 sec/batch)
2016-10-15 01:54:46.423410: step 6870, loss = 0.90 (746.8 examples/sec; 0.171 sec/batch)
2016-10-15 01:54:48.186847: step 6880, loss = 0.77 (725.8 examples/sec; 0.176 sec/batch)
2016-10-15 01:54:49.941361: step 6890, loss = 0.75 (725.6 examples/sec; 0.176 sec/batch)
2016-10-15 01:54:51.693015: step 6900, loss = 0.90 (717.4 examples/sec; 0.178 sec/batch)
2016-10-15 01:54:53.648310: step 6910, loss = 1.00 (733.2 examples/sec; 0.175 sec/batch)
2016-10-15 01:54:55.400760: step 6920, loss = 0.95 (730.1 examples/sec; 0.175 sec/batch)
2016-10-15 01:54:57.163885: step 6930, loss = 0.81 (716.0 examples/sec; 0.179 sec/batch)
2016-10-15 01:54:58.927303: step 6940, loss = 0.86 (767.0 examples/sec; 0.167 sec/batch)
2016-10-15 01:55:00.684787: step 6950, loss = 0.94 (740.3 examples/sec; 0.173 sec/batch)
2016-10-15 01:55:02.453644: step 6960, loss = 1.05 (746.9 examples/sec; 0.171 sec/batch)
2016-10-15 01:55:04.231751: step 6970, loss = 0.88 (708.9 examples/sec; 0.181 sec/batch)
2016-10-15 01:55:05.992863: step 6980, loss = 0.82 (713.6 examples/sec; 0.179 sec/batch)
2016-10-15 01:55:07.758525: step 6990, loss = 1.01 (706.9 examples/sec; 0.181 sec/batch)
2016-10-15 01:55:09.518623: step 7000, loss = 0.94 (732.4 examples/sec; 0.175 sec/batch)
2016-10-15 01:55:12.028328: step 7010, loss = 0.87 (751.4 examples/sec; 0.170 sec/batch)
2016-10-15 01:55:13.802649: step 7020, loss = 0.87 (715.6 examples/sec; 0.179 sec/batch)
2016-10-15 01:55:15.570226: step 7030, loss = 0.95 (710.9 examples/sec; 0.180 sec/batch)
2016-10-15 01:55:17.330537: step 7040, loss = 0.94 (738.6 examples/sec; 0.173 sec/batch)
2016-10-15 01:55:19.085013: step 7050, loss = 0.95 (721.4 examples/sec; 0.177 sec/batch)
2016-10-15 01:55:20.852640: step 7060, loss = 0.97 (715.6 examples/sec; 0.179 sec/batch)
2016-10-15 01:55:22.613352: step 7070, loss = 0.98 (718.5 examples/sec; 0.178 sec/batch)
2016-10-15 01:55:24.371753: step 7080, loss = 1.07 (717.4 examples/sec; 0.178 sec/batch)
2016-10-15 01:55:26.128796: step 7090, loss = 0.89 (716.1 examples/sec; 0.179 sec/batch)
2016-10-15 01:55:27.890615: step 7100, loss = 1.01 (725.4 examples/sec; 0.176 sec/batch)
2016-10-15 01:55:29.834263: step 7110, loss = 0.92 (738.6 examples/sec; 0.173 sec/batch)
2016-10-15 01:55:31.603947: step 7120, loss = 0.89 (702.7 examples/sec; 0.182 sec/batch)
2016-10-15 01:55:33.357382: step 7130, loss = 0.92 (735.7 examples/sec; 0.174 sec/batch)
2016-10-15 01:55:35.112084: step 7140, loss = 0.85 (733.3 examples/sec; 0.175 sec/batch)
2016-10-15 01:55:36.864054: step 7150, loss = 0.91 (744.4 examples/sec; 0.172 sec/batch)
2016-10-15 01:55:38.618815: step 7160, loss = 0.96 (730.3 examples/sec; 0.175 sec/batch)
2016-10-15 01:55:40.375500: step 7170, loss = 0.96 (742.0 examples/sec; 0.173 sec/batch)
2016-10-15 01:55:42.138589: step 7180, loss = 1.25 (727.1 examples/sec; 0.176 sec/batch)
2016-10-15 01:55:43.894726: step 7190, loss = 0.88 (727.8 examples/sec; 0.176 sec/batch)
2016-10-15 01:55:45.654347: step 7200, loss = 0.96 (743.6 examples/sec; 0.172 sec/batch)
2016-10-15 01:55:47.591655: step 7210, loss = 1.06 (745.7 examples/sec; 0.172 sec/batch)
2016-10-15 01:55:49.350680: step 7220, loss = 0.81 (718.0 examples/sec; 0.178 sec/batch)
2016-10-15 01:55:51.105803: step 7230, loss = 0.94 (717.2 examples/sec; 0.178 sec/batch)
2016-10-15 01:55:52.861267: step 7240, loss = 0.96 (725.5 examples/sec; 0.176 sec/batch)
2016-10-15 01:55:54.623177: step 7250, loss = 0.90 (710.9 examples/sec; 0.180 sec/batch)
2016-10-15 01:55:56.377635: step 7260, loss = 1.03 (710.9 examples/sec; 0.180 sec/batch)
2016-10-15 01:55:58.137163: step 7270, loss = 0.85 (745.5 examples/sec; 0.172 sec/batch)
2016-10-15 01:55:59.902954: step 7280, loss = 0.94 (735.7 examples/sec; 0.174 sec/batch)
2016-10-15 01:56:01.666527: step 7290, loss = 1.26 (728.6 examples/sec; 0.176 sec/batch)
2016-10-15 01:56:03.426547: step 7300, loss = 0.98 (733.5 examples/sec; 0.175 sec/batch)
2016-10-15 01:56:05.371535: step 7310, loss = 1.00 (718.9 examples/sec; 0.178 sec/batch)
2016-10-15 01:56:07.131672: step 7320, loss = 0.92 (738.5 examples/sec; 0.173 sec/batch)
2016-10-15 01:56:08.892459: step 7330, loss = 0.96 (705.6 examples/sec; 0.181 sec/batch)
2016-10-15 01:56:10.650751: step 7340, loss = 0.88 (754.1 examples/sec; 0.170 sec/batch)
2016-10-15 01:56:12.413124: step 7350, loss = 1.07 (717.4 examples/sec; 0.178 sec/batch)
2016-10-15 01:56:14.170080: step 7360, loss = 0.94 (742.1 examples/sec; 0.172 sec/batch)
2016-10-15 01:56:15.927535: step 7370, loss = 0.95 (719.4 examples/sec; 0.178 sec/batch)
2016-10-15 01:56:17.684719: step 7380, loss = 0.86 (734.7 examples/sec; 0.174 sec/batch)
2016-10-15 01:56:19.441631: step 7390, loss = 0.81 (735.9 examples/sec; 0.174 sec/batch)
2016-10-15 01:56:21.200333: step 7400, loss = 0.99 (716.5 examples/sec; 0.179 sec/batch)
2016-10-15 01:56:23.138601: step 7410, loss = 0.98 (724.4 examples/sec; 0.177 sec/batch)
2016-10-15 01:56:24.901169: step 7420, loss = 0.94 (717.9 examples/sec; 0.178 sec/batch)
2016-10-15 01:56:26.662249: step 7430, loss = 1.03 (740.3 examples/sec; 0.173 sec/batch)
2016-10-15 01:56:28.432834: step 7440, loss = 1.06 (718.4 examples/sec; 0.178 sec/batch)
2016-10-15 01:56:30.182907: step 7450, loss = 1.00 (741.0 examples/sec; 0.173 sec/batch)
2016-10-15 01:56:31.956836: step 7460, loss = 0.87 (711.8 examples/sec; 0.180 sec/batch)
2016-10-15 01:56:33.710340: step 7470, loss = 0.89 (733.5 examples/sec; 0.175 sec/batch)
2016-10-15 01:56:35.475896: step 7480, loss = 0.88 (730.5 examples/sec; 0.175 sec/batch)
2016-10-15 01:56:37.237426: step 7490, loss = 0.94 (724.4 examples/sec; 0.177 sec/batch)
2016-10-15 01:56:38.996282: step 7500, loss = 0.89 (729.6 examples/sec; 0.175 sec/batch)
2016-10-15 01:56:40.942265: step 7510, loss = 1.07 (738.5 examples/sec; 0.173 sec/batch)
2016-10-15 01:56:42.700003: step 7520, loss = 0.94 (755.7 examples/sec; 0.169 sec/batch)
2016-10-15 01:56:44.455612: step 7530, loss = 0.71 (707.2 examples/sec; 0.181 sec/batch)
2016-10-15 01:56:46.213262: step 7540, loss = 0.88 (724.9 examples/sec; 0.177 sec/batch)
2016-10-15 01:56:47.968848: step 7550, loss = 0.85 (785.2 examples/sec; 0.163 sec/batch)
2016-10-15 01:56:49.730747: step 7560, loss = 0.93 (723.6 examples/sec; 0.177 sec/batch)
2016-10-15 01:56:51.486061: step 7570, loss = 1.08 (720.7 examples/sec; 0.178 sec/batch)
2016-10-15 01:56:53.253509: step 7580, loss = 1.06 (740.5 examples/sec; 0.173 sec/batch)
2016-10-15 01:56:55.008841: step 7590, loss = 1.05 (740.2 examples/sec; 0.173 sec/batch)
2016-10-15 01:56:56.765445: step 7600, loss = 1.01 (712.0 examples/sec; 0.180 sec/batch)
2016-10-15 01:56:58.761268: step 7610, loss = 0.94 (734.5 examples/sec; 0.174 sec/batch)
2016-10-15 01:57:00.524523: step 7620, loss = 1.09 (728.6 examples/sec; 0.176 sec/batch)
2016-10-15 01:57:02.294766: step 7630, loss = 1.24 (717.9 examples/sec; 0.178 sec/batch)
2016-10-15 01:57:04.050443: step 7640, loss = 1.04 (749.9 examples/sec; 0.171 sec/batch)
2016-10-15 01:57:05.813486: step 7650, loss = 0.91 (708.4 examples/sec; 0.181 sec/batch)
2016-10-15 01:57:07.573504: step 7660, loss = 0.90 (725.4 examples/sec; 0.176 sec/batch)
2016-10-15 01:57:09.333508: step 7670, loss = 1.04 (724.5 examples/sec; 0.177 sec/batch)
2016-10-15 01:57:11.092006: step 7680, loss = 0.83 (718.3 examples/sec; 0.178 sec/batch)
2016-10-15 01:57:12.859173: step 7690, loss = 0.81 (733.9 examples/sec; 0.174 sec/batch)
2016-10-15 01:57:14.615979: step 7700, loss = 1.08 (708.5 examples/sec; 0.181 sec/batch)
2016-10-15 01:57:16.551462: step 7710, loss = 0.89 (738.2 examples/sec; 0.173 sec/batch)
2016-10-15 01:57:18.305913: step 7720, loss = 1.06 (734.8 examples/sec; 0.174 sec/batch)
2016-10-15 01:57:20.065272: step 7730, loss = 0.83 (723.0 examples/sec; 0.177 sec/batch)
2016-10-15 01:57:21.821654: step 7740, loss = 0.86 (746.1 examples/sec; 0.172 sec/batch)
2016-10-15 01:57:23.580122: step 7750, loss = 0.92 (738.0 examples/sec; 0.173 sec/batch)
2016-10-15 01:57:25.341452: step 7760, loss = 0.83 (763.5 examples/sec; 0.168 sec/batch)
2016-10-15 01:57:27.098920: step 7770, loss = 0.87 (723.1 examples/sec; 0.177 sec/batch)
2016-10-15 01:57:28.857426: step 7780, loss = 0.87 (749.8 examples/sec; 0.171 sec/batch)
2016-10-15 01:57:30.620362: step 7790, loss = 0.87 (714.0 examples/sec; 0.179 sec/batch)
2016-10-15 01:57:32.383082: step 7800, loss = 1.04 (727.7 examples/sec; 0.176 sec/batch)
2016-10-15 01:57:34.336782: step 7810, loss = 0.95 (734.3 examples/sec; 0.174 sec/batch)
2016-10-15 01:57:36.094872: step 7820, loss = 0.89 (735.1 examples/sec; 0.174 sec/batch)
2016-10-15 01:57:37.857235: step 7830, loss = 1.06 (733.6 examples/sec; 0.174 sec/batch)
2016-10-15 01:57:39.617012: step 7840, loss = 0.98 (724.6 examples/sec; 0.177 sec/batch)
2016-10-15 01:57:41.377298: step 7850, loss = 0.95 (735.6 examples/sec; 0.174 sec/batch)
2016-10-15 01:57:43.124282: step 7860, loss = 0.93 (756.5 examples/sec; 0.169 sec/batch)
2016-10-15 01:57:44.876477: step 7870, loss = 0.88 (743.5 examples/sec; 0.172 sec/batch)
2016-10-15 01:57:46.628098: step 7880, loss = 0.91 (741.6 examples/sec; 0.173 sec/batch)
2016-10-15 01:57:48.387801: step 7890, loss = 1.16 (771.1 examples/sec; 0.166 sec/batch)
2016-10-15 01:57:50.144571: step 7900, loss = 1.02 (738.6 examples/sec; 0.173 sec/batch)
2016-10-15 01:57:52.092326: step 7910, loss = 0.99 (750.8 examples/sec; 0.170 sec/batch)
2016-10-15 01:57:53.854797: step 7920, loss = 1.04 (724.0 examples/sec; 0.177 sec/batch)
2016-10-15 01:57:55.611012: step 7930, loss = 1.04 (734.5 examples/sec; 0.174 sec/batch)
2016-10-15 01:57:57.373353: step 7940, loss = 0.80 (730.0 examples/sec; 0.175 sec/batch)
2016-10-15 01:57:59.135950: step 7950, loss = 0.79 (728.1 examples/sec; 0.176 sec/batch)
2016-10-15 01:58:00.894493: step 7960, loss = 0.92 (739.9 examples/sec; 0.173 sec/batch)
2016-10-15 01:58:02.659171: step 7970, loss = 1.03 (723.3 examples/sec; 0.177 sec/batch)
2016-10-15 01:58:04.423585: step 7980, loss = 0.89 (686.0 examples/sec; 0.187 sec/batch)
2016-10-15 01:58:06.177252: step 7990, loss = 0.96 (740.4 examples/sec; 0.173 sec/batch)
2016-10-15 01:58:07.928658: step 8000, loss = 0.80 (718.2 examples/sec; 0.178 sec/batch)
2016-10-15 01:58:10.422455: step 8010, loss = 0.92 (726.6 examples/sec; 0.176 sec/batch)
2016-10-15 01:58:12.178266: step 8020, loss = 0.95 (738.0 examples/sec; 0.173 sec/batch)
2016-10-15 01:58:13.943875: step 8030, loss = 0.86 (723.1 examples/sec; 0.177 sec/batch)
2016-10-15 01:58:15.696492: step 8040, loss = 0.88 (732.5 examples/sec; 0.175 sec/batch)
2016-10-15 01:58:17.461998: step 8050, loss = 0.95 (740.1 examples/sec; 0.173 sec/batch)
2016-10-15 01:58:19.221746: step 8060, loss = 1.31 (723.9 examples/sec; 0.177 sec/batch)
2016-10-15 01:58:20.982718: step 8070, loss = 0.93 (692.2 examples/sec; 0.185 sec/batch)
2016-10-15 01:58:22.735846: step 8080, loss = 0.74 (716.0 examples/sec; 0.179 sec/batch)
2016-10-15 01:58:24.506230: step 8090, loss = 0.90 (703.0 examples/sec; 0.182 sec/batch)
2016-10-15 01:58:26.267855: step 8100, loss = 0.96 (717.9 examples/sec; 0.178 sec/batch)
2016-10-15 01:58:28.219035: step 8110, loss = 0.93 (694.9 examples/sec; 0.184 sec/batch)
2016-10-15 01:58:29.961858: step 8120, loss = 0.86 (737.2 examples/sec; 0.174 sec/batch)
2016-10-15 01:58:31.733211: step 8130, loss = 1.04 (692.4 examples/sec; 0.185 sec/batch)
2016-10-15 01:58:33.475753: step 8140, loss = 0.88 (717.8 examples/sec; 0.178 sec/batch)
2016-10-15 01:58:35.244306: step 8150, loss = 0.73 (704.6 examples/sec; 0.182 sec/batch)
2016-10-15 01:58:37.008867: step 8160, loss = 0.91 (695.3 examples/sec; 0.184 sec/batch)
2016-10-15 01:58:38.770539: step 8170, loss = 0.79 (744.4 examples/sec; 0.172 sec/batch)
2016-10-15 01:58:40.528970: step 8180, loss = 0.97 (743.1 examples/sec; 0.172 sec/batch)
2016-10-15 01:58:42.288323: step 8190, loss = 1.03 (759.8 examples/sec; 0.168 sec/batch)
2016-10-15 01:58:44.057105: step 8200, loss = 0.89 (710.0 examples/sec; 0.180 sec/batch)
2016-10-15 01:58:45.994362: step 8210, loss = 0.86 (725.7 examples/sec; 0.176 sec/batch)
2016-10-15 01:58:47.746701: step 8220, loss = 0.91 (768.9 examples/sec; 0.166 sec/batch)
2016-10-15 01:58:49.507119: step 8230, loss = 0.82 (743.5 examples/sec; 0.172 sec/batch)
2016-10-15 01:58:51.271695: step 8240, loss = 0.85 (689.2 examples/sec; 0.186 sec/batch)
2016-10-15 01:58:53.038705: step 8250, loss = 1.04 (711.1 examples/sec; 0.180 sec/batch)
2016-10-15 01:58:54.792668: step 8260, loss = 0.85 (730.4 examples/sec; 0.175 sec/batch)
2016-10-15 01:58:56.553388: step 8270, loss = 1.01 (768.1 examples/sec; 0.167 sec/batch)
2016-10-15 01:58:58.323556: step 8280, loss = 1.13 (710.6 examples/sec; 0.180 sec/batch)
2016-10-15 01:59:00.072444: step 8290, loss = 1.10 (735.6 examples/sec; 0.174 sec/batch)
2016-10-15 01:59:01.822987: step 8300, loss = 0.99 (747.2 examples/sec; 0.171 sec/batch)
2016-10-15 01:59:03.788234: step 8310, loss = 0.80 (717.6 examples/sec; 0.178 sec/batch)
2016-10-15 01:59:05.548753: step 8320, loss = 1.03 (733.3 examples/sec; 0.175 sec/batch)
2016-10-15 01:59:07.307456: step 8330, loss = 0.95 (741.2 examples/sec; 0.173 sec/batch)
2016-10-15 01:59:09.070527: step 8340, loss = 0.93 (693.0 examples/sec; 0.185 sec/batch)
2016-10-15 01:59:10.819822: step 8350, loss = 1.10 (760.1 examples/sec; 0.168 sec/batch)
2016-10-15 01:59:12.589213: step 8360, loss = 0.94 (709.7 examples/sec; 0.180 sec/batch)
2016-10-15 01:59:14.343090: step 8370, loss = 0.95 (726.7 examples/sec; 0.176 sec/batch)
2016-10-15 01:59:16.100474: step 8380, loss = 1.09 (726.3 examples/sec; 0.176 sec/batch)
2016-10-15 01:59:17.853082: step 8390, loss = 0.88 (737.1 examples/sec; 0.174 sec/batch)
2016-10-15 01:59:19.616086: step 8400, loss = 0.92 (729.6 examples/sec; 0.175 sec/batch)
2016-10-15 01:59:21.556582: step 8410, loss = 1.05 (748.7 examples/sec; 0.171 sec/batch)
2016-10-15 01:59:23.311823: step 8420, loss = 0.78 (730.7 examples/sec; 0.175 sec/batch)
2016-10-15 01:59:25.074161: step 8430, loss = 0.79 (735.4 examples/sec; 0.174 sec/batch)
2016-10-15 01:59:26.828579: step 8440, loss = 0.88 (742.8 examples/sec; 0.172 sec/batch)
2016-10-15 01:59:28.592764: step 8450, loss = 0.93 (718.6 examples/sec; 0.178 sec/batch)
2016-10-15 01:59:30.341716: step 8460, loss = 0.89 (724.8 examples/sec; 0.177 sec/batch)
2016-10-15 01:59:32.103542: step 8470, loss = 0.84 (737.6 examples/sec; 0.174 sec/batch)
2016-10-15 01:59:33.862742: step 8480, loss = 0.94 (754.6 examples/sec; 0.170 sec/batch)
2016-10-15 01:59:35.614824: step 8490, loss = 1.02 (782.3 examples/sec; 0.164 sec/batch)
2016-10-15 01:59:37.382667: step 8500, loss = 0.69 (729.2 examples/sec; 0.176 sec/batch)
2016-10-15 01:59:39.317908: step 8510, loss = 0.94 (724.0 examples/sec; 0.177 sec/batch)
2016-10-15 01:59:41.085729: step 8520, loss = 1.00 (697.1 examples/sec; 0.184 sec/batch)
2016-10-15 01:59:42.836530: step 8530, loss = 0.93 (751.2 examples/sec; 0.170 sec/batch)
2016-10-15 01:59:44.592549: step 8540, loss = 0.89 (721.9 examples/sec; 0.177 sec/batch)
2016-10-15 01:59:46.360150: step 8550, loss = 0.97 (725.2 examples/sec; 0.176 sec/batch)
2016-10-15 01:59:48.127178: step 8560, loss = 0.79 (714.0 examples/sec; 0.179 sec/batch)
2016-10-15 01:59:49.884543: step 8570, loss = 1.07 (751.5 examples/sec; 0.170 sec/batch)
2016-10-15 01:59:51.646602: step 8580, loss = 0.86 (730.0 examples/sec; 0.175 sec/batch)
2016-10-15 01:59:53.414521: step 8590, loss = 1.01 (706.2 examples/sec; 0.181 sec/batch)
2016-10-15 01:59:55.180097: step 8600, loss = 0.75 (724.3 examples/sec; 0.177 sec/batch)
2016-10-15 01:59:57.122354: step 8610, loss = 0.87 (746.1 examples/sec; 0.172 sec/batch)
2016-10-15 01:59:58.883245: step 8620, loss = 0.96 (768.4 examples/sec; 0.167 sec/batch)
2016-10-15 02:00:00.653664: step 8630, loss = 0.87 (707.2 examples/sec; 0.181 sec/batch)
2016-10-15 02:00:02.452637: step 8640, loss = 0.83 (730.3 examples/sec; 0.175 sec/batch)
2016-10-15 02:00:04.213673: step 8650, loss = 0.90 (723.3 examples/sec; 0.177 sec/batch)
2016-10-15 02:00:05.986525: step 8660, loss = 0.95 (732.6 examples/sec; 0.175 sec/batch)
2016-10-15 02:00:07.744281: step 8670, loss = 0.94 (745.8 examples/sec; 0.172 sec/batch)
2016-10-15 02:00:09.504857: step 8680, loss = 0.90 (709.3 examples/sec; 0.180 sec/batch)
2016-10-15 02:00:11.261098: step 8690, loss = 0.91 (738.8 examples/sec; 0.173 sec/batch)
2016-10-15 02:00:13.020874: step 8700, loss = 0.89 (719.1 examples/sec; 0.178 sec/batch)
2016-10-15 02:00:14.966483: step 8710, loss = 0.90 (727.9 examples/sec; 0.176 sec/batch)
2016-10-15 02:00:16.726716: step 8720, loss = 0.87 (732.5 examples/sec; 0.175 sec/batch)
2016-10-15 02:00:18.483755: step 8730, loss = 0.86 (734.2 examples/sec; 0.174 sec/batch)
2016-10-15 02:00:20.245660: step 8740, loss = 0.90 (721.9 examples/sec; 0.177 sec/batch)
2016-10-15 02:00:22.004974: step 8750, loss = 0.95 (717.2 examples/sec; 0.178 sec/batch)
2016-10-15 02:00:23.768672: step 8760, loss = 0.72 (703.1 examples/sec; 0.182 sec/batch)
2016-10-15 02:00:25.538463: step 8770, loss = 1.03 (685.5 examples/sec; 0.187 sec/batch)
2016-10-15 02:00:27.284555: step 8780, loss = 0.93 (729.8 examples/sec; 0.175 sec/batch)
2016-10-15 02:00:29.043453: step 8790, loss = 0.98 (701.3 examples/sec; 0.183 sec/batch)
2016-10-15 02:00:30.793030: step 8800, loss = 0.85 (722.1 examples/sec; 0.177 sec/batch)
2016-10-15 02:00:32.741460: step 8810, loss = 0.74 (726.9 examples/sec; 0.176 sec/batch)
2016-10-15 02:00:34.499139: step 8820, loss = 0.77 (719.9 examples/sec; 0.178 sec/batch)
2016-10-15 02:00:36.254840: step 8830, loss = 0.92 (732.3 examples/sec; 0.175 sec/batch)
2016-10-15 02:00:38.021891: step 8840, loss = 0.75 (708.3 examples/sec; 0.181 sec/batch)
2016-10-15 02:00:39.775662: step 8850, loss = 0.82 (717.2 examples/sec; 0.178 sec/batch)
2016-10-15 02:00:41.526264: step 8860, loss = 0.68 (741.7 examples/sec; 0.173 sec/batch)
2016-10-15 02:00:43.286634: step 8870, loss = 0.94 (734.0 examples/sec; 0.174 sec/batch)
2016-10-15 02:00:45.047952: step 8880, loss = 0.76 (706.3 examples/sec; 0.181 sec/batch)
2016-10-15 02:00:46.804068: step 8890, loss = 0.91 (744.6 examples/sec; 0.172 sec/batch)
2016-10-15 02:00:48.559598: step 8900, loss = 0.90 (738.5 examples/sec; 0.173 sec/batch)
2016-10-15 02:00:50.502776: step 8910, loss = 0.84 (739.9 examples/sec; 0.173 sec/batch)
2016-10-15 02:00:52.265013: step 8920, loss = 0.85 (744.7 examples/sec; 0.172 sec/batch)
2016-10-15 02:00:54.029372: step 8930, loss = 0.81 (715.7 examples/sec; 0.179 sec/batch)
2016-10-15 02:00:55.787849: step 8940, loss = 0.74 (752.5 examples/sec; 0.170 sec/batch)
2016-10-15 02:00:57.552032: step 8950, loss = 0.66 (698.9 examples/sec; 0.183 sec/batch)
2016-10-15 02:00:59.312672: step 8960, loss = 0.84 (707.2 examples/sec; 0.181 sec/batch)
2016-10-15 02:01:01.071575: step 8970, loss = 0.79 (715.1 examples/sec; 0.179 sec/batch)
2016-10-15 02:01:02.836076: step 8980, loss = 0.85 (715.4 examples/sec; 0.179 sec/batch)
2016-10-15 02:01:04.606741: step 8990, loss = 0.88 (713.0 examples/sec; 0.180 sec/batch)
2016-10-15 02:01:06.372080: step 9000, loss = 0.83 (718.0 examples/sec; 0.178 sec/batch)
2016-10-15 02:01:08.838372: step 9010, loss = 0.98 (725.7 examples/sec; 0.176 sec/batch)
2016-10-15 02:01:10.596013: step 9020, loss = 0.84 (754.8 examples/sec; 0.170 sec/batch)
2016-10-15 02:01:12.367782: step 9030, loss = 1.23 (713.1 examples/sec; 0.179 sec/batch)
2016-10-15 02:01:14.130963: step 9040, loss = 0.93 (722.6 examples/sec; 0.177 sec/batch)
2016-10-15 02:01:15.896315: step 9050, loss = 0.85 (736.4 examples/sec; 0.174 sec/batch)
2016-10-15 02:01:17.663761: step 9060, loss = 0.88 (721.8 examples/sec; 0.177 sec/batch)
2016-10-15 02:01:19.422106: step 9070, loss = 0.83 (715.8 examples/sec; 0.179 sec/batch)
2016-10-15 02:01:21.185916: step 9080, loss = 1.05 (714.4 examples/sec; 0.179 sec/batch)
2016-10-15 02:01:22.948202: step 9090, loss = 1.19 (720.7 examples/sec; 0.178 sec/batch)
2016-10-15 02:01:24.698512: step 9100, loss = 0.96 (743.4 examples/sec; 0.172 sec/batch)
2016-10-15 02:01:26.636547: step 9110, loss = 0.78 (748.6 examples/sec; 0.171 sec/batch)
2016-10-15 02:01:28.405561: step 9120, loss = 0.97 (724.6 examples/sec; 0.177 sec/batch)
2016-10-15 02:01:30.183067: step 9130, loss = 0.87 (675.0 examples/sec; 0.190 sec/batch)
2016-10-15 02:01:31.936612: step 9140, loss = 0.97 (728.5 examples/sec; 0.176 sec/batch)
2016-10-15 02:01:33.704737: step 9150, loss = 0.83 (709.1 examples/sec; 0.181 sec/batch)
2016-10-15 02:01:35.476104: step 9160, loss = 1.07 (703.3 examples/sec; 0.182 sec/batch)
2016-10-15 02:01:37.244000: step 9170, loss = 1.04 (741.7 examples/sec; 0.173 sec/batch)
2016-10-15 02:01:39.010028: step 9180, loss = 0.97 (717.6 examples/sec; 0.178 sec/batch)
2016-10-15 02:01:40.777293: step 9190, loss = 0.99 (716.0 examples/sec; 0.179 sec/batch)
2016-10-15 02:01:42.532561: step 9200, loss = 0.89 (730.2 examples/sec; 0.175 sec/batch)
2016-10-15 02:01:44.479234: step 9210, loss = 0.99 (714.7 examples/sec; 0.179 sec/batch)
2016-10-15 02:01:46.241019: step 9220, loss = 0.92 (733.8 examples/sec; 0.174 sec/batch)
2016-10-15 02:01:48.002206: step 9230, loss = 0.82 (720.4 examples/sec; 0.178 sec/batch)
2016-10-15 02:01:49.753351: step 9240, loss = 0.96 (732.8 examples/sec; 0.175 sec/batch)
2016-10-15 02:01:51.520616: step 9250, loss = 0.79 (726.6 examples/sec; 0.176 sec/batch)
2016-10-15 02:01:53.297897: step 9260, loss = 0.87 (736.2 examples/sec; 0.174 sec/batch)
2016-10-15 02:01:55.055825: step 9270, loss = 0.88 (729.4 examples/sec; 0.175 sec/batch)
2016-10-15 02:01:56.828481: step 9280, loss = 0.77 (706.7 examples/sec; 0.181 sec/batch)
2016-10-15 02:01:58.579524: step 9290, loss = 0.86 (723.6 examples/sec; 0.177 sec/batch)
2016-10-15 02:02:00.334496: step 9300, loss = 0.75 (740.1 examples/sec; 0.173 sec/batch)
2016-10-15 02:02:02.302088: step 9310, loss = 0.77 (716.3 examples/sec; 0.179 sec/batch)
2016-10-15 02:02:04.057007: step 9320, loss = 0.93 (733.4 examples/sec; 0.175 sec/batch)
2016-10-15 02:02:05.815827: step 9330, loss = 0.94 (719.3 examples/sec; 0.178 sec/batch)
2016-10-15 02:02:07.582333: step 9340, loss = 0.98 (733.3 examples/sec; 0.175 sec/batch)
2016-10-15 02:02:09.345354: step 9350, loss = 0.85 (730.6 examples/sec; 0.175 sec/batch)
2016-10-15 02:02:11.116813: step 9360, loss = 0.89 (707.8 examples/sec; 0.181 sec/batch)
2016-10-15 02:02:12.867811: step 9370, loss = 0.96 (726.7 examples/sec; 0.176 sec/batch)
2016-10-15 02:02:14.627271: step 9380, loss = 0.81 (730.2 examples/sec; 0.175 sec/batch)
2016-10-15 02:02:16.378365: step 9390, loss = 1.15 (715.3 examples/sec; 0.179 sec/batch)
2016-10-15 02:02:18.131935: step 9400, loss = 0.91 (724.3 examples/sec; 0.177 sec/batch)
2016-10-15 02:02:20.068319: step 9410, loss = 0.87 (723.5 examples/sec; 0.177 sec/batch)
2016-10-15 02:02:21.834716: step 9420, loss = 0.89 (702.0 examples/sec; 0.182 sec/batch)
2016-10-15 02:02:23.589942: step 9430, loss = 0.80 (737.4 examples/sec; 0.174 sec/batch)
2016-10-15 02:02:25.352585: step 9440, loss = 1.05 (731.3 examples/sec; 0.175 sec/batch)
2016-10-15 02:02:27.114419: step 9450, loss = 0.94 (700.7 examples/sec; 0.183 sec/batch)
2016-10-15 02:02:28.870919: step 9460, loss = 0.80 (724.1 examples/sec; 0.177 sec/batch)
2016-10-15 02:02:30.629222: step 9470, loss = 0.85 (719.7 examples/sec; 0.178 sec/batch)
2016-10-15 02:02:32.393037: step 9480, loss = 0.83 (714.9 examples/sec; 0.179 sec/batch)
2016-10-15 02:02:34.154832: step 9490, loss = 0.80 (713.8 examples/sec; 0.179 sec/batch)
2016-10-15 02:02:35.910285: step 9500, loss = 0.94 (742.9 examples/sec; 0.172 sec/batch)
2016-10-15 02:02:37.853820: step 9510, loss = 0.87 (726.5 examples/sec; 0.176 sec/batch)
2016-10-15 02:02:39.620217: step 9520, loss = 0.87 (721.1 examples/sec; 0.178 sec/batch)
2016-10-15 02:02:41.379862: step 9530, loss = 0.87 (722.4 examples/sec; 0.177 sec/batch)
2016-10-15 02:02:43.147458: step 9540, loss = 1.10 (694.7 examples/sec; 0.184 sec/batch)
2016-10-15 02:02:44.899535: step 9550, loss = 0.85 (740.4 examples/sec; 0.173 sec/batch)
2016-10-15 02:02:46.659196: step 9560, loss = 0.85 (710.8 examples/sec; 0.180 sec/batch)
2016-10-15 02:02:48.415298: step 9570, loss = 0.75 (711.2 examples/sec; 0.180 sec/batch)
2016-10-15 02:02:50.161730: step 9580, loss = 0.84 (745.6 examples/sec; 0.172 sec/batch)
2016-10-15 02:02:51.920523: step 9590, loss = 0.90 (747.9 examples/sec; 0.171 sec/batch)
2016-10-15 02:02:53.684798: step 9600, loss = 0.99 (731.5 examples/sec; 0.175 sec/batch)
2016-10-15 02:02:55.623611: step 9610, loss = 0.83 (729.3 examples/sec; 0.175 sec/batch)
2016-10-15 02:02:57.371698: step 9620, loss = 1.02 (739.8 examples/sec; 0.173 sec/batch)
2016-10-15 02:02:59.128345: step 9630, loss = 0.84 (733.7 examples/sec; 0.174 sec/batch)
2016-10-15 02:03:00.886037: step 9640, loss = 0.81 (698.4 examples/sec; 0.183 sec/batch)
2016-10-15 02:03:02.634520: step 9650, loss = 0.73 (722.3 examples/sec; 0.177 sec/batch)
2016-10-15 02:03:04.373923: step 9660, loss = 0.96 (737.5 examples/sec; 0.174 sec/batch)
2016-10-15 02:03:06.138310: step 9670, loss = 0.74 (706.0 examples/sec; 0.181 sec/batch)
2016-10-15 02:03:07.898958: step 9680, loss = 0.98 (698.9 examples/sec; 0.183 sec/batch)
2016-10-15 02:03:09.646578: step 9690, loss = 0.82 (735.7 examples/sec; 0.174 sec/batch)
2016-10-15 02:03:11.415630: step 9700, loss = 0.84 (714.2 examples/sec; 0.179 sec/batch)
2016-10-15 02:03:13.348966: step 9710, loss = 0.72 (732.8 examples/sec; 0.175 sec/batch)
2016-10-15 02:03:15.109804: step 9720, loss = 1.01 (731.9 examples/sec; 0.175 sec/batch)
2016-10-15 02:03:16.880392: step 9730, loss = 0.86 (699.2 examples/sec; 0.183 sec/batch)
2016-10-15 02:03:18.635191: step 9740, loss = 0.86 (733.6 examples/sec; 0.174 sec/batch)
2016-10-15 02:03:20.395780: step 9750, loss = 1.05 (712.0 examples/sec; 0.180 sec/batch)
2016-10-15 02:03:22.149086: step 9760, loss = 0.89 (725.5 examples/sec; 0.176 sec/batch)
2016-10-15 02:03:23.908794: step 9770, loss = 0.98 (730.3 examples/sec; 0.175 sec/batch)
2016-10-15 02:03:25.662793: step 9780, loss = 0.86 (737.4 examples/sec; 0.174 sec/batch)
2016-10-15 02:03:27.427301: step 9790, loss = 0.87 (712.7 examples/sec; 0.180 sec/batch)
2016-10-15 02:03:29.192577: step 9800, loss = 0.83 (714.8 examples/sec; 0.179 sec/batch)
2016-10-15 02:03:31.129199: step 9810, loss = 1.01 (748.2 examples/sec; 0.171 sec/batch)
2016-10-15 02:03:32.885252: step 9820, loss = 1.02 (735.7 examples/sec; 0.174 sec/batch)
2016-10-15 02:03:34.654846: step 9830, loss = 0.94 (712.2 examples/sec; 0.180 sec/batch)
2016-10-15 02:03:36.406818: step 9840, loss = 0.85 (725.3 examples/sec; 0.176 sec/batch)
2016-10-15 02:03:38.172506: step 9850, loss = 0.97 (693.4 examples/sec; 0.185 sec/batch)
2016-10-15 02:03:39.939145: step 9860, loss = 0.83 (695.3 examples/sec; 0.184 sec/batch)
2016-10-15 02:03:41.705016: step 9870, loss = 0.87 (730.3 examples/sec; 0.175 sec/batch)
2016-10-15 02:03:43.470767: step 9880, loss = 1.00 (721.2 examples/sec; 0.177 sec/batch)
2016-10-15 02:03:45.229915: step 9890, loss = 0.92 (726.0 examples/sec; 0.176 sec/batch)
2016-10-15 02:03:46.990741: step 9900, loss = 0.86 (735.4 examples/sec; 0.174 sec/batch)
2016-10-15 02:03:48.932266: step 9910, loss = 0.87 (717.8 examples/sec; 0.178 sec/batch)
2016-10-15 02:03:50.690461: step 9920, loss = 0.85 (734.8 examples/sec; 0.174 sec/batch)
2016-10-15 02:03:52.458696: step 9930, loss = 0.93 (716.5 examples/sec; 0.179 sec/batch)
2016-10-15 02:03:54.217887: step 9940, loss = 0.94 (710.3 examples/sec; 0.180 sec/batch)
2016-10-15 02:03:55.963721: step 9950, loss = 0.82 (726.8 examples/sec; 0.176 sec/batch)
2016-10-15 02:03:57.717797: step 9960, loss = 0.94 (740.2 examples/sec; 0.173 sec/batch)
2016-10-15 02:03:59.483552: step 9970, loss = 0.84 (705.8 examples/sec; 0.181 sec/batch)
2016-10-15 02:04:01.239363: step 9980, loss = 0.80 (714.2 examples/sec; 0.179 sec/batch)
2016-10-15 02:04:02.999979: step 9990, loss = 0.85 (721.1 examples/sec; 0.178 sec/batch)
2016-10-15 02:04:04.761295: step 10000, loss = 0.92 (746.3 examples/sec; 0.172 sec/batch)
2016-10-15 02:04:07.304369: step 10010, loss = 0.79 (733.4 examples/sec; 0.175 sec/batch)
2016-10-15 02:04:09.075818: step 10020, loss = 0.74 (703.2 examples/sec; 0.182 sec/batch)
2016-10-15 02:04:10.829639: step 10030, loss = 1.09 (737.9 examples/sec; 0.173 sec/batch)
2016-10-15 02:04:12.585687: step 10040, loss = 1.02 (734.2 examples/sec; 0.174 sec/batch)
2016-10-15 02:04:14.352565: step 10050, loss = 0.90 (717.9 examples/sec; 0.178 sec/batch)
2016-10-15 02:04:16.102259: step 10060, loss = 0.89 (721.5 examples/sec; 0.177 sec/batch)
2016-10-15 02:04:17.859339: step 10070, loss = 0.72 (725.5 examples/sec; 0.176 sec/batch)
2016-10-15 02:04:19.627092: step 10080, loss = 0.80 (720.9 examples/sec; 0.178 sec/batch)
2016-10-15 02:04:21.390430: step 10090, loss = 0.92 (711.9 examples/sec; 0.180 sec/batch)
2016-10-15 02:04:23.146489: step 10100, loss = 0.96 (727.3 examples/sec; 0.176 sec/batch)
2016-10-15 02:04:25.109493: step 10110, loss = 0.75 (698.8 examples/sec; 0.183 sec/batch)
2016-10-15 02:04:26.863739: step 10120, loss = 0.80 (711.5 examples/sec; 0.180 sec/batch)
2016-10-15 02:04:28.617919: step 10130, loss = 0.84 (736.8 examples/sec; 0.174 sec/batch)
2016-10-15 02:04:30.378477: step 10140, loss = 1.02 (725.1 examples/sec; 0.177 sec/batch)
2016-10-15 02:04:32.144647: step 10150, loss = 0.78 (711.2 examples/sec; 0.180 sec/batch)
2016-10-15 02:04:33.896028: step 10160, loss = 0.73 (732.2 examples/sec; 0.175 sec/batch)
2016-10-15 02:04:35.652937: step 10170, loss = 0.86 (707.7 examples/sec; 0.181 sec/batch)
2016-10-15 02:04:37.404861: step 10180, loss = 0.84 (727.3 examples/sec; 0.176 sec/batch)
2016-10-15 02:04:39.153887: step 10190, loss = 0.79 (746.6 examples/sec; 0.171 sec/batch)
2016-10-15 02:04:40.911445: step 10200, loss = 0.82 (740.9 examples/sec; 0.173 sec/batch)
2016-10-15 02:04:42.850918: step 10210, loss = 0.75 (723.7 examples/sec; 0.177 sec/batch)
2016-10-15 02:04:44.611391: step 10220, loss = 0.85 (733.8 examples/sec; 0.174 sec/batch)
2016-10-15 02:04:46.371768: step 10230, loss = 0.92 (734.9 examples/sec; 0.174 sec/batch)
2016-10-15 02:04:48.136381: step 10240, loss = 0.76 (729.3 examples/sec; 0.176 sec/batch)
2016-10-15 02:04:49.898170: step 10250, loss = 0.71 (724.4 examples/sec; 0.177 sec/batch)
2016-10-15 02:04:51.660308: step 10260, loss = 0.88 (739.7 examples/sec; 0.173 sec/batch)
2016-10-15 02:04:53.418418: step 10270, loss = 0.84 (732.7 examples/sec; 0.175 sec/batch)
2016-10-15 02:04:55.176000: step 10280, loss = 0.87 (717.5 examples/sec; 0.178 sec/batch)
2016-10-15 02:04:56.930243: step 10290, loss = 1.08 (744.5 examples/sec; 0.172 sec/batch)
2016-10-15 02:04:58.686733: step 10300, loss = 0.81 (733.1 examples/sec; 0.175 sec/batch)
2016-10-15 02:05:00.622125: step 10310, loss = 1.04 (752.1 examples/sec; 0.170 sec/batch)
2016-10-15 02:05:02.404168: step 10320, loss = 1.06 (719.7 examples/sec; 0.178 sec/batch)
2016-10-15 02:05:04.157072: step 10330, loss = 0.86 (747.6 examples/sec; 0.171 sec/batch)
2016-10-15 02:05:05.917822: step 10340, loss = 0.97 (717.9 examples/sec; 0.178 sec/batch)
2016-10-15 02:05:07.676678: step 10350, loss = 0.95 (721.0 examples/sec; 0.178 sec/batch)
2016-10-15 02:05:09.430542: step 10360, loss = 0.82 (730.2 examples/sec; 0.175 sec/batch)
2016-10-15 02:05:11.191237: step 10370, loss = 0.94 (740.3 examples/sec; 0.173 sec/batch)
2016-10-15 02:05:12.954890: step 10380, loss = 0.89 (750.5 examples/sec; 0.171 sec/batch)
2016-10-15 02:05:14.700702: step 10390, loss = 0.89 (720.2 examples/sec; 0.178 sec/batch)
2016-10-15 02:05:16.459875: step 10400, loss = 0.93 (727.5 examples/sec; 0.176 sec/batch)
2016-10-15 02:05:18.406048: step 10410, loss = 0.80 (730.0 examples/sec; 0.175 sec/batch)
2016-10-15 02:05:20.156266: step 10420, loss = 0.86 (735.8 examples/sec; 0.174 sec/batch)
2016-10-15 02:05:21.919950: step 10430, loss = 0.83 (740.6 examples/sec; 0.173 sec/batch)
2016-10-15 02:05:23.680780: step 10440, loss = 0.72 (716.8 examples/sec; 0.179 sec/batch)
2016-10-15 02:05:25.447817: step 10450, loss = 0.96 (734.6 examples/sec; 0.174 sec/batch)
2016-10-15 02:05:27.203381: step 10460, loss = 0.85 (746.5 examples/sec; 0.171 sec/batch)
2016-10-15 02:05:28.956349: step 10470, loss = 0.90 (736.5 examples/sec; 0.174 sec/batch)
2016-10-15 02:05:30.726135: step 10480, loss = 0.80 (692.5 examples/sec; 0.185 sec/batch)
2016-10-15 02:05:32.508124: step 10490, loss = 0.87 (728.7 examples/sec; 0.176 sec/batch)
2016-10-15 02:05:34.281028: step 10500, loss = 0.70 (704.9 examples/sec; 0.182 sec/batch)
2016-10-15 02:05:36.210723: step 10510, loss = 0.98 (757.2 examples/sec; 0.169 sec/batch)
2016-10-15 02:05:37.966340: step 10520, loss = 0.81 (756.5 examples/sec; 0.169 sec/batch)
2016-10-15 02:05:39.723629: step 10530, loss = 0.82 (741.1 examples/sec; 0.173 sec/batch)
2016-10-15 02:05:41.482454: step 10540, loss = 0.84 (734.2 examples/sec; 0.174 sec/batch)
2016-10-15 02:05:43.234501: step 10550, loss = 0.97 (730.9 examples/sec; 0.175 sec/batch)
2016-10-15 02:05:44.985556: step 10560, loss = 0.98 (738.2 examples/sec; 0.173 sec/batch)
2016-10-15 02:05:46.736121: step 10570, loss = 0.86 (728.9 examples/sec; 0.176 sec/batch)
2016-10-15 02:05:48.487825: step 10580, loss = 0.82 (725.8 examples/sec; 0.176 sec/batch)
2016-10-15 02:05:50.247092: step 10590, loss = 0.95 (733.8 examples/sec; 0.174 sec/batch)
2016-10-15 02:05:51.993764: step 10600, loss = 0.84 (725.4 examples/sec; 0.176 sec/batch)
2016-10-15 02:05:53.945679: step 10610, loss = 0.82 (723.0 examples/sec; 0.177 sec/batch)
2016-10-15 02:05:55.697334: step 10620, loss = 0.85 (738.5 examples/sec; 0.173 sec/batch)
2016-10-15 02:05:57.460346: step 10630, loss = 0.76 (711.2 examples/sec; 0.180 sec/batch)
2016-10-15 02:05:59.222577: step 10640, loss = 0.72 (732.3 examples/sec; 0.175 sec/batch)
2016-10-15 02:06:00.984729: step 10650, loss = 0.97 (715.2 examples/sec; 0.179 sec/batch)
2016-10-15 02:06:02.747390: step 10660, loss = 0.99 (736.1 examples/sec; 0.174 sec/batch)
2016-10-15 02:06:04.505148: step 10670, loss = 1.22 (731.4 examples/sec; 0.175 sec/batch)
2016-10-15 02:06:06.275685: step 10680, loss = 1.09 (712.2 examples/sec; 0.180 sec/batch)
2016-10-15 02:06:08.038772: step 10690, loss = 0.89 (724.0 examples/sec; 0.177 sec/batch)
2016-10-15 02:06:09.792229: step 10700, loss = 0.83 (730.0 examples/sec; 0.175 sec/batch)
2016-10-15 02:06:11.729851: step 10710, loss = 0.74 (715.0 examples/sec; 0.179 sec/batch)
2016-10-15 02:06:13.491488: step 10720, loss = 0.75 (753.3 examples/sec; 0.170 sec/batch)
2016-10-15 02:06:15.245772: step 10730, loss = 0.90 (733.0 examples/sec; 0.175 sec/batch)
2016-10-15 02:06:17.009956: step 10740, loss = 0.78 (744.7 examples/sec; 0.172 sec/batch)
2016-10-15 02:06:18.776944: step 10750, loss = 1.02 (732.1 examples/sec; 0.175 sec/batch)
2016-10-15 02:06:20.542501: step 10760, loss = 0.72 (734.0 examples/sec; 0.174 sec/batch)
2016-10-15 02:06:22.309792: step 10770, loss = 0.98 (710.0 examples/sec; 0.180 sec/batch)
2016-10-15 02:06:24.062412: step 10780, loss = 0.81 (745.3 examples/sec; 0.172 sec/batch)
2016-10-15 02:06:25.825453: step 10790, loss = 0.89 (728.5 examples/sec; 0.176 sec/batch)
2016-10-15 02:06:27.593463: step 10800, loss = 0.89 (716.5 examples/sec; 0.179 sec/batch)
2016-10-15 02:06:29.535269: step 10810, loss = 0.85 (759.3 examples/sec; 0.169 sec/batch)
2016-10-15 02:06:31.297429: step 10820, loss = 0.91 (728.3 examples/sec; 0.176 sec/batch)
2016-10-15 02:06:33.070491: step 10830, loss = 0.93 (724.8 examples/sec; 0.177 sec/batch)
2016-10-15 02:06:34.833726: step 10840, loss = 0.97 (714.0 examples/sec; 0.179 sec/batch)
2016-10-15 02:06:36.596834: step 10850, loss = 0.85 (717.3 examples/sec; 0.178 sec/batch)
2016-10-15 02:06:38.351159: step 10860, loss = 0.94 (730.3 examples/sec; 0.175 sec/batch)
2016-10-15 02:06:40.113538: step 10870, loss = 0.90 (736.9 examples/sec; 0.174 sec/batch)
2016-10-15 02:06:41.882180: step 10880, loss = 1.20 (723.6 examples/sec; 0.177 sec/batch)
2016-10-15 02:06:43.648383: step 10890, loss = 0.78 (723.9 examples/sec; 0.177 sec/batch)
2016-10-15 02:06:45.405652: step 10900, loss = 0.86 (768.4 examples/sec; 0.167 sec/batch)
2016-10-15 02:06:47.353881: step 10910, loss = 0.85 (717.4 examples/sec; 0.178 sec/batch)
2016-10-15 02:06:49.114572: step 10920, loss = 0.94 (743.0 examples/sec; 0.172 sec/batch)
2016-10-15 02:06:50.876339: step 10930, loss = 0.90 (701.0 examples/sec; 0.183 sec/batch)
2016-10-15 02:06:52.621038: step 10940, loss = 0.85 (738.6 examples/sec; 0.173 sec/batch)
2016-10-15 02:06:54.378185: step 10950, loss = 0.66 (732.8 examples/sec; 0.175 sec/batch)
2016-10-15 02:06:56.144071: step 10960, loss = 0.88 (747.8 examples/sec; 0.171 sec/batch)
2016-10-15 02:06:57.904243: step 10970, loss = 0.96 (744.0 examples/sec; 0.172 sec/batch)
2016-10-15 02:06:59.670409: step 10980, loss = 0.94 (706.7 examples/sec; 0.181 sec/batch)
2016-10-15 02:07:01.427539: step 10990, loss = 1.00 (732.6 examples/sec; 0.175 sec/batch)
2016-10-15 02:07:03.191880: step 11000, loss = 0.91 (717.2 examples/sec; 0.178 sec/batch)
2016-10-15 02:07:05.641726: step 11010, loss = 0.97 (741.6 examples/sec; 0.173 sec/batch)
2016-10-15 02:07:07.401408: step 11020, loss = 0.95 (724.9 examples/sec; 0.177 sec/batch)
2016-10-15 02:07:09.164422: step 11030, loss = 0.80 (703.5 examples/sec; 0.182 sec/batch)
2016-10-15 02:07:10.917180: step 11040, loss = 0.73 (722.4 examples/sec; 0.177 sec/batch)
2016-10-15 02:07:12.670806: step 11050, loss = 0.82 (754.1 examples/sec; 0.170 sec/batch)
2016-10-15 02:07:14.429311: step 11060, loss = 0.81 (747.0 examples/sec; 0.171 sec/batch)
2016-10-15 02:07:16.187230: step 11070, loss = 0.91 (724.1 examples/sec; 0.177 sec/batch)
2016-10-15 02:07:17.944477: step 11080, loss = 0.80 (754.9 examples/sec; 0.170 sec/batch)
2016-10-15 02:07:19.695472: step 11090, loss = 0.85 (742.8 examples/sec; 0.172 sec/batch)
2016-10-15 02:07:21.455401: step 11100, loss = 1.01 (726.6 examples/sec; 0.176 sec/batch)
2016-10-15 02:07:23.394755: step 11110, loss = 0.74 (718.7 examples/sec; 0.178 sec/batch)
2016-10-15 02:07:25.154557: step 11120, loss = 0.90 (714.7 examples/sec; 0.179 sec/batch)
2016-10-15 02:07:26.900855: step 11130, loss = 0.87 (729.7 examples/sec; 0.175 sec/batch)
2016-10-15 02:07:28.665672: step 11140, loss = 0.92 (731.1 examples/sec; 0.175 sec/batch)
2016-10-15 02:07:30.418185: step 11150, loss = 0.88 (751.3 examples/sec; 0.170 sec/batch)
2016-10-15 02:07:32.182139: step 11160, loss = 0.70 (728.5 examples/sec; 0.176 sec/batch)
2016-10-15 02:07:33.947559: step 11170, loss = 0.81 (713.0 examples/sec; 0.180 sec/batch)
2016-10-15 02:07:35.696129: step 11180, loss = 0.93 (731.7 examples/sec; 0.175 sec/batch)
2016-10-15 02:07:37.450283: step 11190, loss = 0.83 (752.7 examples/sec; 0.170 sec/batch)
2016-10-15 02:07:39.208936: step 11200, loss = 0.85 (753.7 examples/sec; 0.170 sec/batch)
2016-10-15 02:07:41.146259: step 11210, loss = 1.00 (746.7 examples/sec; 0.171 sec/batch)
2016-10-15 02:07:42.906649: step 11220, loss = 0.78 (725.6 examples/sec; 0.176 sec/batch)
2016-10-15 02:07:44.661559: step 11230, loss = 0.88 (754.5 examples/sec; 0.170 sec/batch)
2016-10-15 02:07:46.420146: step 11240, loss = 0.86 (728.2 examples/sec; 0.176 sec/batch)
2016-10-15 02:07:48.176170: step 11250, loss = 0.82 (702.0 examples/sec; 0.182 sec/batch)
2016-10-15 02:07:49.931301: step 11260, loss = 0.73 (728.5 examples/sec; 0.176 sec/batch)
2016-10-15 02:07:51.694288: step 11270, loss = 0.86 (786.5 examples/sec; 0.163 sec/batch)
2016-10-15 02:07:53.456408: step 11280, loss = 0.85 (727.9 examples/sec; 0.176 sec/batch)
2016-10-15 02:07:55.218043: step 11290, loss = 0.83 (701.4 examples/sec; 0.182 sec/batch)
2016-10-15 02:07:56.987431: step 11300, loss = 0.63 (688.6 examples/sec; 0.186 sec/batch)
2016-10-15 02:07:58.928853: step 11310, loss = 1.01 (718.4 examples/sec; 0.178 sec/batch)
2016-10-15 02:08:00.674229: step 11320, loss = 0.80 (747.4 examples/sec; 0.171 sec/batch)
2016-10-15 02:08:02.450299: step 11330, loss = 0.93 (739.9 examples/sec; 0.173 sec/batch)
2016-10-15 02:08:04.205722: step 11340, loss = 0.91 (776.6 examples/sec; 0.165 sec/batch)
2016-10-15 02:08:05.964577: step 11350, loss = 0.90 (756.0 examples/sec; 0.169 sec/batch)
2016-10-15 02:08:07.723297: step 11360, loss = 0.97 (741.2 examples/sec; 0.173 sec/batch)
2016-10-15 02:08:09.478417: step 11370, loss = 0.84 (727.4 examples/sec; 0.176 sec/batch)
2016-10-15 02:08:11.239566: step 11380, loss = 0.80 (736.1 examples/sec; 0.174 sec/batch)
2016-10-15 02:08:13.000650: step 11390, loss = 0.85 (735.6 examples/sec; 0.174 sec/batch)
2016-10-15 02:08:14.758084: step 11400, loss = 0.94 (721.6 examples/sec; 0.177 sec/batch)
2016-10-15 02:08:16.700330: step 11410, loss = 0.86 (719.9 examples/sec; 0.178 sec/batch)
2016-10-15 02:08:18.461380: step 11420, loss = 0.75 (720.5 examples/sec; 0.178 sec/batch)
2016-10-15 02:08:20.231475: step 11430, loss = 0.93 (712.6 examples/sec; 0.180 sec/batch)
2016-10-15 02:08:21.986771: step 11440, loss = 0.75 (735.8 examples/sec; 0.174 sec/batch)
2016-10-15 02:08:23.753481: step 11450, loss = 0.91 (727.9 examples/sec; 0.176 sec/batch)
2016-10-15 02:08:25.518588: step 11460, loss = 0.99 (730.6 examples/sec; 0.175 sec/batch)
2016-10-15 02:08:27.275427: step 11470, loss = 0.79 (745.3 examples/sec; 0.172 sec/batch)
2016-10-15 02:08:29.036937: step 11480, loss = 0.86 (727.4 examples/sec; 0.176 sec/batch)
2016-10-15 02:08:30.801077: step 11490, loss = 0.93 (723.5 examples/sec; 0.177 sec/batch)
2016-10-15 02:08:32.565605: step 11500, loss = 0.81 (722.7 examples/sec; 0.177 sec/batch)
2016-10-15 02:08:34.518945: step 11510, loss = 1.05 (723.2 examples/sec; 0.177 sec/batch)
2016-10-15 02:08:36.287856: step 11520, loss = 0.80 (707.3 examples/sec; 0.181 sec/batch)
2016-10-15 02:08:38.042620: step 11530, loss = 0.85 (728.6 examples/sec; 0.176 sec/batch)
2016-10-15 02:08:39.806659: step 11540, loss = 1.02 (712.8 examples/sec; 0.180 sec/batch)
2016-10-15 02:08:41.570199: step 11550, loss = 0.88 (736.0 examples/sec; 0.174 sec/batch)
2016-10-15 02:08:43.348728: step 11560, loss = 0.82 (682.2 examples/sec; 0.188 sec/batch)
2016-10-15 02:08:45.110924: step 11570, loss = 0.79 (747.5 examples/sec; 0.171 sec/batch)
2016-10-15 02:08:46.867754: step 11580, loss = 0.80 (714.4 examples/sec; 0.179 sec/batch)
2016-10-15 02:08:48.623609: step 11590, loss = 0.85 (761.7 examples/sec; 0.168 sec/batch)
2016-10-15 02:08:50.390092: step 11600, loss = 0.74 (716.6 examples/sec; 0.179 sec/batch)
2016-10-15 02:08:52.338459: step 11610, loss = 0.89 (765.3 examples/sec; 0.167 sec/batch)
2016-10-15 02:08:54.100601: step 11620, loss = 0.86 (746.2 examples/sec; 0.172 sec/batch)
2016-10-15 02:08:55.862518: step 11630, loss = 0.89 (699.1 examples/sec; 0.183 sec/batch)
2016-10-15 02:08:57.617737: step 11640, loss = 0.97 (721.0 examples/sec; 0.178 sec/batch)
2016-10-15 02:08:59.369853: step 11650, loss = 0.74 (720.3 examples/sec; 0.178 sec/batch)
2016-10-15 02:09:01.133002: step 11660, loss = 0.95 (728.5 examples/sec; 0.176 sec/batch)
2016-10-15 02:09:02.894244: step 11670, loss = 0.98 (709.0 examples/sec; 0.181 sec/batch)
2016-10-15 02:09:04.652770: step 11680, loss = 0.83 (719.6 examples/sec; 0.178 sec/batch)
2016-10-15 02:09:06.406178: step 11690, loss = 0.87 (734.3 examples/sec; 0.174 sec/batch)
2016-10-15 02:09:08.164018: step 11700, loss = 0.83 (752.1 examples/sec; 0.170 sec/batch)
2016-10-15 02:09:10.116550: step 11710, loss = 0.95 (709.2 examples/sec; 0.180 sec/batch)
2016-10-15 02:09:11.865547: step 11720, loss = 0.85 (730.7 examples/sec; 0.175 sec/batch)
2016-10-15 02:09:13.635389: step 11730, loss = 0.85 (727.0 examples/sec; 0.176 sec/batch)
2016-10-15 02:09:15.387234: step 11740, loss = 0.82 (719.9 examples/sec; 0.178 sec/batch)
2016-10-15 02:09:17.148453: step 11750, loss = 0.83 (719.9 examples/sec; 0.178 sec/batch)
2016-10-15 02:09:18.905580: step 11760, loss = 0.86 (731.5 examples/sec; 0.175 sec/batch)
2016-10-15 02:09:20.681450: step 11770, loss = 0.80 (701.0 examples/sec; 0.183 sec/batch)
2016-10-15 02:09:22.437428: step 11780, loss = 0.85 (725.8 examples/sec; 0.176 sec/batch)
2016-10-15 02:09:24.187685: step 11790, loss = 0.74 (724.3 examples/sec; 0.177 sec/batch)
2016-10-15 02:09:25.943324: step 11800, loss = 0.87 (729.2 examples/sec; 0.176 sec/batch)
2016-10-15 02:09:27.882165: step 11810, loss = 0.74 (733.4 examples/sec; 0.175 sec/batch)
2016-10-15 02:09:29.637134: step 11820, loss = 0.87 (752.5 examples/sec; 0.170 sec/batch)
2016-10-15 02:09:31.397555: step 11830, loss = 0.90 (741.0 examples/sec; 0.173 sec/batch)
2016-10-15 02:09:33.167992: step 11840, loss = 0.91 (708.3 examples/sec; 0.181 sec/batch)
2016-10-15 02:09:34.926394: step 11850, loss = 0.92 (720.3 examples/sec; 0.178 sec/batch)
2016-10-15 02:09:36.682396: step 11860, loss = 0.90 (731.1 examples/sec; 0.175 sec/batch)
2016-10-15 02:09:38.448199: step 11870, loss = 0.98 (716.9 examples/sec; 0.179 sec/batch)
2016-10-15 02:09:40.208444: step 11880, loss = 0.98 (715.7 examples/sec; 0.179 sec/batch)
2016-10-15 02:09:41.966251: step 11890, loss = 0.73 (746.5 examples/sec; 0.171 sec/batch)
2016-10-15 02:09:43.726041: step 11900, loss = 0.79 (738.7 examples/sec; 0.173 sec/batch)
2016-10-15 02:09:45.671106: step 11910, loss = 0.92 (721.3 examples/sec; 0.177 sec/batch)
2016-10-15 02:09:47.426797: step 11920, loss = 0.83 (741.5 examples/sec; 0.173 sec/batch)
2016-10-15 02:09:49.190105: step 11930, loss = 0.95 (720.9 examples/sec; 0.178 sec/batch)
2016-10-15 02:09:50.943031: step 11940, loss = 0.82 (725.3 examples/sec; 0.176 sec/batch)
2016-10-15 02:09:52.708740: step 11950, loss = 0.85 (734.9 examples/sec; 0.174 sec/batch)
2016-10-15 02:09:54.466044: step 11960, loss = 1.00 (716.8 examples/sec; 0.179 sec/batch)
2016-10-15 02:09:56.226437: step 11970, loss = 0.85 (725.0 examples/sec; 0.177 sec/batch)
2016-10-15 02:09:57.984068: step 11980, loss = 0.88 (737.2 examples/sec; 0.174 sec/batch)
2016-10-15 02:09:59.755432: step 11990, loss = 0.87 (702.4 examples/sec; 0.182 sec/batch)
2016-10-15 02:10:01.503884: step 12000, loss = 0.97 (735.4 examples/sec; 0.174 sec/batch)
2016-10-15 02:10:03.996867: step 12010, loss = 0.84 (708.7 examples/sec; 0.181 sec/batch)
2016-10-15 02:10:05.762121: step 12020, loss = 0.71 (722.7 examples/sec; 0.177 sec/batch)
2016-10-15 02:10:07.530305: step 12030, loss = 0.83 (705.6 examples/sec; 0.181 sec/batch)
2016-10-15 02:10:09.285468: step 12040, loss = 0.81 (772.3 examples/sec; 0.166 sec/batch)
2016-10-15 02:10:11.036306: step 12050, loss = 0.83 (711.4 examples/sec; 0.180 sec/batch)
2016-10-15 02:10:12.800829: step 12060, loss = 0.96 (733.7 examples/sec; 0.174 sec/batch)
2016-10-15 02:10:14.566184: step 12070, loss = 0.79 (743.8 examples/sec; 0.172 sec/batch)
2016-10-15 02:10:16.325852: step 12080, loss = 0.84 (733.1 examples/sec; 0.175 sec/batch)
2016-10-15 02:10:18.084854: step 12090, loss = 0.74 (720.7 examples/sec; 0.178 sec/batch)
2016-10-15 02:10:19.838204: step 12100, loss = 0.84 (760.3 examples/sec; 0.168 sec/batch)
2016-10-15 02:10:21.792466: step 12110, loss = 0.80 (712.4 examples/sec; 0.180 sec/batch)
2016-10-15 02:10:23.549757: step 12120, loss = 0.86 (734.2 examples/sec; 0.174 sec/batch)
2016-10-15 02:10:25.317800: step 12130, loss = 0.67 (714.7 examples/sec; 0.179 sec/batch)
2016-10-15 02:10:27.068912: step 12140, loss = 0.73 (721.5 examples/sec; 0.177 sec/batch)
2016-10-15 02:10:28.826758: step 12150, loss = 0.81 (735.4 examples/sec; 0.174 sec/batch)
2016-10-15 02:10:30.589772: step 12160, loss = 0.88 (742.9 examples/sec; 0.172 sec/batch)
2016-10-15 02:10:32.353400: step 12170, loss = 0.80 (694.6 examples/sec; 0.184 sec/batch)
2016-10-15 02:10:34.116995: step 12180, loss = 0.84 (712.5 examples/sec; 0.180 sec/batch)
2016-10-15 02:10:35.865020: step 12190, loss = 0.87 (740.6 examples/sec; 0.173 sec/batch)
2016-10-15 02:10:37.625562: step 12200, loss = 0.87 (729.4 examples/sec; 0.175 sec/batch)
2016-10-15 02:10:39.569886: step 12210, loss = 0.90 (748.9 examples/sec; 0.171 sec/batch)
2016-10-15 02:10:41.323297: step 12220, loss = 0.93 (732.2 examples/sec; 0.175 sec/batch)
2016-10-15 02:10:43.085271: step 12230, loss = 0.95 (726.0 examples/sec; 0.176 sec/batch)
2016-10-15 02:10:44.859004: step 12240, loss = 0.87 (694.3 examples/sec; 0.184 sec/batch)
2016-10-15 02:10:46.605390: step 12250, loss = 1.06 (741.7 examples/sec; 0.173 sec/batch)
2016-10-15 02:10:48.365616: step 12260, loss = 0.98 (715.2 examples/sec; 0.179 sec/batch)
2016-10-15 02:10:50.119236: step 12270, loss = 0.94 (707.9 examples/sec; 0.181 sec/batch)
2016-10-15 02:10:51.875263: step 12280, loss = 1.14 (736.2 examples/sec; 0.174 sec/batch)
2016-10-15 02:10:53.634806: step 12290, loss = 0.86 (735.3 examples/sec; 0.174 sec/batch)
2016-10-15 02:10:55.391873: step 12300, loss = 0.99 (717.0 examples/sec; 0.179 sec/batch)
2016-10-15 02:10:57.336377: step 12310, loss = 0.80 (736.8 examples/sec; 0.174 sec/batch)
2016-10-15 02:10:59.099838: step 12320, loss = 0.76 (719.2 examples/sec; 0.178 sec/batch)
2016-10-15 02:11:00.860178: step 12330, loss = 1.04 (726.5 examples/sec; 0.176 sec/batch)
2016-10-15 02:11:02.625086: step 12340, loss = 0.84 (736.6 examples/sec; 0.174 sec/batch)
2016-10-15 02:11:04.388642: step 12350, loss = 0.88 (695.3 examples/sec; 0.184 sec/batch)
2016-10-15 02:11:06.130877: step 12360, loss = 0.97 (746.6 examples/sec; 0.171 sec/batch)
2016-10-15 02:11:07.880238: step 12370, loss = 0.95 (737.9 examples/sec; 0.173 sec/batch)
2016-10-15 02:11:09.650414: step 12380, loss = 0.93 (697.1 examples/sec; 0.184 sec/batch)
2016-10-15 02:11:11.403684: step 12390, loss = 1.10 (729.3 examples/sec; 0.176 sec/batch)
2016-10-15 02:11:13.166804: step 12400, loss = 0.74 (733.8 examples/sec; 0.174 sec/batch)
2016-10-15 02:11:15.110073: step 12410, loss = 0.88 (719.1 examples/sec; 0.178 sec/batch)
2016-10-15 02:11:16.870388: step 12420, loss = 0.69 (712.5 examples/sec; 0.180 sec/batch)
2016-10-15 02:11:18.632055: step 12430, loss = 0.81 (741.7 examples/sec; 0.173 sec/batch)
2016-10-15 02:11:20.395042: step 12440, loss = 0.89 (725.5 examples/sec; 0.176 sec/batch)
2016-10-15 02:11:22.150663: step 12450, loss = 0.85 (763.9 examples/sec; 0.168 sec/batch)
2016-10-15 02:11:23.908965: step 12460, loss = 0.98 (714.6 examples/sec; 0.179 sec/batch)
2016-10-15 02:11:25.664089: step 12470, loss = 0.84 (755.6 examples/sec; 0.169 sec/batch)
2016-10-15 02:11:27.415386: step 12480, loss = 0.77 (763.8 examples/sec; 0.168 sec/batch)
2016-10-15 02:11:29.178286: step 12490, loss = 1.00 (722.4 examples/sec; 0.177 sec/batch)
2016-10-15 02:11:30.929924: step 12500, loss = 0.81 (715.7 examples/sec; 0.179 sec/batch)
2016-10-15 02:11:32.874360: step 12510, loss = 0.77 (734.7 examples/sec; 0.174 sec/batch)
2016-10-15 02:11:34.627341: step 12520, loss = 0.92 (709.3 examples/sec; 0.180 sec/batch)
2016-10-15 02:11:36.384460: step 12530, loss = 0.72 (727.7 examples/sec; 0.176 sec/batch)
2016-10-15 02:11:38.139513: step 12540, loss = 0.99 (757.2 examples/sec; 0.169 sec/batch)
2016-10-15 02:11:39.904769: step 12550, loss = 0.97 (723.2 examples/sec; 0.177 sec/batch)
2016-10-15 02:11:41.660452: step 12560, loss = 0.85 (716.8 examples/sec; 0.179 sec/batch)
2016-10-15 02:11:43.415219: step 12570, loss = 0.67 (782.1 examples/sec; 0.164 sec/batch)
2016-10-15 02:11:45.182782: step 12580, loss = 0.83 (707.1 examples/sec; 0.181 sec/batch)
2016-10-15 02:11:46.940355: step 12590, loss = 0.73 (731.3 examples/sec; 0.175 sec/batch)
2016-10-15 02:11:48.699608: step 12600, loss = 0.86 (756.1 examples/sec; 0.169 sec/batch)
2016-10-15 02:11:50.644735: step 12610, loss = 0.82 (723.3 examples/sec; 0.177 sec/batch)
2016-10-15 02:11:52.408760: step 12620, loss = 0.85 (733.7 examples/sec; 0.174 sec/batch)
2016-10-15 02:11:54.165441: step 12630, loss = 0.83 (738.0 examples/sec; 0.173 sec/batch)
2016-10-15 02:11:55.928101: step 12640, loss = 0.78 (720.5 examples/sec; 0.178 sec/batch)
2016-10-15 02:11:57.681548: step 12650, loss = 0.71 (763.7 examples/sec; 0.168 sec/batch)
2016-10-15 02:11:59.444902: step 12660, loss = 0.92 (725.5 examples/sec; 0.176 sec/batch)
2016-10-15 02:12:01.208357: step 12670, loss = 0.79 (704.0 examples/sec; 0.182 sec/batch)
2016-10-15 02:12:02.966869: step 12680, loss = 0.91 (725.0 examples/sec; 0.177 sec/batch)
2016-10-15 02:12:04.731988: step 12690, loss = 0.74 (700.5 examples/sec; 0.183 sec/batch)
2016-10-15 02:12:06.483033: step 12700, loss = 0.80 (717.9 examples/sec; 0.178 sec/batch)
2016-10-15 02:12:08.428107: step 12710, loss = 0.87 (713.8 examples/sec; 0.179 sec/batch)
2016-10-15 02:12:10.185737: step 12720, loss = 0.79 (733.9 examples/sec; 0.174 sec/batch)
2016-10-15 02:12:11.949829: step 12730, loss = 0.86 (749.6 examples/sec; 0.171 sec/batch)
2016-10-15 02:12:13.720177: step 12740, loss = 0.78 (710.6 examples/sec; 0.180 sec/batch)
2016-10-15 02:12:15.472572: step 12750, loss = 1.01 (731.4 examples/sec; 0.175 sec/batch)
2016-10-15 02:12:17.233912: step 12760, loss = 0.78 (732.8 examples/sec; 0.175 sec/batch)
2016-10-15 02:12:19.007311: step 12770, loss = 0.90 (703.8 examples/sec; 0.182 sec/batch)
2016-10-15 02:12:20.770801: step 12780, loss = 0.88 (716.5 examples/sec; 0.179 sec/batch)
2016-10-15 02:12:22.537763: step 12790, loss = 0.97 (741.3 examples/sec; 0.173 sec/batch)
2016-10-15 02:12:24.296883: step 12800, loss = 0.89 (717.1 examples/sec; 0.178 sec/batch)
2016-10-15 02:12:26.244145: step 12810, loss = 0.80 (738.8 examples/sec; 0.173 sec/batch)
2016-10-15 02:12:28.006550: step 12820, loss = 1.00 (731.5 examples/sec; 0.175 sec/batch)
2016-10-15 02:12:29.771419: step 12830, loss = 0.90 (703.9 examples/sec; 0.182 sec/batch)
2016-10-15 02:12:31.532754: step 12840, loss = 0.92 (717.4 examples/sec; 0.178 sec/batch)
2016-10-15 02:12:33.295654: step 12850, loss = 0.77 (733.4 examples/sec; 0.175 sec/batch)
2016-10-15 02:12:35.058731: step 12860, loss = 0.90 (730.7 examples/sec; 0.175 sec/batch)
2016-10-15 02:12:36.824209: step 12870, loss = 0.95 (732.3 examples/sec; 0.175 sec/batch)
2016-10-15 02:12:38.584842: step 12880, loss = 1.01 (725.1 examples/sec; 0.177 sec/batch)
2016-10-15 02:12:40.346689: step 12890, loss = 0.87 (732.1 examples/sec; 0.175 sec/batch)
2016-10-15 02:12:42.112235: step 12900, loss = 0.80 (733.1 examples/sec; 0.175 sec/batch)
2016-10-15 02:12:44.065851: step 12910, loss = 0.78 (723.2 examples/sec; 0.177 sec/batch)
2016-10-15 02:12:45.832300: step 12920, loss = 0.84 (713.0 examples/sec; 0.180 sec/batch)
2016-10-15 02:12:47.587547: step 12930, loss = 0.81 (743.1 examples/sec; 0.172 sec/batch)
2016-10-15 02:12:49.355410: step 12940, loss = 0.86 (721.7 examples/sec; 0.177 sec/batch)
2016-10-15 02:12:51.124298: step 12950, loss = 0.90 (711.2 examples/sec; 0.180 sec/batch)
2016-10-15 02:12:52.890688: step 12960, loss = 0.73 (721.8 examples/sec; 0.177 sec/batch)
2016-10-15 02:12:54.651180: step 12970, loss = 0.99 (736.2 examples/sec; 0.174 sec/batch)
2016-10-15 02:12:56.417222: step 12980, loss = 0.83 (726.5 examples/sec; 0.176 sec/batch)
2016-10-15 02:12:58.177887: step 12990, loss = 0.95 (709.9 examples/sec; 0.180 sec/batch)
2016-10-15 02:12:59.944891: step 13000, loss = 0.89 (703.5 examples/sec; 0.182 sec/batch)
2016-10-15 02:13:02.515925: step 13010, loss = 0.85 (731.0 examples/sec; 0.175 sec/batch)
2016-10-15 02:13:04.276265: step 13020, loss = 0.82 (728.4 examples/sec; 0.176 sec/batch)
2016-10-15 02:13:06.033930: step 13030, loss = 0.94 (732.6 examples/sec; 0.175 sec/batch)
2016-10-15 02:13:07.791962: step 13040, loss = 0.77 (721.6 examples/sec; 0.177 sec/batch)
2016-10-15 02:13:09.563759: step 13050, loss = 0.80 (708.2 examples/sec; 0.181 sec/batch)
2016-10-15 02:13:11.315723: step 13060, loss = 0.74 (719.1 examples/sec; 0.178 sec/batch)
2016-10-15 02:13:13.092143: step 13070, loss = 0.93 (700.6 examples/sec; 0.183 sec/batch)
2016-10-15 02:13:14.846607: step 13080, loss = 0.83 (714.2 examples/sec; 0.179 sec/batch)
2016-10-15 02:13:16.615003: step 13090, loss = 0.90 (735.5 examples/sec; 0.174 sec/batch)
2016-10-15 02:13:18.383885: step 13100, loss = 0.87 (705.2 examples/sec; 0.182 sec/batch)
2016-10-15 02:13:20.331246: step 13110, loss = 1.03 (719.3 examples/sec; 0.178 sec/batch)
2016-10-15 02:13:22.087914: step 13120, loss = 0.94 (725.0 examples/sec; 0.177 sec/batch)
2016-10-15 02:13:23.856963: step 13130, loss = 0.84 (748.1 examples/sec; 0.171 sec/batch)
2016-10-15 02:13:25.624769: step 13140, loss = 0.91 (691.0 examples/sec; 0.185 sec/batch)
2016-10-15 02:13:27.380872: step 13150, loss = 0.92 (737.5 examples/sec; 0.174 sec/batch)
2016-10-15 02:13:29.157005: step 13160, loss = 0.83 (721.4 examples/sec; 0.177 sec/batch)
2016-10-15 02:13:30.929827: step 13170, loss = 0.77 (718.4 examples/sec; 0.178 sec/batch)
2016-10-15 02:13:32.702848: step 13180, loss = 1.06 (740.5 examples/sec; 0.173 sec/batch)
2016-10-15 02:13:34.471443: step 13190, loss = 0.90 (727.8 examples/sec; 0.176 sec/batch)
2016-10-15 02:13:36.240136: step 13200, loss = 0.82 (696.0 examples/sec; 0.184 sec/batch)
2016-10-15 02:13:38.195563: step 13210, loss = 0.98 (716.6 examples/sec; 0.179 sec/batch)
2016-10-15 02:13:39.945565: step 13220, loss = 0.72 (752.5 examples/sec; 0.170 sec/batch)
2016-10-15 02:13:41.712578: step 13230, loss = 0.76 (739.4 examples/sec; 0.173 sec/batch)
2016-10-15 02:13:43.472561: step 13240, loss = 0.87 (738.7 examples/sec; 0.173 sec/batch)
2016-10-15 02:13:45.228835: step 13250, loss = 0.73 (720.1 examples/sec; 0.178 sec/batch)
2016-10-15 02:13:46.996605: step 13260, loss = 0.84 (741.1 examples/sec; 0.173 sec/batch)
2016-10-15 02:13:48.765747: step 13270, loss = 0.96 (716.6 examples/sec; 0.179 sec/batch)
2016-10-15 02:13:50.532639: step 13280, loss = 0.79 (735.7 examples/sec; 0.174 sec/batch)
2016-10-15 02:13:52.293343: step 13290, loss = 0.75 (705.8 examples/sec; 0.181 sec/batch)
2016-10-15 02:13:54.064295: step 13300, loss = 0.81 (734.8 examples/sec; 0.174 sec/batch)
2016-10-15 02:13:56.011103: step 13310, loss = 0.70 (737.7 examples/sec; 0.174 sec/batch)
2016-10-15 02:13:57.775438: step 13320, loss = 0.85 (711.0 examples/sec; 0.180 sec/batch)
2016-10-15 02:13:59.543820: step 13330, loss = 0.85 (724.8 examples/sec; 0.177 sec/batch)
2016-10-15 02:14:01.312447: step 13340, loss = 0.85 (730.5 examples/sec; 0.175 sec/batch)
2016-10-15 02:14:03.080686: step 13350, loss = 0.87 (694.7 examples/sec; 0.184 sec/batch)
2016-10-15 02:14:04.843448: step 13360, loss = 0.86 (749.4 examples/sec; 0.171 sec/batch)
2016-10-15 02:14:06.603952: step 13370, loss = 0.85 (721.4 examples/sec; 0.177 sec/batch)
2016-10-15 02:14:08.365623: step 13380, loss = 0.84 (760.5 examples/sec; 0.168 sec/batch)
2016-10-15 02:14:10.127197: step 13390, loss = 0.76 (751.7 examples/sec; 0.170 sec/batch)
2016-10-15 02:14:11.892574: step 13400, loss = 0.88 (724.6 examples/sec; 0.177 sec/batch)
2016-10-15 02:14:13.842628: step 13410, loss = 0.77 (712.0 examples/sec; 0.180 sec/batch)
2016-10-15 02:14:15.603605: step 13420, loss = 0.84 (731.6 examples/sec; 0.175 sec/batch)
2016-10-15 02:14:17.354485: step 13430, loss = 1.02 (748.0 examples/sec; 0.171 sec/batch)
2016-10-15 02:14:19.122475: step 13440, loss = 0.75 (721.9 examples/sec; 0.177 sec/batch)
2016-10-15 02:14:20.879277: step 13450, loss = 0.88 (749.1 examples/sec; 0.171 sec/batch)
2016-10-15 02:14:22.632853: step 13460, loss = 0.76 (742.4 examples/sec; 0.172 sec/batch)
2016-10-15 02:14:24.404456: step 13470, loss = 0.86 (714.4 examples/sec; 0.179 sec/batch)
2016-10-15 02:14:26.160410: step 13480, loss = 0.82 (727.3 examples/sec; 0.176 sec/batch)
2016-10-15 02:14:27.915304: step 13490, loss = 0.84 (730.1 examples/sec; 0.175 sec/batch)
2016-10-15 02:14:29.678305: step 13500, loss = 0.72 (746.3 examples/sec; 0.172 sec/batch)
2016-10-15 02:14:31.626842: step 13510, loss = 0.94 (709.0 examples/sec; 0.181 sec/batch)
2016-10-15 02:14:33.381757: step 13520, loss = 0.97 (723.0 examples/sec; 0.177 sec/batch)
2016-10-15 02:14:35.140084: step 13530, loss = 0.71 (729.9 examples/sec; 0.175 sec/batch)
2016-10-15 02:14:36.906582: step 13540, loss = 1.00 (746.8 examples/sec; 0.171 sec/batch)
2016-10-15 02:14:38.676088: step 13550, loss = 0.79 (718.5 examples/sec; 0.178 sec/batch)
2016-10-15 02:14:40.426480: step 13560, loss = 0.88 (718.1 examples/sec; 0.178 sec/batch)
2016-10-15 02:14:42.182503: step 13570, loss = 0.90 (756.1 examples/sec; 0.169 sec/batch)
2016-10-15 02:14:43.945516: step 13580, loss = 0.75 (712.0 examples/sec; 0.180 sec/batch)
2016-10-15 02:14:45.702244: step 13590, loss = 0.77 (737.5 examples/sec; 0.174 sec/batch)
2016-10-15 02:14:47.453333: step 13600, loss = 0.88 (726.8 examples/sec; 0.176 sec/batch)
2016-10-15 02:14:49.388689: step 13610, loss = 0.82 (748.9 examples/sec; 0.171 sec/batch)
2016-10-15 02:14:51.146552: step 13620, loss = 0.88 (730.4 examples/sec; 0.175 sec/batch)
2016-10-15 02:14:52.911920: step 13630, loss = 0.79 (712.0 examples/sec; 0.180 sec/batch)
2016-10-15 02:14:54.661939: step 13640, loss = 0.97 (746.1 examples/sec; 0.172 sec/batch)
2016-10-15 02:14:56.421757: step 13650, loss = 0.78 (724.5 examples/sec; 0.177 sec/batch)
2016-10-15 02:14:58.179618: step 13660, loss = 0.79 (730.8 examples/sec; 0.175 sec/batch)
2016-10-15 02:14:59.938917: step 13670, loss = 0.73 (736.6 examples/sec; 0.174 sec/batch)
2016-10-15 02:15:01.701557: step 13680, loss = 0.86 (724.3 examples/sec; 0.177 sec/batch)
2016-10-15 02:15:03.485208: step 13690, loss = 0.87 (744.5 examples/sec; 0.172 sec/batch)
2016-10-15 02:15:05.238052: step 13700, loss = 0.93 (727.2 examples/sec; 0.176 sec/batch)
2016-10-15 02:15:07.182313: step 13710, loss = 0.72 (728.8 examples/sec; 0.176 sec/batch)
2016-10-15 02:15:08.944387: step 13720, loss = 0.90 (714.0 examples/sec; 0.179 sec/batch)
2016-10-15 02:15:10.702745: step 13730, loss = 0.76 (735.5 examples/sec; 0.174 sec/batch)
2016-10-15 02:15:12.467762: step 13740, loss = 0.86 (726.2 examples/sec; 0.176 sec/batch)
2016-10-15 02:15:14.224444: step 13750, loss = 1.04 (717.3 examples/sec; 0.178 sec/batch)
2016-10-15 02:15:15.973537: step 13760, loss = 0.85 (733.7 examples/sec; 0.174 sec/batch)
2016-10-15 02:15:17.732351: step 13770, loss = 1.15 (721.6 examples/sec; 0.177 sec/batch)
2016-10-15 02:15:19.492707: step 13780, loss = 0.92 (744.9 examples/sec; 0.172 sec/batch)
2016-10-15 02:15:21.259736: step 13790, loss = 0.87 (728.8 examples/sec; 0.176 sec/batch)
2016-10-15 02:15:23.019918: step 13800, loss = 0.77 (740.5 examples/sec; 0.173 sec/batch)
2016-10-15 02:15:24.976897: step 13810, loss = 0.80 (711.1 examples/sec; 0.180 sec/batch)
2016-10-15 02:15:26.729611: step 13820, loss = 0.95 (720.6 examples/sec; 0.178 sec/batch)
2016-10-15 02:15:28.487103: step 13830, loss = 0.94 (729.8 examples/sec; 0.175 sec/batch)
2016-10-15 02:15:30.243493: step 13840, loss = 0.85 (739.9 examples/sec; 0.173 sec/batch)
2016-10-15 02:15:31.999827: step 13850, loss = 0.90 (717.9 examples/sec; 0.178 sec/batch)
2016-10-15 02:15:33.777668: step 13860, loss = 0.79 (655.9 examples/sec; 0.195 sec/batch)
2016-10-15 02:15:35.516027: step 13870, loss = 0.86 (740.4 examples/sec; 0.173 sec/batch)
2016-10-15 02:15:37.281208: step 13880, loss = 0.83 (712.0 examples/sec; 0.180 sec/batch)
2016-10-15 02:15:39.046000: step 13890, loss = 0.62 (708.6 examples/sec; 0.181 sec/batch)
2016-10-15 02:15:40.803176: step 13900, loss = 0.73 (736.7 examples/sec; 0.174 sec/batch)
2016-10-15 02:15:42.755673: step 13910, loss = 0.81 (725.4 examples/sec; 0.176 sec/batch)
2016-10-15 02:15:44.519567: step 13920, loss = 1.07 (733.7 examples/sec; 0.174 sec/batch)
2016-10-15 02:15:46.287209: step 13930, loss = 0.69 (718.2 examples/sec; 0.178 sec/batch)
2016-10-15 02:15:48.049079: step 13940, loss = 0.81 (724.9 examples/sec; 0.177 sec/batch)
2016-10-15 02:15:49.797192: step 13950, loss = 0.91 (771.3 examples/sec; 0.166 sec/batch)
2016-10-15 02:15:51.554285: step 13960, loss = 0.85 (738.6 examples/sec; 0.173 sec/batch)
2016-10-15 02:15:53.320375: step 13970, loss = 0.88 (717.2 examples/sec; 0.178 sec/batch)
2016-10-15 02:15:55.080644: step 13980, loss = 0.85 (740.0 examples/sec; 0.173 sec/batch)
2016-10-15 02:15:56.843510: step 13990, loss = 0.79 (726.1 examples/sec; 0.176 sec/batch)
2016-10-15 02:15:58.603520: step 14000, loss = 0.83 (736.3 examples/sec; 0.174 sec/batch)
2016-10-15 02:16:01.126507: step 14010, loss = 0.95 (702.5 examples/sec; 0.182 sec/batch)
2016-10-15 02:16:02.891772: step 14020, loss = 0.92 (726.7 examples/sec; 0.176 sec/batch)
2016-10-15 02:16:04.651685: step 14030, loss = 0.85 (722.2 examples/sec; 0.177 sec/batch)
2016-10-15 02:16:06.417139: step 14040, loss = 0.93 (736.4 examples/sec; 0.174 sec/batch)
2016-10-15 02:16:08.174203: step 14050, loss = 0.83 (715.9 examples/sec; 0.179 sec/batch)
2016-10-15 02:16:09.926572: step 14060, loss = 1.03 (728.8 examples/sec; 0.176 sec/batch)
2016-10-15 02:16:11.683286: step 14070, loss = 0.83 (716.2 examples/sec; 0.179 sec/batch)
2016-10-15 02:16:13.446023: step 14080, loss = 0.88 (755.1 examples/sec; 0.170 sec/batch)
2016-10-15 02:16:15.206808: step 14090, loss = 0.82 (736.5 examples/sec; 0.174 sec/batch)
2016-10-15 02:16:16.968021: step 14100, loss = 0.79 (750.7 examples/sec; 0.171 sec/batch)
2016-10-15 02:16:18.908077: step 14110, loss = 0.85 (759.5 examples/sec; 0.169 sec/batch)
2016-10-15 02:16:20.671072: step 14120, loss = 1.09 (739.6 examples/sec; 0.173 sec/batch)
2016-10-15 02:16:22.430943: step 14130, loss = 0.75 (720.2 examples/sec; 0.178 sec/batch)
2016-10-15 02:16:24.199736: step 14140, loss = 0.87 (705.8 examples/sec; 0.181 sec/batch)
2016-10-15 02:16:25.960496: step 14150, loss = 0.73 (718.1 examples/sec; 0.178 sec/batch)
2016-10-15 02:16:27.712624: step 14160, loss = 0.80 (723.2 examples/sec; 0.177 sec/batch)
2016-10-15 02:16:29.474111: step 14170, loss = 0.72 (728.1 examples/sec; 0.176 sec/batch)
2016-10-15 02:16:31.234198: step 14180, loss = 0.91 (725.0 examples/sec; 0.177 sec/batch)
2016-10-15 02:16:32.996844: step 14190, loss = 0.82 (730.3 examples/sec; 0.175 sec/batch)
2016-10-15 02:16:34.764940: step 14200, loss = 0.94 (717.4 examples/sec; 0.178 sec/batch)
2016-10-15 02:16:36.721898: step 14210, loss = 0.89 (711.5 examples/sec; 0.180 sec/batch)
2016-10-15 02:16:38.485272: step 14220, loss = 0.86 (710.0 examples/sec; 0.180 sec/batch)
2016-10-15 02:16:40.242714: step 14230, loss = 0.93 (733.5 examples/sec; 0.175 sec/batch)
2016-10-15 02:16:42.006371: step 14240, loss = 0.87 (723.6 examples/sec; 0.177 sec/batch)
2016-10-15 02:16:43.768529: step 14250, loss = 0.73 (724.1 examples/sec; 0.177 sec/batch)
2016-10-15 02:16:45.522014: step 14260, loss = 0.94 (744.1 examples/sec; 0.172 sec/batch)
2016-10-15 02:16:47.279593: step 14270, loss = 0.89 (713.1 examples/sec; 0.180 sec/batch)
2016-10-15 02:16:49.029762: step 14280, loss = 0.89 (716.0 examples/sec; 0.179 sec/batch)
2016-10-15 02:16:50.777696: step 14290, loss = 0.86 (749.7 examples/sec; 0.171 sec/batch)
2016-10-15 02:16:52.537305: step 14300, loss = 0.68 (730.5 examples/sec; 0.175 sec/batch)
2016-10-15 02:16:54.479024: step 14310, loss = 1.03 (719.2 examples/sec; 0.178 sec/batch)
2016-10-15 02:16:56.234924: step 14320, loss = 0.82 (715.5 examples/sec; 0.179 sec/batch)
2016-10-15 02:16:58.002896: step 14330, loss = 0.92 (735.7 examples/sec; 0.174 sec/batch)
2016-10-15 02:16:59.766421: step 14340, loss = 0.98 (708.9 examples/sec; 0.181 sec/batch)
2016-10-15 02:17:01.522370: step 14350, loss = 0.81 (727.7 examples/sec; 0.176 sec/batch)
2016-10-15 02:17:03.278861: step 14360, loss = 0.78 (764.4 examples/sec; 0.167 sec/batch)
2016-10-15 02:17:05.048074: step 14370, loss = 0.78 (717.2 examples/sec; 0.178 sec/batch)
2016-10-15 02:17:06.797389: step 14380, loss = 0.90 (738.0 examples/sec; 0.173 sec/batch)
2016-10-15 02:17:08.565237: step 14390, loss = 0.76 (710.0 examples/sec; 0.180 sec/batch)
2016-10-15 02:17:10.329812: step 14400, loss = 0.88 (739.8 examples/sec; 0.173 sec/batch)
2016-10-15 02:17:12.269851: step 14410, loss = 0.80 (734.9 examples/sec; 0.174 sec/batch)
2016-10-15 02:17:14.033261: step 14420, loss = 0.92 (720.8 examples/sec; 0.178 sec/batch)
2016-10-15 02:17:15.784625: step 14430, loss = 1.00 (714.1 examples/sec; 0.179 sec/batch)
2016-10-15 02:17:17.540218: step 14440, loss = 0.84 (733.9 examples/sec; 0.174 sec/batch)
2016-10-15 02:17:19.297152: step 14450, loss = 0.69 (730.5 examples/sec; 0.175 sec/batch)
2016-10-15 02:17:21.048469: step 14460, loss = 0.86 (733.6 examples/sec; 0.174 sec/batch)
2016-10-15 02:17:22.806649: step 14470, loss = 0.87 (706.3 examples/sec; 0.181 sec/batch)
2016-10-15 02:17:24.575284: step 14480, loss = 0.80 (735.5 examples/sec; 0.174 sec/batch)
2016-10-15 02:17:26.337973: step 14490, loss = 0.86 (726.1 examples/sec; 0.176 sec/batch)
2016-10-15 02:17:28.089342: step 14500, loss = 0.81 (710.2 examples/sec; 0.180 sec/batch)
2016-10-15 02:17:30.056037: step 14510, loss = 0.92 (717.0 examples/sec; 0.179 sec/batch)
2016-10-15 02:17:31.818096: step 14520, loss = 0.86 (742.5 examples/sec; 0.172 sec/batch)
2016-10-15 02:17:33.575946: step 14530, loss = 0.79 (738.8 examples/sec; 0.173 sec/batch)
2016-10-15 02:17:35.337849: step 14540, loss = 0.91 (737.6 examples/sec; 0.174 sec/batch)
2016-10-15 02:17:37.112123: step 14550, loss = 0.74 (671.8 examples/sec; 0.191 sec/batch)
2016-10-15 02:17:38.870942: step 14560, loss = 0.75 (692.2 examples/sec; 0.185 sec/batch)
2016-10-15 02:17:40.620179: step 14570, loss = 0.79 (722.3 examples/sec; 0.177 sec/batch)
2016-10-15 02:17:42.388217: step 14580, loss = 0.94 (711.3 examples/sec; 0.180 sec/batch)
2016-10-15 02:17:44.146032: step 14590, loss = 0.86 (727.4 examples/sec; 0.176 sec/batch)
2016-10-15 02:17:45.899037: step 14600, loss = 0.87 (720.9 examples/sec; 0.178 sec/batch)
2016-10-15 02:17:47.842363: step 14610, loss = 0.93 (726.7 examples/sec; 0.176 sec/batch)
2016-10-15 02:17:49.607003: step 14620, loss = 0.84 (727.0 examples/sec; 0.176 sec/batch)
2016-10-15 02:17:51.365982: step 14630, loss = 0.78 (721.0 examples/sec; 0.178 sec/batch)
2016-10-15 02:17:53.136233: step 14640, loss = 0.79 (737.9 examples/sec; 0.173 sec/batch)
2016-10-15 02:17:54.896111: step 14650, loss = 0.73 (742.3 examples/sec; 0.172 sec/batch)
2016-10-15 02:17:56.658150: step 14660, loss = 0.81 (727.7 examples/sec; 0.176 sec/batch)
2016-10-15 02:17:58.422978: step 14670, loss = 0.89 (733.9 examples/sec; 0.174 sec/batch)
2016-10-15 02:18:00.188482: step 14680, loss = 0.92 (721.3 examples/sec; 0.177 sec/batch)
2016-10-15 02:18:01.961710: step 14690, loss = 0.78 (719.9 examples/sec; 0.178 sec/batch)
2016-10-15 02:18:03.729652: step 14700, loss = 0.76 (711.2 examples/sec; 0.180 sec/batch)
2016-10-15 02:18:05.687619: step 14710, loss = 0.96 (724.0 examples/sec; 0.177 sec/batch)
2016-10-15 02:18:07.452595: step 14720, loss = 1.04 (717.9 examples/sec; 0.178 sec/batch)
2016-10-15 02:18:09.214918: step 14730, loss = 0.78 (770.3 examples/sec; 0.166 sec/batch)
2016-10-15 02:18:10.983015: step 14740, loss = 0.80 (706.3 examples/sec; 0.181 sec/batch)
2016-10-15 02:18:12.735200: step 14750, loss = 0.70 (747.4 examples/sec; 0.171 sec/batch)
2016-10-15 02:18:14.499274: step 14760, loss = 0.79 (723.2 examples/sec; 0.177 sec/batch)
2016-10-15 02:18:16.246541: step 14770, loss = 0.79 (730.7 examples/sec; 0.175 sec/batch)
2016-10-15 02:18:18.009452: step 14780, loss = 0.84 (736.4 examples/sec; 0.174 sec/batch)
2016-10-15 02:18:19.760988: step 14790, loss = 0.91 (748.1 examples/sec; 0.171 sec/batch)
2016-10-15 02:18:21.518487: step 14800, loss = 0.83 (765.3 examples/sec; 0.167 sec/batch)
2016-10-15 02:18:23.501869: step 14810, loss = 0.72 (697.8 examples/sec; 0.183 sec/batch)
2016-10-15 02:18:25.257860: step 14820, loss = 0.80 (720.8 examples/sec; 0.178 sec/batch)
2016-10-15 02:18:27.014346: step 14830, loss = 0.79 (744.0 examples/sec; 0.172 sec/batch)
2016-10-15 02:18:28.785276: step 14840, loss = 0.86 (719.2 examples/sec; 0.178 sec/batch)
2016-10-15 02:18:30.536387: step 14850, loss = 0.66 (754.1 examples/sec; 0.170 sec/batch)
2016-10-15 02:18:32.298888: step 14860, loss = 0.83 (739.6 examples/sec; 0.173 sec/batch)
2016-10-15 02:18:34.065957: step 14870, loss = 0.96 (771.4 examples/sec; 0.166 sec/batch)
2016-10-15 02:18:35.825753: step 14880, loss = 0.87 (719.2 examples/sec; 0.178 sec/batch)
2016-10-15 02:18:37.587889: step 14890, loss = 0.79 (745.9 examples/sec; 0.172 sec/batch)
2016-10-15 02:18:39.342243: step 14900, loss = 1.05 (749.9 examples/sec; 0.171 sec/batch)
2016-10-15 02:18:41.291057: step 14910, loss = 0.72 (693.0 examples/sec; 0.185 sec/batch)
2016-10-15 02:18:43.038093: step 14920, loss = 1.02 (704.8 examples/sec; 0.182 sec/batch)
2016-10-15 02:18:44.793134: step 14930, loss = 0.97 (750.1 examples/sec; 0.171 sec/batch)
2016-10-15 02:18:46.554194: step 14940, loss = 0.84 (749.5 examples/sec; 0.171 sec/batch)
2016-10-15 02:18:48.323772: step 14950, loss = 0.72 (721.6 examples/sec; 0.177 sec/batch)
2016-10-15 02:18:50.087692: step 14960, loss = 0.80 (735.9 examples/sec; 0.174 sec/batch)
2016-10-15 02:18:51.857255: step 14970, loss = 0.97 (712.0 examples/sec; 0.180 sec/batch)
2016-10-15 02:18:53.614987: step 14980, loss = 0.65 (721.4 examples/sec; 0.177 sec/batch)
2016-10-15 02:18:55.369248: step 14990, loss = 0.82 (733.6 examples/sec; 0.174 sec/batch)
2016-10-15 02:18:57.130976: step 15000, loss = 0.75 (729.6 examples/sec; 0.175 sec/batch)
2016-10-15 02:18:59.655328: step 15010, loss = 0.81 (728.0 examples/sec; 0.176 sec/batch)
2016-10-15 02:19:01.416579: step 15020, loss = 0.89 (710.5 examples/sec; 0.180 sec/batch)
2016-10-15 02:19:03.174480: step 15030, loss = 0.71 (729.0 examples/sec; 0.176 sec/batch)
2016-10-15 02:19:04.935718: step 15040, loss = 1.00 (735.7 examples/sec; 0.174 sec/batch)
2016-10-15 02:19:06.691879: step 15050, loss = 0.85 (787.5 examples/sec; 0.163 sec/batch)
2016-10-15 02:19:08.454821: step 15060, loss = 0.99 (737.4 examples/sec; 0.174 sec/batch)
2016-10-15 02:19:10.214429: step 15070, loss = 0.90 (723.3 examples/sec; 0.177 sec/batch)
2016-10-15 02:19:11.969616: step 15080, loss = 0.79 (729.8 examples/sec; 0.175 sec/batch)
2016-10-15 02:19:13.733241: step 15090, loss = 0.88 (733.3 examples/sec; 0.175 sec/batch)
2016-10-15 02:19:15.489756: step 15100, loss = 0.87 (740.2 examples/sec; 0.173 sec/batch)
2016-10-15 02:19:17.440814: step 15110, loss = 0.79 (729.1 examples/sec; 0.176 sec/batch)
2016-10-15 02:19:19.208449: step 15120, loss = 0.81 (744.8 examples/sec; 0.172 sec/batch)
2016-10-15 02:19:20.976833: step 15130, loss = 1.03 (696.6 examples/sec; 0.184 sec/batch)
2016-10-15 02:19:22.720643: step 15140, loss = 0.79 (752.1 examples/sec; 0.170 sec/batch)
2016-10-15 02:19:24.475205: step 15150, loss = 0.81 (729.2 examples/sec; 0.176 sec/batch)
2016-10-15 02:19:26.239777: step 15160, loss = 0.80 (732.2 examples/sec; 0.175 sec/batch)
2016-10-15 02:19:27.989293: step 15170, loss = 0.81 (730.7 examples/sec; 0.175 sec/batch)
2016-10-15 02:19:29.756162: step 15180, loss = 0.94 (713.1 examples/sec; 0.180 sec/batch)
2016-10-15 02:19:31.517837: step 15190, loss = 0.91 (729.1 examples/sec; 0.176 sec/batch)
2016-10-15 02:19:33.273010: step 15200, loss = 0.90 (725.4 examples/sec; 0.176 sec/batch)
2016-10-15 02:19:35.212383: step 15210, loss = 0.92 (722.4 examples/sec; 0.177 sec/batch)
2016-10-15 02:19:36.976287: step 15220, loss = 0.78 (725.5 examples/sec; 0.176 sec/batch)
2016-10-15 02:19:38.737582: step 15230, loss = 1.05 (734.0 examples/sec; 0.174 sec/batch)
2016-10-15 02:19:40.498451: step 15240, loss = 0.93 (747.0 examples/sec; 0.171 sec/batch)
2016-10-15 02:19:42.272252: step 15250, loss = 0.90 (692.8 examples/sec; 0.185 sec/batch)
2016-10-15 02:19:44.017231: step 15260, loss = 0.82 (719.4 examples/sec; 0.178 sec/batch)
2016-10-15 02:19:45.773362: step 15270, loss = 0.81 (739.5 examples/sec; 0.173 sec/batch)
2016-10-15 02:19:47.525550: step 15280, loss = 0.84 (742.0 examples/sec; 0.173 sec/batch)
2016-10-15 02:19:49.289871: step 15290, loss = 0.74 (731.1 examples/sec; 0.175 sec/batch)
2016-10-15 02:19:51.041968: step 15300, loss = 0.97 (720.8 examples/sec; 0.178 sec/batch)
2016-10-15 02:19:52.987840: step 15310, loss = 0.76 (726.6 examples/sec; 0.176 sec/batch)
2016-10-15 02:19:54.763001: step 15320, loss = 0.79 (722.1 examples/sec; 0.177 sec/batch)
2016-10-15 02:19:56.520920: step 15330, loss = 0.84 (715.4 examples/sec; 0.179 sec/batch)
2016-10-15 02:19:58.283366: step 15340, loss = 0.81 (756.6 examples/sec; 0.169 sec/batch)
2016-10-15 02:20:00.044541: step 15350, loss = 0.96 (717.9 examples/sec; 0.178 sec/batch)
2016-10-15 02:20:01.832821: step 15360, loss = 0.80 (716.4 examples/sec; 0.179 sec/batch)
2016-10-15 02:20:03.593078: step 15370, loss = 0.89 (726.5 examples/sec; 0.176 sec/batch)
2016-10-15 02:20:05.357208: step 15380, loss = 0.80 (714.2 examples/sec; 0.179 sec/batch)
2016-10-15 02:20:07.116281: step 15390, loss = 0.81 (742.1 examples/sec; 0.172 sec/batch)
2016-10-15 02:20:08.884177: step 15400, loss = 0.84 (738.2 examples/sec; 0.173 sec/batch)
2016-10-15 02:20:10.833246: step 15410, loss = 0.82 (742.8 examples/sec; 0.172 sec/batch)
2016-10-15 02:20:12.601140: step 15420, loss = 0.94 (736.3 examples/sec; 0.174 sec/batch)
2016-10-15 02:20:14.378753: step 15430, loss = 0.66 (723.7 examples/sec; 0.177 sec/batch)
2016-10-15 02:20:16.144938: step 15440, loss = 1.04 (718.8 examples/sec; 0.178 sec/batch)
2016-10-15 02:20:17.908147: step 15450, loss = 0.75 (731.4 examples/sec; 0.175 sec/batch)
2016-10-15 02:20:19.664601: step 15460, loss = 0.90 (761.1 examples/sec; 0.168 sec/batch)
2016-10-15 02:20:21.435768: step 15470, loss = 0.84 (731.5 examples/sec; 0.175 sec/batch)
2016-10-15 02:20:23.192945: step 15480, loss = 1.14 (731.2 examples/sec; 0.175 sec/batch)
2016-10-15 02:20:24.960540: step 15490, loss = 0.78 (734.5 examples/sec; 0.174 sec/batch)
2016-10-15 02:20:26.720538: step 15500, loss = 0.94 (736.8 examples/sec; 0.174 sec/batch)
2016-10-15 02:20:28.710123: step 15510, loss = 0.88 (741.8 examples/sec; 0.173 sec/batch)
2016-10-15 02:20:30.479328: step 15520, loss = 0.83 (720.4 examples/sec; 0.178 sec/batch)
2016-10-15 02:20:32.243912: step 15530, loss = 0.82 (716.8 examples/sec; 0.179 sec/batch)
2016-10-15 02:20:34.016365: step 15540, loss = 1.01 (717.8 examples/sec; 0.178 sec/batch)
2016-10-15 02:20:35.784136: step 15550, loss = 0.82 (713.1 examples/sec; 0.179 sec/batch)
2016-10-15 02:20:37.546043: step 15560, loss = 0.89 (745.1 examples/sec; 0.172 sec/batch)
2016-10-15 02:20:39.309411: step 15570, loss = 0.95 (715.1 examples/sec; 0.179 sec/batch)
2016-10-15 02:20:41.065597: step 15580, loss = 0.81 (733.9 examples/sec; 0.174 sec/batch)
2016-10-15 02:20:42.826043: step 15590, loss = 0.81 (719.4 examples/sec; 0.178 sec/batch)
2016-10-15 02:20:44.591224: step 15600, loss = 0.91 (720.8 examples/sec; 0.178 sec/batch)
2016-10-15 02:20:46.542043: step 15610, loss = 0.86 (715.1 examples/sec; 0.179 sec/batch)
2016-10-15 02:20:48.306770: step 15620, loss = 0.94 (696.8 examples/sec; 0.184 sec/batch)
2016-10-15 02:20:50.069288: step 15630, loss = 0.87 (732.6 examples/sec; 0.175 sec/batch)
2016-10-15 02:20:51.828370: step 15640, loss = 0.78 (743.6 examples/sec; 0.172 sec/batch)
2016-10-15 02:20:53.600015: step 15650, loss = 0.81 (712.0 examples/sec; 0.180 sec/batch)
2016-10-15 02:20:55.370793: step 15660, loss = 0.86 (719.1 examples/sec; 0.178 sec/batch)
2016-10-15 02:20:57.141921: step 15670, loss = 0.92 (719.7 examples/sec; 0.178 sec/batch)
2016-10-15 02:20:58.900414: step 15680, loss = 0.86 (741.9 examples/sec; 0.173 sec/batch)
2016-10-15 02:21:00.661810: step 15690, loss = 0.94 (743.9 examples/sec; 0.172 sec/batch)
2016-10-15 02:21:02.436336: step 15700, loss = 0.85 (678.6 examples/sec; 0.189 sec/batch)
2016-10-15 02:21:04.380545: step 15710, loss = 0.81 (713.8 examples/sec; 0.179 sec/batch)
2016-10-15 02:21:06.140676: step 15720, loss = 0.73 (722.7 examples/sec; 0.177 sec/batch)
2016-10-15 02:21:07.897206: step 15730, loss = 0.93 (736.7 examples/sec; 0.174 sec/batch)
2016-10-15 02:21:09.665058: step 15740, loss = 0.84 (733.5 examples/sec; 0.175 sec/batch)
2016-10-15 02:21:11.433571: step 15750, loss = 0.80 (729.8 examples/sec; 0.175 sec/batch)
2016-10-15 02:21:13.199676: step 15760, loss = 0.81 (731.6 examples/sec; 0.175 sec/batch)
2016-10-15 02:21:14.971309: step 15770, loss = 0.74 (725.3 examples/sec; 0.176 sec/batch)
2016-10-15 02:21:16.734651: step 15780, loss = 0.85 (741.5 examples/sec; 0.173 sec/batch)
2016-10-15 02:21:18.508099: step 15790, loss = 0.85 (735.1 examples/sec; 0.174 sec/batch)
2016-10-15 02:21:20.271619: step 15800, loss = 0.91 (729.9 examples/sec; 0.175 sec/batch)
2016-10-15 02:21:22.241952: step 15810, loss = 0.93 (750.3 examples/sec; 0.171 sec/batch)
2016-10-15 02:21:23.999186: step 15820, loss = 0.97 (747.1 examples/sec; 0.171 sec/batch)
2016-10-15 02:21:25.774887: step 15830, loss = 0.85 (720.0 examples/sec; 0.178 sec/batch)
2016-10-15 02:21:27.537204: step 15840, loss = 0.81 (708.3 examples/sec; 0.181 sec/batch)
2016-10-15 02:21:29.305931: step 15850, loss = 0.82 (730.2 examples/sec; 0.175 sec/batch)
2016-10-15 02:21:31.065884: step 15860, loss = 0.78 (740.3 examples/sec; 0.173 sec/batch)
2016-10-15 02:21:32.833808: step 15870, loss = 0.83 (737.5 examples/sec; 0.174 sec/batch)
2016-10-15 02:21:34.599561: step 15880, loss = 0.80 (734.0 examples/sec; 0.174 sec/batch)
2016-10-15 02:21:36.357787: step 15890, loss = 0.90 (731.1 examples/sec; 0.175 sec/batch)
2016-10-15 02:21:38.123365: step 15900, loss = 0.85 (719.3 examples/sec; 0.178 sec/batch)
2016-10-15 02:21:40.076058: step 15910, loss = 0.76 (747.1 examples/sec; 0.171 sec/batch)
2016-10-15 02:21:41.838346: step 15920, loss = 0.84 (711.6 examples/sec; 0.180 sec/batch)
2016-10-15 02:21:43.602877: step 15930, loss = 0.88 (740.1 examples/sec; 0.173 sec/batch)
2016-10-15 02:21:45.365567: step 15940, loss = 0.72 (721.5 examples/sec; 0.177 sec/batch)
2016-10-15 02:21:47.122533: step 15950, loss = 0.99 (731.7 examples/sec; 0.175 sec/batch)
2016-10-15 02:21:48.894845: step 15960, loss = 0.82 (673.5 examples/sec; 0.190 sec/batch)
2016-10-15 02:21:50.643070: step 15970, loss = 0.85 (732.3 examples/sec; 0.175 sec/batch)
2016-10-15 02:21:52.402424: step 15980, loss = 0.69 (722.9 examples/sec; 0.177 sec/batch)
2016-10-15 02:21:54.173992: step 15990, loss = 0.89 (702.0 examples/sec; 0.182 sec/batch)
2016-10-15 02:21:55.932055: step 16000, loss = 0.91 (729.7 examples/sec; 0.175 sec/batch)
2016-10-15 02:21:58.456231: step 16010, loss = 0.71 (738.8 examples/sec; 0.173 sec/batch)
2016-10-15 02:22:00.214265: step 16020, loss = 0.92 (755.3 examples/sec; 0.169 sec/batch)
2016-10-15 02:22:01.974091: step 16030, loss = 0.66 (714.6 examples/sec; 0.179 sec/batch)
2016-10-15 02:22:03.731586: step 16040, loss = 0.90 (735.0 examples/sec; 0.174 sec/batch)
2016-10-15 02:22:05.496091: step 16050, loss = 0.84 (732.4 examples/sec; 0.175 sec/batch)
2016-10-15 02:22:07.257693: step 16060, loss = 0.96 (731.9 examples/sec; 0.175 sec/batch)
2016-10-15 02:22:09.030336: step 16070, loss = 0.73 (726.7 examples/sec; 0.176 sec/batch)
2016-10-15 02:22:10.783584: step 16080, loss = 0.67 (734.9 examples/sec; 0.174 sec/batch)
2016-10-15 02:22:12.539254: step 16090, loss = 0.94 (722.9 examples/sec; 0.177 sec/batch)
2016-10-15 02:22:14.298203: step 16100, loss = 0.74 (725.7 examples/sec; 0.176 sec/batch)
2016-10-15 02:22:16.238383: step 16110, loss = 0.83 (737.8 examples/sec; 0.173 sec/batch)
2016-10-15 02:22:17.999596: step 16120, loss = 1.02 (715.9 examples/sec; 0.179 sec/batch)
2016-10-15 02:22:19.755691: step 16130, loss = 1.13 (762.7 examples/sec; 0.168 sec/batch)
2016-10-15 02:22:21.526521: step 16140, loss = 0.89 (707.4 examples/sec; 0.181 sec/batch)
2016-10-15 02:22:23.279772: step 16150, loss = 0.96 (720.1 examples/sec; 0.178 sec/batch)
2016-10-15 02:22:25.030134: step 16160, loss = 0.85 (736.5 examples/sec; 0.174 sec/batch)
2016-10-15 02:22:26.789537: step 16170, loss = 0.84 (734.1 examples/sec; 0.174 sec/batch)
2016-10-15 02:22:28.552958: step 16180, loss = 0.81 (727.1 examples/sec; 0.176 sec/batch)
2016-10-15 02:22:30.308450: step 16190, loss = 0.83 (756.3 examples/sec; 0.169 sec/batch)
2016-10-15 02:22:32.067629: step 16200, loss = 0.68 (724.0 examples/sec; 0.177 sec/batch)
2016-10-15 02:22:34.028864: step 16210, loss = 0.98 (733.5 examples/sec; 0.174 sec/batch)
2016-10-15 02:22:35.790171: step 16220, loss = 0.73 (714.5 examples/sec; 0.179 sec/batch)
2016-10-15 02:22:37.550266: step 16230, loss = 0.93 (722.1 examples/sec; 0.177 sec/batch)
2016-10-15 02:22:39.322677: step 16240, loss = 0.89 (704.0 examples/sec; 0.182 sec/batch)
2016-10-15 02:22:41.077989: step 16250, loss = 0.98 (705.9 examples/sec; 0.181 sec/batch)
2016-10-15 02:22:42.825756: step 16260, loss = 0.84 (741.4 examples/sec; 0.173 sec/batch)
2016-10-15 02:22:44.585757: step 16270, loss = 0.86 (710.9 examples/sec; 0.180 sec/batch)
2016-10-15 02:22:46.340075: step 16280, loss = 0.72 (718.1 examples/sec; 0.178 sec/batch)
2016-10-15 02:22:48.098714: step 16290, loss = 0.87 (720.9 examples/sec; 0.178 sec/batch)
2016-10-15 02:22:49.858757: step 16300, loss = 0.84 (729.0 examples/sec; 0.176 sec/batch)
2016-10-15 02:22:51.795721: step 16310, loss = 0.84 (737.7 examples/sec; 0.174 sec/batch)
2016-10-15 02:22:53.567887: step 16320, loss = 0.72 (715.9 examples/sec; 0.179 sec/batch)
2016-10-15 02:22:55.322423: step 16330, loss = 0.84 (734.5 examples/sec; 0.174 sec/batch)
2016-10-15 02:22:57.084482: step 16340, loss = 1.07 (732.4 examples/sec; 0.175 sec/batch)
2016-10-15 02:22:58.840833: step 16350, loss = 0.89 (735.0 examples/sec; 0.174 sec/batch)
2016-10-15 02:23:00.606930: step 16360, loss = 0.84 (689.4 examples/sec; 0.186 sec/batch)
2016-10-15 02:23:02.373439: step 16370, loss = 0.88 (722.0 examples/sec; 0.177 sec/batch)
2016-10-15 02:23:04.131987: step 16380, loss = 0.87 (728.3 examples/sec; 0.176 sec/batch)
2016-10-15 02:23:05.895267: step 16390, loss = 0.78 (732.1 examples/sec; 0.175 sec/batch)
2016-10-15 02:23:07.661933: step 16400, loss = 0.90 (704.3 examples/sec; 0.182 sec/batch)
2016-10-15 02:23:09.602258: step 16410, loss = 0.87 (733.1 examples/sec; 0.175 sec/batch)
2016-10-15 02:23:11.354675: step 16420, loss = 0.79 (729.8 examples/sec; 0.175 sec/batch)
2016-10-15 02:23:13.124599: step 16430, loss = 0.82 (729.7 examples/sec; 0.175 sec/batch)
2016-10-15 02:23:14.885790: step 16440, loss = 0.93 (764.4 examples/sec; 0.167 sec/batch)
2016-10-15 02:23:16.649660: step 16450, loss = 0.91 (740.8 examples/sec; 0.173 sec/batch)
2016-10-15 02:23:18.409677: step 16460, loss = 0.75 (728.0 examples/sec; 0.176 sec/batch)
2016-10-15 02:23:20.172759: step 16470, loss = 0.95 (702.7 examples/sec; 0.182 sec/batch)
2016-10-15 02:23:21.929592: step 16480, loss = 0.78 (723.5 examples/sec; 0.177 sec/batch)
2016-10-15 02:23:23.689727: step 16490, loss = 0.88 (723.1 examples/sec; 0.177 sec/batch)
2016-10-15 02:23:25.456670: step 16500, loss = 0.82 (712.6 examples/sec; 0.180 sec/batch)
2016-10-15 02:23:27.400999: step 16510, loss = 0.91 (720.2 examples/sec; 0.178 sec/batch)
2016-10-15 02:23:29.159423: step 16520, loss = 0.82 (727.1 examples/sec; 0.176 sec/batch)
2016-10-15 02:23:30.918787: step 16530, loss = 0.84 (725.7 examples/sec; 0.176 sec/batch)
2016-10-15 02:23:32.681970: step 16540, loss = 0.79 (719.1 examples/sec; 0.178 sec/batch)
2016-10-15 02:23:34.437474: step 16550, loss = 0.91 (726.8 examples/sec; 0.176 sec/batch)
2016-10-15 02:23:36.187203: step 16560, loss = 0.91 (747.6 examples/sec; 0.171 sec/batch)
2016-10-15 02:23:37.948284: step 16570, loss = 0.76 (739.3 examples/sec; 0.173 sec/batch)
2016-10-15 02:23:39.704559: step 16580, loss = 0.71 (720.3 examples/sec; 0.178 sec/batch)
2016-10-15 02:23:41.456942: step 16590, loss = 0.79 (733.6 examples/sec; 0.174 sec/batch)
2016-10-15 02:23:43.218456: step 16600, loss = 0.98 (720.4 examples/sec; 0.178 sec/batch)
2016-10-15 02:23:45.167929: step 16610, loss = 0.74 (719.7 examples/sec; 0.178 sec/batch)
2016-10-15 02:23:46.927071: step 16620, loss = 0.85 (715.6 examples/sec; 0.179 sec/batch)
2016-10-15 02:23:48.692204: step 16630, loss = 0.85 (712.6 examples/sec; 0.180 sec/batch)
2016-10-15 02:23:50.456533: step 16640, loss = 0.77 (728.5 examples/sec; 0.176 sec/batch)
2016-10-15 02:23:52.229052: step 16650, loss = 0.84 (710.8 examples/sec; 0.180 sec/batch)
2016-10-15 02:23:53.996610: step 16660, loss = 0.81 (734.9 examples/sec; 0.174 sec/batch)
2016-10-15 02:23:55.761085: step 16670, loss = 0.87 (712.0 examples/sec; 0.180 sec/batch)
2016-10-15 02:23:57.526810: step 16680, loss = 0.93 (717.1 examples/sec; 0.178 sec/batch)
2016-10-15 02:23:59.292577: step 16690, loss = 0.99 (734.7 examples/sec; 0.174 sec/batch)
2016-10-15 02:24:01.057041: step 16700, loss = 0.70 (721.2 examples/sec; 0.177 sec/batch)
2016-10-15 02:24:03.017337: step 16710, loss = 0.88 (703.3 examples/sec; 0.182 sec/batch)
2016-10-15 02:24:04.790629: step 16720, loss = 1.02 (704.5 examples/sec; 0.182 sec/batch)
2016-10-15 02:24:06.555412: step 16730, loss = 0.78 (711.8 examples/sec; 0.180 sec/batch)
2016-10-15 02:24:08.316621: step 16740, loss = 0.84 (722.3 examples/sec; 0.177 sec/batch)
2016-10-15 02:24:10.086165: step 16750, loss = 1.07 (718.6 examples/sec; 0.178 sec/batch)
2016-10-15 02:24:11.854845: step 16760, loss = 0.85 (745.0 examples/sec; 0.172 sec/batch)
2016-10-15 02:24:13.616604: step 16770, loss = 0.85 (775.4 examples/sec; 0.165 sec/batch)
2016-10-15 02:24:15.385864: step 16780, loss = 0.78 (708.8 examples/sec; 0.181 sec/batch)
2016-10-15 02:24:17.144577: step 16790, loss = 0.89 (725.0 examples/sec; 0.177 sec/batch)
2016-10-15 02:24:18.904963: step 16800, loss = 0.76 (721.4 examples/sec; 0.177 sec/batch)
2016-10-15 02:24:20.853407: step 16810, loss = 0.84 (719.6 examples/sec; 0.178 sec/batch)
2016-10-15 02:24:22.624068: step 16820, loss = 0.97 (715.1 examples/sec; 0.179 sec/batch)
2016-10-15 02:24:24.396799: step 16830, loss = 0.99 (714.4 examples/sec; 0.179 sec/batch)
2016-10-15 02:24:26.152911: step 16840, loss = 0.79 (733.9 examples/sec; 0.174 sec/batch)
2016-10-15 02:24:27.908042: step 16850, loss = 0.67 (742.3 examples/sec; 0.172 sec/batch)
2016-10-15 02:24:29.667216: step 16860, loss = 0.75 (720.8 examples/sec; 0.178 sec/batch)
2016-10-15 02:24:31.432051: step 16870, loss = 0.90 (710.0 examples/sec; 0.180 sec/batch)
2016-10-15 02:24:33.196095: step 16880, loss = 0.94 (714.3 examples/sec; 0.179 sec/batch)
2016-10-15 02:24:34.945989: step 16890, loss = 0.95 (734.8 examples/sec; 0.174 sec/batch)
2016-10-15 02:24:36.707471: step 16900, loss = 0.67 (766.2 examples/sec; 0.167 sec/batch)
2016-10-15 02:24:38.650457: step 16910, loss = 0.81 (727.1 examples/sec; 0.176 sec/batch)
2016-10-15 02:24:40.403306: step 16920, loss = 0.89 (733.8 examples/sec; 0.174 sec/batch)
2016-10-15 02:24:42.178118: step 16930, loss = 0.78 (709.0 examples/sec; 0.181 sec/batch)
2016-10-15 02:24:43.932952: step 16940, loss = 0.88 (738.1 examples/sec; 0.173 sec/batch)
2016-10-15 02:24:45.687536: step 16950, loss = 0.76 (735.5 examples/sec; 0.174 sec/batch)
2016-10-15 02:24:47.448805: step 16960, loss = 0.88 (718.8 examples/sec; 0.178 sec/batch)
2016-10-15 02:24:49.214261: step 16970, loss = 0.79 (755.1 examples/sec; 0.170 sec/batch)
2016-10-15 02:24:50.967137: step 16980, loss = 0.78 (734.2 examples/sec; 0.174 sec/batch)
2016-10-15 02:24:52.720362: step 16990, loss = 0.85 (721.5 examples/sec; 0.177 sec/batch)
2016-10-15 02:24:54.480252: step 17000, loss = 0.82 (734.6 examples/sec; 0.174 sec/batch)
2016-10-15 02:24:57.002065: step 17010, loss = 1.03 (746.5 examples/sec; 0.171 sec/batch)
2016-10-15 02:24:58.763496: step 17020, loss = 0.78 (751.7 examples/sec; 0.170 sec/batch)
2016-10-15 02:25:00.528975: step 17030, loss = 0.69 (726.7 examples/sec; 0.176 sec/batch)
2016-10-15 02:25:02.307941: step 17040, loss = 0.91 (754.6 examples/sec; 0.170 sec/batch)
2016-10-15 02:25:04.068178: step 17050, loss = 0.79 (714.9 examples/sec; 0.179 sec/batch)
2016-10-15 02:25:05.828287: step 17060, loss = 0.89 (708.5 examples/sec; 0.181 sec/batch)
2016-10-15 02:25:07.593376: step 17070, loss = 0.71 (721.4 examples/sec; 0.177 sec/batch)
2016-10-15 02:25:09.349765: step 17080, loss = 0.74 (732.2 examples/sec; 0.175 sec/batch)
2016-10-15 02:25:11.101584: step 17090, loss = 0.80 (741.2 examples/sec; 0.173 sec/batch)
2016-10-15 02:25:12.853311: step 17100, loss = 0.96 (745.4 examples/sec; 0.172 sec/batch)
2016-10-15 02:25:14.812676: step 17110, loss = 1.04 (722.0 examples/sec; 0.177 sec/batch)
2016-10-15 02:25:16.578038: step 17120, loss = 0.79 (713.8 examples/sec; 0.179 sec/batch)
2016-10-15 02:25:18.336226: step 17130, loss = 0.92 (739.7 examples/sec; 0.173 sec/batch)
2016-10-15 02:25:20.098753: step 17140, loss = 0.83 (732.8 examples/sec; 0.175 sec/batch)
2016-10-15 02:25:21.863565: step 17150, loss = 0.84 (716.0 examples/sec; 0.179 sec/batch)
2016-10-15 02:25:23.625747: step 17160, loss = 0.90 (732.9 examples/sec; 0.175 sec/batch)
2016-10-15 02:25:25.391266: step 17170, loss = 0.68 (716.3 examples/sec; 0.179 sec/batch)
2016-10-15 02:25:27.154322: step 17180, loss = 0.75 (736.4 examples/sec; 0.174 sec/batch)
2016-10-15 02:25:28.904370: step 17190, loss = 0.96 (728.4 examples/sec; 0.176 sec/batch)
2016-10-15 02:25:30.671710: step 17200, loss = 0.74 (707.4 examples/sec; 0.181 sec/batch)
2016-10-15 02:25:32.621469: step 17210, loss = 0.84 (726.0 examples/sec; 0.176 sec/batch)
2016-10-15 02:25:34.375790: step 17220, loss = 0.85 (740.7 examples/sec; 0.173 sec/batch)
2016-10-15 02:25:36.134097: step 17230, loss = 0.83 (746.3 examples/sec; 0.172 sec/batch)
2016-10-15 02:25:37.895505: step 17240, loss = 0.76 (740.5 examples/sec; 0.173 sec/batch)
2016-10-15 02:25:39.655691: step 17250, loss = 0.87 (734.0 examples/sec; 0.174 sec/batch)
2016-10-15 02:25:41.402480: step 17260, loss = 0.88 (744.9 examples/sec; 0.172 sec/batch)
2016-10-15 02:25:43.160266: step 17270, loss = 0.87 (734.6 examples/sec; 0.174 sec/batch)
2016-10-15 02:25:44.918950: step 17280, loss = 0.89 (708.2 examples/sec; 0.181 sec/batch)
2016-10-15 02:25:46.678640: step 17290, loss = 0.82 (727.6 examples/sec; 0.176 sec/batch)
2016-10-15 02:25:48.431507: step 17300, loss = 0.80 (749.0 examples/sec; 0.171 sec/batch)
2016-10-15 02:25:50.374476: step 17310, loss = 0.69 (738.1 examples/sec; 0.173 sec/batch)
2016-10-15 02:25:52.139113: step 17320, loss = 0.85 (711.6 examples/sec; 0.180 sec/batch)
2016-10-15 02:25:53.901642: step 17330, loss = 0.83 (704.9 examples/sec; 0.182 sec/batch)
2016-10-15 02:25:55.650353: step 17340, loss = 0.78 (743.6 examples/sec; 0.172 sec/batch)
2016-10-15 02:25:57.420410: step 17350, loss = 0.75 (740.2 examples/sec; 0.173 sec/batch)
2016-10-15 02:25:59.182755: step 17360, loss = 0.77 (720.1 examples/sec; 0.178 sec/batch)
2016-10-15 02:26:00.940468: step 17370, loss = 0.82 (721.0 examples/sec; 0.178 sec/batch)
2016-10-15 02:26:02.699115: step 17380, loss = 0.85 (748.7 examples/sec; 0.171 sec/batch)
2016-10-15 02:26:04.463634: step 17390, loss = 0.90 (731.5 examples/sec; 0.175 sec/batch)
2016-10-15 02:26:06.222277: step 17400, loss = 0.68 (739.6 examples/sec; 0.173 sec/batch)
2016-10-15 02:26:08.182839: step 17410, loss = 0.99 (712.7 examples/sec; 0.180 sec/batch)
2016-10-15 02:26:09.932804: step 17420, loss = 0.89 (751.7 examples/sec; 0.170 sec/batch)
2016-10-15 02:26:11.688542: step 17430, loss = 1.03 (738.9 examples/sec; 0.173 sec/batch)
2016-10-15 02:26:13.440116: step 17440, loss = 0.81 (729.7 examples/sec; 0.175 sec/batch)
2016-10-15 02:26:15.197059: step 17450, loss = 0.66 (743.2 examples/sec; 0.172 sec/batch)
2016-10-15 02:26:16.949558: step 17460, loss = 0.87 (738.4 examples/sec; 0.173 sec/batch)
2016-10-15 02:26:18.710413: step 17470, loss = 0.78 (735.7 examples/sec; 0.174 sec/batch)
2016-10-15 02:26:20.465635: step 17480, loss = 0.88 (751.7 examples/sec; 0.170 sec/batch)
2016-10-15 02:26:22.230134: step 17490, loss = 0.99 (741.5 examples/sec; 0.173 sec/batch)
2016-10-15 02:26:23.985632: step 17500, loss = 0.94 (755.6 examples/sec; 0.169 sec/batch)
2016-10-15 02:26:25.925747: step 17510, loss = 0.90 (729.2 examples/sec; 0.176 sec/batch)
2016-10-15 02:26:27.690209: step 17520, loss = 1.06 (705.3 examples/sec; 0.181 sec/batch)
2016-10-15 02:26:29.448664: step 17530, loss = 0.84 (724.9 examples/sec; 0.177 sec/batch)
2016-10-15 02:26:31.209159: step 17540, loss = 0.86 (739.1 examples/sec; 0.173 sec/batch)
2016-10-15 02:26:32.983937: step 17550, loss = 0.80 (696.7 examples/sec; 0.184 sec/batch)
2016-10-15 02:26:34.742117: step 17560, loss = 0.88 (749.1 examples/sec; 0.171 sec/batch)
2016-10-15 02:26:36.502967: step 17570, loss = 0.79 (735.5 examples/sec; 0.174 sec/batch)
2016-10-15 02:26:38.265302: step 17580, loss = 1.15 (745.4 examples/sec; 0.172 sec/batch)
2016-10-15 02:26:40.027127: step 17590, loss = 0.87 (760.5 examples/sec; 0.168 sec/batch)
2016-10-15 02:26:41.787910: step 17600, loss = 0.78 (750.7 examples/sec; 0.171 sec/batch)
2016-10-15 02:26:43.737903: step 17610, loss = 0.78 (743.7 examples/sec; 0.172 sec/batch)
2016-10-15 02:26:45.498608: step 17620, loss = 0.87 (741.2 examples/sec; 0.173 sec/batch)
2016-10-15 02:26:47.249302: step 17630, loss = 0.88 (734.6 examples/sec; 0.174 sec/batch)
2016-10-15 02:26:49.022246: step 17640, loss = 0.76 (750.7 examples/sec; 0.171 sec/batch)
2016-10-15 02:26:50.788151: step 17650, loss = 0.70 (729.0 examples/sec; 0.176 sec/batch)
2016-10-15 02:26:52.543764: step 17660, loss = 0.80 (727.2 examples/sec; 0.176 sec/batch)
2016-10-15 02:26:54.309182: step 17670, loss = 0.77 (716.7 examples/sec; 0.179 sec/batch)
2016-10-15 02:26:56.058723: step 17680, loss = 0.96 (728.7 examples/sec; 0.176 sec/batch)
2016-10-15 02:26:57.816554: step 17690, loss = 0.78 (715.1 examples/sec; 0.179 sec/batch)
2016-10-15 02:26:59.573158: step 17700, loss = 0.62 (723.9 examples/sec; 0.177 sec/batch)
2016-10-15 02:27:01.509876: step 17710, loss = 0.85 (726.0 examples/sec; 0.176 sec/batch)
2016-10-15 02:27:03.268890: step 17720, loss = 0.90 (715.0 examples/sec; 0.179 sec/batch)
2016-10-15 02:27:05.034469: step 17730, loss = 0.76 (738.1 examples/sec; 0.173 sec/batch)
2016-10-15 02:27:06.800819: step 17740, loss = 0.84 (703.0 examples/sec; 0.182 sec/batch)
2016-10-15 02:27:08.567740: step 17750, loss = 0.80 (706.7 examples/sec; 0.181 sec/batch)
2016-10-15 02:27:10.340334: step 17760, loss = 0.67 (735.5 examples/sec; 0.174 sec/batch)
2016-10-15 02:27:12.103338: step 17770, loss = 0.83 (735.5 examples/sec; 0.174 sec/batch)
2016-10-15 02:27:13.869587: step 17780, loss = 0.79 (725.4 examples/sec; 0.176 sec/batch)
2016-10-15 02:27:15.626227: step 17790, loss = 1.03 (719.0 examples/sec; 0.178 sec/batch)
2016-10-15 02:27:17.393720: step 17800, loss = 0.88 (692.1 examples/sec; 0.185 sec/batch)
2016-10-15 02:27:19.337579: step 17810, loss = 0.76 (725.8 examples/sec; 0.176 sec/batch)
2016-10-15 02:27:21.092722: step 17820, loss = 0.88 (740.1 examples/sec; 0.173 sec/batch)
2016-10-15 02:27:22.852015: step 17830, loss = 0.84 (754.6 examples/sec; 0.170 sec/batch)
2016-10-15 02:27:24.619667: step 17840, loss = 0.69 (735.9 examples/sec; 0.174 sec/batch)
2016-10-15 02:27:26.377753: step 17850, loss = 1.11 (747.7 examples/sec; 0.171 sec/batch)
2016-10-15 02:27:28.134970: step 17860, loss = 0.76 (712.4 examples/sec; 0.180 sec/batch)
2016-10-15 02:27:29.889852: step 17870, loss = 0.80 (711.7 examples/sec; 0.180 sec/batch)
2016-10-15 02:27:31.652552: step 17880, loss = 0.87 (728.5 examples/sec; 0.176 sec/batch)
2016-10-15 02:27:33.416642: step 17890, loss = 0.74 (715.5 examples/sec; 0.179 sec/batch)
2016-10-15 02:27:35.174596: step 17900, loss = 0.69 (741.5 examples/sec; 0.173 sec/batch)
2016-10-15 02:27:37.130667: step 17910, loss = 0.71 (721.6 examples/sec; 0.177 sec/batch)
2016-10-15 02:27:38.894294: step 17920, loss = 0.84 (732.5 examples/sec; 0.175 sec/batch)
2016-10-15 02:27:40.669351: step 17930, loss = 0.85 (708.1 examples/sec; 0.181 sec/batch)
2016-10-15 02:27:42.433451: step 17940, loss = 0.96 (720.5 examples/sec; 0.178 sec/batch)
2016-10-15 02:27:44.199533: step 17950, loss = 0.88 (704.3 examples/sec; 0.182 sec/batch)
2016-10-15 02:27:45.950918: step 17960, loss = 0.68 (724.1 examples/sec; 0.177 sec/batch)
2016-10-15 02:27:47.709782: step 17970, loss = 0.84 (738.0 examples/sec; 0.173 sec/batch)
2016-10-15 02:27:49.466756: step 17980, loss = 0.68 (724.3 examples/sec; 0.177 sec/batch)
2016-10-15 02:27:51.216290: step 17990, loss = 0.86 (729.8 examples/sec; 0.175 sec/batch)
2016-10-15 02:27:52.983838: step 18000, loss = 0.82 (730.5 examples/sec; 0.175 sec/batch)
2016-10-15 02:27:55.491564: step 18010, loss = 0.83 (721.5 examples/sec; 0.177 sec/batch)
2016-10-15 02:27:57.249300: step 18020, loss = 0.89 (730.6 examples/sec; 0.175 sec/batch)
2016-10-15 02:27:59.016973: step 18030, loss = 0.84 (725.5 examples/sec; 0.176 sec/batch)
2016-10-15 02:28:00.774966: step 18040, loss = 0.89 (715.2 examples/sec; 0.179 sec/batch)
2016-10-15 02:28:02.535390: step 18050, loss = 0.87 (740.5 examples/sec; 0.173 sec/batch)
2016-10-15 02:28:04.308211: step 18060, loss = 0.82 (717.1 examples/sec; 0.179 sec/batch)
2016-10-15 02:28:06.073035: step 18070, loss = 0.89 (690.6 examples/sec; 0.185 sec/batch)
2016-10-15 02:28:07.823014: step 18080, loss = 0.81 (730.0 examples/sec; 0.175 sec/batch)
2016-10-15 02:28:09.585768: step 18090, loss = 0.79 (757.7 examples/sec; 0.169 sec/batch)
2016-10-15 02:28:11.342593: step 18100, loss = 0.79 (735.9 examples/sec; 0.174 sec/batch)
2016-10-15 02:28:13.281329: step 18110, loss = 0.77 (747.3 examples/sec; 0.171 sec/batch)
2016-10-15 02:28:15.037695: step 18120, loss = 0.85 (733.0 examples/sec; 0.175 sec/batch)
2016-10-15 02:28:16.796540: step 18130, loss = 0.85 (730.0 examples/sec; 0.175 sec/batch)
2016-10-15 02:28:18.555833: step 18140, loss = 0.87 (717.6 examples/sec; 0.178 sec/batch)
2016-10-15 02:28:20.315332: step 18150, loss = 0.73 (744.7 examples/sec; 0.172 sec/batch)
2016-10-15 02:28:22.077166: step 18160, loss = 0.80 (718.6 examples/sec; 0.178 sec/batch)
2016-10-15 02:28:23.831861: step 18170, loss = 0.78 (732.1 examples/sec; 0.175 sec/batch)
2016-10-15 02:28:25.594320: step 18180, loss = 0.96 (729.0 examples/sec; 0.176 sec/batch)
2016-10-15 02:28:27.348477: step 18190, loss = 0.85 (729.5 examples/sec; 0.175 sec/batch)
2016-10-15 02:28:29.107883: step 18200, loss = 0.93 (750.8 examples/sec; 0.170 sec/batch)
2016-10-15 02:28:31.056084: step 18210, loss = 0.81 (729.8 examples/sec; 0.175 sec/batch)
2016-10-15 02:28:32.822009: step 18220, loss = 0.72 (720.7 examples/sec; 0.178 sec/batch)
2016-10-15 02:28:34.583963: step 18230, loss = 0.76 (737.6 examples/sec; 0.174 sec/batch)
2016-10-15 02:28:36.343742: step 18240, loss = 0.80 (731.4 examples/sec; 0.175 sec/batch)
2016-10-15 02:28:38.109170: step 18250, loss = 0.73 (725.1 examples/sec; 0.177 sec/batch)
2016-10-15 02:28:39.870870: step 18260, loss = 0.80 (720.2 examples/sec; 0.178 sec/batch)
2016-10-15 02:28:41.637633: step 18270, loss = 0.82 (683.1 examples/sec; 0.187 sec/batch)
2016-10-15 02:28:43.392526: step 18280, loss = 0.77 (697.9 examples/sec; 0.183 sec/batch)
2016-10-15 02:28:45.151273: step 18290, loss = 0.81 (750.1 examples/sec; 0.171 sec/batch)
2016-10-15 02:28:46.909532: step 18300, loss = 0.96 (746.6 examples/sec; 0.171 sec/batch)
2016-10-15 02:28:48.863804: step 18310, loss = 0.86 (720.8 examples/sec; 0.178 sec/batch)
2016-10-15 02:28:50.625884: step 18320, loss = 0.83 (726.1 examples/sec; 0.176 sec/batch)
2016-10-15 02:28:52.390031: step 18330, loss = 0.88 (763.0 examples/sec; 0.168 sec/batch)
2016-10-15 02:28:54.170022: step 18340, loss = 0.91 (715.0 examples/sec; 0.179 sec/batch)
2016-10-15 02:28:55.931546: step 18350, loss = 0.79 (739.2 examples/sec; 0.173 sec/batch)
2016-10-15 02:28:57.700484: step 18360, loss = 0.80 (719.5 examples/sec; 0.178 sec/batch)
2016-10-15 02:28:59.456461: step 18370, loss = 0.92 (728.8 examples/sec; 0.176 sec/batch)
2016-10-15 02:29:01.222107: step 18380, loss = 0.82 (713.2 examples/sec; 0.179 sec/batch)
2016-10-15 02:29:02.983376: step 18390, loss = 0.73 (716.3 examples/sec; 0.179 sec/batch)
2016-10-15 02:29:04.739892: step 18400, loss = 0.86 (734.4 examples/sec; 0.174 sec/batch)
2016-10-15 02:29:06.686012: step 18410, loss = 0.86 (740.7 examples/sec; 0.173 sec/batch)
2016-10-15 02:29:08.452522: step 18420, loss = 0.72 (729.1 examples/sec; 0.176 sec/batch)
2016-10-15 02:29:10.206670: step 18430, loss = 0.83 (733.9 examples/sec; 0.174 sec/batch)
2016-10-15 02:29:11.981407: step 18440, loss = 0.74 (682.0 examples/sec; 0.188 sec/batch)
2016-10-15 02:29:13.737227: step 18450, loss = 0.80 (719.7 examples/sec; 0.178 sec/batch)
2016-10-15 02:29:15.493347: step 18460, loss = 0.82 (726.8 examples/sec; 0.176 sec/batch)
2016-10-15 02:29:17.251042: step 18470, loss = 0.85 (763.7 examples/sec; 0.168 sec/batch)
2016-10-15 02:29:19.010625: step 18480, loss = 0.99 (725.4 examples/sec; 0.176 sec/batch)
2016-10-15 02:29:20.776800: step 18490, loss = 0.69 (705.5 examples/sec; 0.181 sec/batch)
2016-10-15 02:29:22.545043: step 18500, loss = 0.94 (720.9 examples/sec; 0.178 sec/batch)
2016-10-15 02:29:24.494106: step 18510, loss = 0.75 (722.4 examples/sec; 0.177 sec/batch)
2016-10-15 02:29:26.255610: step 18520, loss = 0.76 (743.2 examples/sec; 0.172 sec/batch)
2016-10-15 02:29:28.012485: step 18530, loss = 0.79 (733.9 examples/sec; 0.174 sec/batch)
2016-10-15 02:29:29.772545: step 18540, loss = 0.96 (717.4 examples/sec; 0.178 sec/batch)
2016-10-15 02:29:31.532127: step 18550, loss = 0.87 (766.2 examples/sec; 0.167 sec/batch)
2016-10-15 02:29:33.303061: step 18560, loss = 0.77 (700.1 examples/sec; 0.183 sec/batch)
2016-10-15 02:29:35.069447: step 18570, loss = 1.00 (728.5 examples/sec; 0.176 sec/batch)
2016-10-15 02:29:36.831524: step 18580, loss = 0.69 (728.5 examples/sec; 0.176 sec/batch)
2016-10-15 02:29:38.594343: step 18590, loss = 0.85 (729.7 examples/sec; 0.175 sec/batch)
2016-10-15 02:29:40.357166: step 18600, loss = 0.92 (715.0 examples/sec; 0.179 sec/batch)
2016-10-15 02:29:42.296293: step 18610, loss = 0.83 (726.4 examples/sec; 0.176 sec/batch)
2016-10-15 02:29:44.062492: step 18620, loss = 0.82 (716.8 examples/sec; 0.179 sec/batch)
2016-10-15 02:29:45.825894: step 18630, loss = 0.78 (709.8 examples/sec; 0.180 sec/batch)
2016-10-15 02:29:47.580446: step 18640, loss = 0.84 (723.8 examples/sec; 0.177 sec/batch)
2016-10-15 02:29:49.337339: step 18650, loss = 0.80 (740.2 examples/sec; 0.173 sec/batch)
2016-10-15 02:29:51.091619: step 18660, loss = 0.89 (730.3 examples/sec; 0.175 sec/batch)
2016-10-15 02:29:52.864842: step 18670, loss = 0.97 (718.2 examples/sec; 0.178 sec/batch)
2016-10-15 02:29:54.626349: step 18680, loss = 0.98 (727.2 examples/sec; 0.176 sec/batch)
2016-10-15 02:29:56.378937: step 18690, loss = 0.78 (724.0 examples/sec; 0.177 sec/batch)
2016-10-15 02:29:58.143686: step 18700, loss = 0.73 (719.6 examples/sec; 0.178 sec/batch)
2016-10-15 02:30:00.086676: step 18710, loss = 0.75 (717.6 examples/sec; 0.178 sec/batch)
2016-10-15 02:30:01.875269: step 18720, loss = 0.76 (613.3 examples/sec; 0.209 sec/batch)
2016-10-15 02:30:03.645354: step 18730, loss = 0.76 (743.3 examples/sec; 0.172 sec/batch)
2016-10-15 02:30:05.415350: step 18740, loss = 0.77 (700.6 examples/sec; 0.183 sec/batch)
2016-10-15 02:30:07.168295: step 18750, loss = 0.81 (717.0 examples/sec; 0.179 sec/batch)
2016-10-15 02:30:08.921544: step 18760, loss = 0.87 (762.0 examples/sec; 0.168 sec/batch)
2016-10-15 02:30:10.688169: step 18770, loss = 0.83 (735.2 examples/sec; 0.174 sec/batch)
2016-10-15 02:30:12.451569: step 18780, loss = 0.80 (736.2 examples/sec; 0.174 sec/batch)
2016-10-15 02:30:14.208851: step 18790, loss = 0.85 (749.1 examples/sec; 0.171 sec/batch)
2016-10-15 02:30:15.978444: step 18800, loss = 0.87 (670.1 examples/sec; 0.191 sec/batch)
2016-10-15 02:30:17.914035: step 18810, loss = 1.08 (744.5 examples/sec; 0.172 sec/batch)
2016-10-15 02:30:19.673921: step 18820, loss = 0.79 (742.0 examples/sec; 0.173 sec/batch)
2016-10-15 02:30:21.434399: step 18830, loss = 0.74 (725.8 examples/sec; 0.176 sec/batch)
2016-10-15 02:30:23.197522: step 18840, loss = 1.09 (704.5 examples/sec; 0.182 sec/batch)
2016-10-15 02:30:24.964050: step 18850, loss = 0.77 (737.4 examples/sec; 0.174 sec/batch)
2016-10-15 02:30:26.730174: step 18860, loss = 0.72 (731.5 examples/sec; 0.175 sec/batch)
2016-10-15 02:30:28.499856: step 18870, loss = 0.76 (708.2 examples/sec; 0.181 sec/batch)
2016-10-15 02:30:30.261340: step 18880, loss = 1.00 (750.8 examples/sec; 0.170 sec/batch)
2016-10-15 02:30:32.030606: step 18890, loss = 0.75 (730.3 examples/sec; 0.175 sec/batch)
2016-10-15 02:30:33.804702: step 18900, loss = 0.92 (703.5 examples/sec; 0.182 sec/batch)
2016-10-15 02:30:35.755199: step 18910, loss = 0.70 (737.6 examples/sec; 0.174 sec/batch)
2016-10-15 02:30:37.521286: step 18920, loss = 0.76 (734.3 examples/sec; 0.174 sec/batch)
2016-10-15 02:30:39.293400: step 18930, loss = 0.85 (713.4 examples/sec; 0.179 sec/batch)
2016-10-15 02:30:41.037996: step 18940, loss = 1.16 (727.2 examples/sec; 0.176 sec/batch)
2016-10-15 02:30:42.801318: step 18950, loss = 0.76 (727.2 examples/sec; 0.176 sec/batch)
2016-10-15 02:30:44.561994: step 18960, loss = 0.77 (733.7 examples/sec; 0.174 sec/batch)
2016-10-15 02:30:46.334044: step 18970, loss = 0.75 (713.7 examples/sec; 0.179 sec/batch)
2016-10-15 02:30:48.097266: step 18980, loss = 0.84 (694.0 examples/sec; 0.184 sec/batch)
2016-10-15 02:30:49.859077: step 18990, loss = 0.79 (725.4 examples/sec; 0.176 sec/batch)
2016-10-15 02:30:51.621498: step 19000, loss = 0.72 (715.5 examples/sec; 0.179 sec/batch)
2016-10-15 02:30:54.161998: step 19010, loss = 0.83 (701.3 examples/sec; 0.183 sec/batch)
2016-10-15 02:30:55.931981: step 19020, loss = 0.92 (727.2 examples/sec; 0.176 sec/batch)
2016-10-15 02:30:57.704798: step 19030, loss = 0.73 (717.0 examples/sec; 0.179 sec/batch)
2016-10-15 02:30:59.455403: step 19040, loss = 0.97 (735.6 examples/sec; 0.174 sec/batch)
2016-10-15 02:31:01.216881: step 19050, loss = 0.84 (726.1 examples/sec; 0.176 sec/batch)
2016-10-15 02:31:02.977744: step 19060, loss = 0.70 (696.1 examples/sec; 0.184 sec/batch)
2016-10-15 02:31:04.736582: step 19070, loss = 0.77 (705.6 examples/sec; 0.181 sec/batch)
2016-10-15 02:31:06.499606: step 19080, loss = 0.90 (731.5 examples/sec; 0.175 sec/batch)
2016-10-15 02:31:08.257124: step 19090, loss = 0.93 (727.8 examples/sec; 0.176 sec/batch)
2016-10-15 02:31:10.023039: step 19100, loss = 0.79 (718.6 examples/sec; 0.178 sec/batch)
2016-10-15 02:31:11.971125: step 19110, loss = 0.78 (734.5 examples/sec; 0.174 sec/batch)
2016-10-15 02:31:13.739305: step 19120, loss = 0.71 (729.8 examples/sec; 0.175 sec/batch)
2016-10-15 02:31:15.506062: step 19130, loss = 0.73 (719.5 examples/sec; 0.178 sec/batch)
2016-10-15 02:31:17.264374: step 19140, loss = 0.71 (727.4 examples/sec; 0.176 sec/batch)
2016-10-15 02:31:19.026630: step 19150, loss = 0.70 (724.2 examples/sec; 0.177 sec/batch)
2016-10-15 02:31:20.797170: step 19160, loss = 0.85 (712.3 examples/sec; 0.180 sec/batch)
2016-10-15 02:31:22.558796: step 19170, loss = 0.83 (732.2 examples/sec; 0.175 sec/batch)
2016-10-15 02:31:24.330432: step 19180, loss = 0.68 (727.0 examples/sec; 0.176 sec/batch)
2016-10-15 02:31:26.097178: step 19190, loss = 0.81 (727.6 examples/sec; 0.176 sec/batch)
2016-10-15 02:31:27.853784: step 19200, loss = 0.89 (728.8 examples/sec; 0.176 sec/batch)
2016-10-15 02:31:29.810466: step 19210, loss = 0.78 (720.0 examples/sec; 0.178 sec/batch)
2016-10-15 02:31:31.567870: step 19220, loss = 0.72 (726.0 examples/sec; 0.176 sec/batch)
2016-10-15 02:31:33.331066: step 19230, loss = 0.87 (725.0 examples/sec; 0.177 sec/batch)
2016-10-15 02:31:35.091348: step 19240, loss = 0.87 (736.4 examples/sec; 0.174 sec/batch)
2016-10-15 02:31:36.847545: step 19250, loss = 0.95 (737.6 examples/sec; 0.174 sec/batch)
2016-10-15 02:31:38.611539: step 19260, loss = 0.90 (743.7 examples/sec; 0.172 sec/batch)
2016-10-15 02:31:40.374825: step 19270, loss = 0.72 (747.8 examples/sec; 0.171 sec/batch)
2016-10-15 02:31:42.134520: step 19280, loss = 0.89 (731.4 examples/sec; 0.175 sec/batch)
2016-10-15 02:31:43.896969: step 19290, loss = 0.74 (741.0 examples/sec; 0.173 sec/batch)
2016-10-15 02:31:45.655797: step 19300, loss = 0.84 (732.5 examples/sec; 0.175 sec/batch)
2016-10-15 02:31:47.597169: step 19310, loss = 0.73 (736.0 examples/sec; 0.174 sec/batch)
2016-10-15 02:31:49.364207: step 19320, loss = 1.05 (747.0 examples/sec; 0.171 sec/batch)
2016-10-15 02:31:51.125384: step 19330, loss = 0.75 (747.7 examples/sec; 0.171 sec/batch)
2016-10-15 02:31:52.888327: step 19340, loss = 0.80 (717.2 examples/sec; 0.178 sec/batch)
2016-10-15 02:31:54.656237: step 19350, loss = 0.83 (707.7 examples/sec; 0.181 sec/batch)
2016-10-15 02:31:56.414767: step 19360, loss = 0.78 (739.2 examples/sec; 0.173 sec/batch)
2016-10-15 02:31:58.184693: step 19370, loss = 0.85 (724.5 examples/sec; 0.177 sec/batch)
2016-10-15 02:31:59.944520: step 19380, loss = 0.87 (729.2 examples/sec; 0.176 sec/batch)
2016-10-15 02:32:01.704804: step 19390, loss = 0.88 (725.5 examples/sec; 0.176 sec/batch)
2016-10-15 02:32:03.460160: step 19400, loss = 0.85 (729.3 examples/sec; 0.176 sec/batch)
2016-10-15 02:32:05.405761: step 19410, loss = 0.73 (737.7 examples/sec; 0.174 sec/batch)
2016-10-15 02:32:07.166292: step 19420, loss = 0.77 (725.6 examples/sec; 0.176 sec/batch)
2016-10-15 02:32:08.920368: step 19430, loss = 0.83 (730.5 examples/sec; 0.175 sec/batch)
2016-10-15 02:32:10.682019: step 19440, loss = 0.88 (723.9 examples/sec; 0.177 sec/batch)
2016-10-15 02:32:12.451438: step 19450, loss = 0.78 (721.5 examples/sec; 0.177 sec/batch)
2016-10-15 02:32:14.216586: step 19460, loss = 1.02 (713.6 examples/sec; 0.179 sec/batch)
2016-10-15 02:32:15.973278: step 19470, loss = 0.86 (749.5 examples/sec; 0.171 sec/batch)
2016-10-15 02:32:17.732242: step 19480, loss = 0.62 (721.4 examples/sec; 0.177 sec/batch)
2016-10-15 02:32:19.492174: step 19490, loss = 0.81 (705.6 examples/sec; 0.181 sec/batch)
2016-10-15 02:32:21.245305: step 19500, loss = 0.77 (713.8 examples/sec; 0.179 sec/batch)
2016-10-15 02:32:23.185422: step 19510, loss = 0.81 (727.6 examples/sec; 0.176 sec/batch)
2016-10-15 02:32:24.949897: step 19520, loss = 0.83 (740.3 examples/sec; 0.173 sec/batch)
2016-10-15 02:32:26.708391: step 19530, loss = 0.97 (758.0 examples/sec; 0.169 sec/batch)
2016-10-15 02:32:28.474973: step 19540, loss = 0.85 (700.6 examples/sec; 0.183 sec/batch)
2016-10-15 02:32:30.228689: step 19550, loss = 0.77 (737.5 examples/sec; 0.174 sec/batch)
2016-10-15 02:32:31.992711: step 19560, loss = 0.81 (732.7 examples/sec; 0.175 sec/batch)
2016-10-15 02:32:33.754388: step 19570, loss = 0.96 (740.8 examples/sec; 0.173 sec/batch)
2016-10-15 02:32:35.510951: step 19580, loss = 0.73 (724.5 examples/sec; 0.177 sec/batch)
2016-10-15 02:32:37.266273: step 19590, loss = 0.80 (731.1 examples/sec; 0.175 sec/batch)
2016-10-15 02:32:39.017228: step 19600, loss = 1.01 (738.4 examples/sec; 0.173 sec/batch)
2016-10-15 02:32:40.969372: step 19610, loss = 0.78 (724.0 examples/sec; 0.177 sec/batch)
2016-10-15 02:32:42.730220: step 19620, loss = 0.82 (716.5 examples/sec; 0.179 sec/batch)
2016-10-15 02:32:44.485222: step 19630, loss = 0.88 (725.8 examples/sec; 0.176 sec/batch)
2016-10-15 02:32:46.248331: step 19640, loss = 0.88 (725.4 examples/sec; 0.176 sec/batch)
2016-10-15 02:32:48.002865: step 19650, loss = 0.82 (727.2 examples/sec; 0.176 sec/batch)
2016-10-15 02:32:49.763866: step 19660, loss = 0.85 (720.5 examples/sec; 0.178 sec/batch)
2016-10-15 02:32:51.524079: step 19670, loss = 0.86 (753.6 examples/sec; 0.170 sec/batch)
2016-10-15 02:32:53.281579: step 19680, loss = 0.71 (735.3 examples/sec; 0.174 sec/batch)
2016-10-15 02:32:55.050308: step 19690, loss = 0.74 (716.4 examples/sec; 0.179 sec/batch)
2016-10-15 02:32:56.817403: step 19700, loss = 0.66 (721.0 examples/sec; 0.178 sec/batch)
2016-10-15 02:32:58.752464: step 19710, loss = 0.79 (708.8 examples/sec; 0.181 sec/batch)
2016-10-15 02:33:00.507047: step 19720, loss = 0.71 (725.2 examples/sec; 0.176 sec/batch)
2016-10-15 02:33:02.271763: step 19730, loss = 0.71 (745.9 examples/sec; 0.172 sec/batch)
2016-10-15 02:33:04.036954: step 19740, loss = 0.87 (714.2 examples/sec; 0.179 sec/batch)
2016-10-15 02:33:05.785308: step 19750, loss = 0.90 (735.8 examples/sec; 0.174 sec/batch)
2016-10-15 02:33:07.550496: step 19760, loss = 0.95 (708.5 examples/sec; 0.181 sec/batch)
2016-10-15 02:33:09.312636: step 19770, loss = 0.81 (730.2 examples/sec; 0.175 sec/batch)
2016-10-15 02:33:11.058531: step 19780, loss = 0.83 (759.4 examples/sec; 0.169 sec/batch)
2016-10-15 02:33:12.822292: step 19790, loss = 0.78 (738.4 examples/sec; 0.173 sec/batch)
2016-10-15 02:33:14.590563: step 19800, loss = 0.96 (708.6 examples/sec; 0.181 sec/batch)
2016-10-15 02:33:16.528691: step 19810, loss = 0.64 (741.9 examples/sec; 0.173 sec/batch)
2016-10-15 02:33:18.286063: step 19820, loss = 0.97 (730.0 examples/sec; 0.175 sec/batch)
2016-10-15 02:33:20.048375: step 19830, loss = 0.85 (717.6 examples/sec; 0.178 sec/batch)
2016-10-15 02:33:21.802446: step 19840, loss = 0.76 (717.9 examples/sec; 0.178 sec/batch)
2016-10-15 02:33:23.559858: step 19850, loss = 0.72 (740.7 examples/sec; 0.173 sec/batch)
2016-10-15 02:33:25.323679: step 19860, loss = 0.70 (727.8 examples/sec; 0.176 sec/batch)
2016-10-15 02:33:27.076673: step 19870, loss = 0.93 (728.9 examples/sec; 0.176 sec/batch)
2016-10-15 02:33:28.839235: step 19880, loss = 0.75 (742.7 examples/sec; 0.172 sec/batch)
2016-10-15 02:33:30.595211: step 19890, loss = 0.88 (711.7 examples/sec; 0.180 sec/batch)
2016-10-15 02:33:32.351711: step 19900, loss = 0.75 (726.1 examples/sec; 0.176 sec/batch)
2016-10-15 02:33:34.292824: step 19910, loss = 0.93 (709.5 examples/sec; 0.180 sec/batch)
2016-10-15 02:33:36.045208: step 19920, loss = 0.74 (716.3 examples/sec; 0.179 sec/batch)
2016-10-15 02:33:37.808643: step 19930, loss = 0.81 (710.6 examples/sec; 0.180 sec/batch)
2016-10-15 02:33:39.561017: step 19940, loss = 1.04 (737.1 examples/sec; 0.174 sec/batch)
2016-10-15 02:33:41.325955: step 19950, loss = 0.81 (705.4 examples/sec; 0.181 sec/batch)
2016-10-15 02:33:43.082233: step 19960, loss = 0.76 (716.6 examples/sec; 0.179 sec/batch)
2016-10-15 02:33:44.844781: step 19970, loss = 0.74 (735.1 examples/sec; 0.174 sec/batch)
2016-10-15 02:33:46.604848: step 19980, loss = 0.80 (736.0 examples/sec; 0.174 sec/batch)
2016-10-15 02:33:48.355455: step 19990, loss = 0.76 (730.4 examples/sec; 0.175 sec/batch)
2016-10-15 02:33:50.110277: step 20000, loss = 0.78 (733.6 examples/sec; 0.174 sec/batch)
2016-10-15 02:33:52.632533: step 20010, loss = 0.88 (724.3 examples/sec; 0.177 sec/batch)
2016-10-15 02:33:54.393388: step 20020, loss = 0.79 (704.8 examples/sec; 0.182 sec/batch)
2016-10-15 02:33:56.143466: step 20030, loss = 0.77 (736.5 examples/sec; 0.174 sec/batch)
2016-10-15 02:33:57.914710: step 20040, loss = 0.77 (718.8 examples/sec; 0.178 sec/batch)
2016-10-15 02:33:59.681721: step 20050, loss = 0.85 (717.9 examples/sec; 0.178 sec/batch)
2016-10-15 02:34:01.453511: step 20060, loss = 0.86 (690.9 examples/sec; 0.185 sec/batch)
2016-10-15 02:34:03.212175: step 20070, loss = 0.86 (726.1 examples/sec; 0.176 sec/batch)
2016-10-15 02:34:04.982293: step 20080, loss = 1.01 (709.8 examples/sec; 0.180 sec/batch)
2016-10-15 02:34:06.737751: step 20090, loss = 0.72 (737.2 examples/sec; 0.174 sec/batch)
2016-10-15 02:34:08.505043: step 20100, loss = 0.88 (733.1 examples/sec; 0.175 sec/batch)
2016-10-15 02:34:10.451700: step 20110, loss = 0.87 (710.8 examples/sec; 0.180 sec/batch)
2016-10-15 02:34:12.202915: step 20120, loss = 0.78 (769.5 examples/sec; 0.166 sec/batch)
2016-10-15 02:34:13.971123: step 20130, loss = 0.88 (720.5 examples/sec; 0.178 sec/batch)
2016-10-15 02:34:15.729917: step 20140, loss = 0.83 (724.5 examples/sec; 0.177 sec/batch)
2016-10-15 02:34:17.479484: step 20150, loss = 0.74 (744.4 examples/sec; 0.172 sec/batch)
2016-10-15 02:34:19.246449: step 20160, loss = 0.73 (728.4 examples/sec; 0.176 sec/batch)
2016-10-15 02:34:21.012254: step 20170, loss = 0.87 (736.0 examples/sec; 0.174 sec/batch)
2016-10-15 02:34:22.771272: step 20180, loss = 0.73 (738.4 examples/sec; 0.173 sec/batch)
2016-10-15 02:34:24.540894: step 20190, loss = 0.77 (692.6 examples/sec; 0.185 sec/batch)
2016-10-15 02:34:26.296546: step 20200, loss = 0.83 (718.6 examples/sec; 0.178 sec/batch)
2016-10-15 02:34:28.241049: step 20210, loss = 0.75 (716.4 examples/sec; 0.179 sec/batch)
2016-10-15 02:34:30.007665: step 20220, loss = 0.75 (697.3 examples/sec; 0.184 sec/batch)
2016-10-15 02:34:31.764015: step 20230, loss = 0.78 (749.3 examples/sec; 0.171 sec/batch)
2016-10-15 02:34:33.524749: step 20240, loss = 0.86 (723.7 examples/sec; 0.177 sec/batch)
2016-10-15 02:34:35.292659: step 20250, loss = 0.91 (719.0 examples/sec; 0.178 sec/batch)
2016-10-15 02:34:37.056316: step 20260, loss = 0.85 (723.0 examples/sec; 0.177 sec/batch)
2016-10-15 02:34:38.815191: step 20270, loss = 0.75 (726.4 examples/sec; 0.176 sec/batch)
2016-10-15 02:34:40.569073: step 20280, loss = 0.79 (716.0 examples/sec; 0.179 sec/batch)
2016-10-15 02:34:42.335603: step 20290, loss = 0.80 (710.8 examples/sec; 0.180 sec/batch)
2016-10-15 02:34:44.082225: step 20300, loss = 0.82 (723.9 examples/sec; 0.177 sec/batch)
2016-10-15 02:34:46.032530: step 20310, loss = 0.76 (710.0 examples/sec; 0.180 sec/batch)
2016-10-15 02:34:47.796741: step 20320, loss = 0.68 (710.6 examples/sec; 0.180 sec/batch)
2016-10-15 02:34:49.555516: step 20330, loss = 0.78 (760.7 examples/sec; 0.168 sec/batch)
2016-10-15 02:34:51.320155: step 20340, loss = 0.92 (727.1 examples/sec; 0.176 sec/batch)
2016-10-15 02:34:53.078063: step 20350, loss = 0.89 (764.2 examples/sec; 0.168 sec/batch)
2016-10-15 02:34:54.837185: step 20360, loss = 0.85 (745.1 examples/sec; 0.172 sec/batch)
2016-10-15 02:34:56.593336: step 20370, loss = 0.92 (722.2 examples/sec; 0.177 sec/batch)
2016-10-15 02:34:58.358164: step 20380, loss = 0.82 (706.1 examples/sec; 0.181 sec/batch)
2016-10-15 02:35:00.102286: step 20390, loss = 0.81 (770.4 examples/sec; 0.166 sec/batch)
2016-10-15 02:35:01.858781: step 20400, loss = 0.88 (753.5 examples/sec; 0.170 sec/batch)
2016-10-15 02:35:03.825022: step 20410, loss = 0.78 (709.9 examples/sec; 0.180 sec/batch)
2016-10-15 02:35:05.581426: step 20420, loss = 0.89 (727.0 examples/sec; 0.176 sec/batch)
2016-10-15 02:35:07.341970: step 20430, loss = 0.74 (717.5 examples/sec; 0.178 sec/batch)
2016-10-15 02:35:09.099867: step 20440, loss = 0.85 (715.1 examples/sec; 0.179 sec/batch)
2016-10-15 02:35:10.853610: step 20450, loss = 0.80 (753.3 examples/sec; 0.170 sec/batch)
2016-10-15 02:35:12.611704: step 20460, loss = 0.86 (734.5 examples/sec; 0.174 sec/batch)
2016-10-15 02:35:14.374193: step 20470, loss = 0.89 (723.6 examples/sec; 0.177 sec/batch)
2016-10-15 02:35:16.133321: step 20480, loss = 0.91 (730.0 examples/sec; 0.175 sec/batch)
2016-10-15 02:35:17.887170: step 20490, loss = 0.83 (736.0 examples/sec; 0.174 sec/batch)
2016-10-15 02:35:19.640288: step 20500, loss = 0.90 (741.9 examples/sec; 0.173 sec/batch)
2016-10-15 02:35:21.597858: step 20510, loss = 0.83 (729.5 examples/sec; 0.175 sec/batch)
2016-10-15 02:35:23.358986: step 20520, loss = 0.86 (708.0 examples/sec; 0.181 sec/batch)
2016-10-15 02:35:25.132600: step 20530, loss = 0.72 (719.1 examples/sec; 0.178 sec/batch)
2016-10-15 02:35:26.879780: step 20540, loss = 0.82 (715.9 examples/sec; 0.179 sec/batch)
2016-10-15 02:35:28.643393: step 20550, loss = 0.64 (747.8 examples/sec; 0.171 sec/batch)
2016-10-15 02:35:30.410175: step 20560, loss = 0.76 (728.2 examples/sec; 0.176 sec/batch)
2016-10-15 02:35:32.179149: step 20570, loss = 0.73 (691.7 examples/sec; 0.185 sec/batch)
2016-10-15 02:35:33.932727: step 20580, loss = 0.92 (712.3 examples/sec; 0.180 sec/batch)
2016-10-15 02:35:35.696884: step 20590, loss = 0.83 (755.6 examples/sec; 0.169 sec/batch)
2016-10-15 02:35:37.467750: step 20600, loss = 0.71 (691.8 examples/sec; 0.185 sec/batch)
2016-10-15 02:35:39.412504: step 20610, loss = 0.64 (724.4 examples/sec; 0.177 sec/batch)
2016-10-15 02:35:41.172436: step 20620, loss = 0.80 (709.8 examples/sec; 0.180 sec/batch)
2016-10-15 02:35:42.930444: step 20630, loss = 0.83 (723.0 examples/sec; 0.177 sec/batch)
2016-10-15 02:35:44.694559: step 20640, loss = 0.92 (711.5 examples/sec; 0.180 sec/batch)
2016-10-15 02:35:46.457078: step 20650, loss = 0.71 (766.2 examples/sec; 0.167 sec/batch)
2016-10-15 02:35:48.217853: step 20660, loss = 0.81 (712.1 examples/sec; 0.180 sec/batch)
2016-10-15 02:35:49.968293: step 20670, loss = 0.71 (733.6 examples/sec; 0.174 sec/batch)
2016-10-15 02:35:51.746954: step 20680, loss = 0.74 (665.8 examples/sec; 0.192 sec/batch)
2016-10-15 02:35:53.495360: step 20690, loss = 0.88 (729.5 examples/sec; 0.175 sec/batch)
2016-10-15 02:35:55.249292: step 20700, loss = 0.72 (726.5 examples/sec; 0.176 sec/batch)
2016-10-15 02:35:57.210493: step 20710, loss = 0.78 (715.3 examples/sec; 0.179 sec/batch)
2016-10-15 02:35:58.971550: step 20720, loss = 0.78 (711.4 examples/sec; 0.180 sec/batch)
2016-10-15 02:36:00.725505: step 20730, loss = 0.79 (719.2 examples/sec; 0.178 sec/batch)
2016-10-15 02:36:02.491526: step 20740, loss = 0.89 (727.8 examples/sec; 0.176 sec/batch)
2016-10-15 02:36:04.248108: step 20750, loss = 0.72 (723.6 examples/sec; 0.177 sec/batch)
2016-10-15 02:36:05.998173: step 20760, loss = 0.75 (744.7 examples/sec; 0.172 sec/batch)
2016-10-15 02:36:07.761306: step 20770, loss = 0.79 (726.9 examples/sec; 0.176 sec/batch)
2016-10-15 02:36:09.521952: step 20780, loss = 0.88 (751.6 examples/sec; 0.170 sec/batch)
2016-10-15 02:36:11.284323: step 20790, loss = 0.81 (739.3 examples/sec; 0.173 sec/batch)
2016-10-15 02:36:13.046088: step 20800, loss = 0.78 (723.4 examples/sec; 0.177 sec/batch)
2016-10-15 02:36:14.989364: step 20810, loss = 0.77 (758.4 examples/sec; 0.169 sec/batch)
2016-10-15 02:36:16.754572: step 20820, loss = 0.73 (748.5 examples/sec; 0.171 sec/batch)
2016-10-15 02:36:18.507610: step 20830, loss = 0.80 (737.0 examples/sec; 0.174 sec/batch)
2016-10-15 02:36:20.262795: step 20840, loss = 0.78 (733.4 examples/sec; 0.175 sec/batch)
2016-10-15 02:36:22.022693: step 20850, loss = 1.06 (752.5 examples/sec; 0.170 sec/batch)
2016-10-15 02:36:23.783092: step 20860, loss = 0.79 (722.6 examples/sec; 0.177 sec/batch)
2016-10-15 02:36:25.545043: step 20870, loss = 0.68 (690.5 examples/sec; 0.185 sec/batch)
2016-10-15 02:36:27.287777: step 20880, loss = 0.67 (755.8 examples/sec; 0.169 sec/batch)
2016-10-15 02:36:29.060698: step 20890, loss = 0.91 (683.3 examples/sec; 0.187 sec/batch)
2016-10-15 02:36:30.817067: step 20900, loss = 0.92 (732.5 examples/sec; 0.175 sec/batch)
2016-10-15 02:36:32.771180: step 20910, loss = 0.88 (713.6 examples/sec; 0.179 sec/batch)
2016-10-15 02:36:34.520204: step 20920, loss = 0.95 (737.6 examples/sec; 0.174 sec/batch)
2016-10-15 02:36:36.278691: step 20930, loss = 0.80 (794.7 examples/sec; 0.161 sec/batch)
2016-10-15 02:36:38.044422: step 20940, loss = 0.91 (715.0 examples/sec; 0.179 sec/batch)
2016-10-15 02:36:39.801014: step 20950, loss = 0.82 (717.5 examples/sec; 0.178 sec/batch)
2016-10-15 02:36:41.556900: step 20960, loss = 0.75 (726.8 examples/sec; 0.176 sec/batch)
2016-10-15 02:36:43.309970: step 20970, loss = 0.88 (718.5 examples/sec; 0.178 sec/batch)
2016-10-15 02:36:45.068867: step 20980, loss = 0.94 (733.2 examples/sec; 0.175 sec/batch)
2016-10-15 02:36:46.817705: step 20990, loss = 0.88 (732.2 examples/sec; 0.175 sec/batch)
2016-10-15 02:36:48.574258: step 21000, loss = 0.90 (737.0 examples/sec; 0.174 sec/batch)
2016-10-15 02:36:51.140586: step 21010, loss = 0.72 (745.3 examples/sec; 0.172 sec/batch)
2016-10-15 02:36:52.914270: step 21020, loss = 0.90 (720.5 examples/sec; 0.178 sec/batch)
2016-10-15 02:36:54.675253: step 21030, loss = 0.72 (729.8 examples/sec; 0.175 sec/batch)
2016-10-15 02:36:56.434828: step 21040, loss = 0.77 (724.9 examples/sec; 0.177 sec/batch)
2016-10-15 02:36:58.192392: step 21050, loss = 0.91 (717.9 examples/sec; 0.178 sec/batch)
2016-10-15 02:36:59.945930: step 21060, loss = 0.82 (726.2 examples/sec; 0.176 sec/batch)
2016-10-15 02:37:01.700124: step 21070, loss = 0.69 (738.1 examples/sec; 0.173 sec/batch)
2016-10-15 02:37:03.464277: step 21080, loss = 0.83 (719.5 examples/sec; 0.178 sec/batch)
2016-10-15 02:37:05.225683: step 21090, loss = 0.81 (740.6 examples/sec; 0.173 sec/batch)
2016-10-15 02:37:06.973633: step 21100, loss = 0.72 (708.0 examples/sec; 0.181 sec/batch)
2016-10-15 02:37:08.912189: step 21110, loss = 0.73 (739.0 examples/sec; 0.173 sec/batch)
2016-10-15 02:37:10.673681: step 21120, loss = 0.96 (748.2 examples/sec; 0.171 sec/batch)
2016-10-15 02:37:12.427043: step 21130, loss = 0.95 (736.1 examples/sec; 0.174 sec/batch)
2016-10-15 02:37:14.178808: step 21140, loss = 0.73 (730.3 examples/sec; 0.175 sec/batch)
2016-10-15 02:37:15.941573: step 21150, loss = 0.76 (720.5 examples/sec; 0.178 sec/batch)
2016-10-15 02:37:17.697021: step 21160, loss = 0.79 (754.3 examples/sec; 0.170 sec/batch)
2016-10-15 02:37:19.455454: step 21170, loss = 0.82 (725.6 examples/sec; 0.176 sec/batch)
2016-10-15 02:37:21.218906: step 21180, loss = 0.74 (734.5 examples/sec; 0.174 sec/batch)
2016-10-15 02:37:22.972363: step 21190, loss = 0.83 (746.6 examples/sec; 0.171 sec/batch)
2016-10-15 02:37:24.739916: step 21200, loss = 0.78 (708.2 examples/sec; 0.181 sec/batch)
2016-10-15 02:37:26.687111: step 21210, loss = 0.84 (743.9 examples/sec; 0.172 sec/batch)
2016-10-15 02:37:28.438838: step 21220, loss = 0.78 (718.9 examples/sec; 0.178 sec/batch)
2016-10-15 02:37:30.191576: step 21230, loss = 0.82 (735.1 examples/sec; 0.174 sec/batch)
2016-10-15 02:37:31.952920: step 21240, loss = 0.78 (725.0 examples/sec; 0.177 sec/batch)
2016-10-15 02:37:33.706332: step 21250, loss = 0.78 (738.3 examples/sec; 0.173 sec/batch)
2016-10-15 02:37:35.470470: step 21260, loss = 0.88 (701.0 examples/sec; 0.183 sec/batch)
2016-10-15 02:37:37.225614: step 21270, loss = 0.88 (728.6 examples/sec; 0.176 sec/batch)
2016-10-15 02:37:38.987289: step 21280, loss = 0.68 (721.5 examples/sec; 0.177 sec/batch)
2016-10-15 02:37:40.748317: step 21290, loss = 0.87 (722.0 examples/sec; 0.177 sec/batch)
2016-10-15 02:37:42.506855: step 21300, loss = 0.80 (726.8 examples/sec; 0.176 sec/batch)
2016-10-15 02:37:44.442643: step 21310, loss = 0.77 (726.1 examples/sec; 0.176 sec/batch)
2016-10-15 02:37:46.199024: step 21320, loss = 0.88 (741.5 examples/sec; 0.173 sec/batch)
2016-10-15 02:37:47.962277: step 21330, loss = 1.21 (710.5 examples/sec; 0.180 sec/batch)
2016-10-15 02:37:49.723378: step 21340, loss = 0.75 (755.9 examples/sec; 0.169 sec/batch)
2016-10-15 02:37:51.475245: step 21350, loss = 0.70 (730.0 examples/sec; 0.175 sec/batch)
2016-10-15 02:37:53.239623: step 21360, loss = 0.79 (743.2 examples/sec; 0.172 sec/batch)
2016-10-15 02:37:54.996502: step 21370, loss = 0.85 (746.0 examples/sec; 0.172 sec/batch)
2016-10-15 02:37:56.756866: step 21380, loss = 0.93 (733.3 examples/sec; 0.175 sec/batch)
2016-10-15 02:37:58.519261: step 21390, loss = 0.91 (714.4 examples/sec; 0.179 sec/batch)
2016-10-15 02:38:00.280766: step 21400, loss = 0.73 (725.9 examples/sec; 0.176 sec/batch)
2016-10-15 02:38:02.232086: step 21410, loss = 1.02 (699.7 examples/sec; 0.183 sec/batch)
2016-10-15 02:38:03.991118: step 21420, loss = 0.84 (740.9 examples/sec; 0.173 sec/batch)
2016-10-15 02:38:05.758079: step 21430, loss = 0.75 (697.0 examples/sec; 0.184 sec/batch)
2016-10-15 02:38:07.506700: step 21440, loss = 0.76 (726.7 examples/sec; 0.176 sec/batch)
2016-10-15 02:38:09.263238: step 21450, loss = 0.75 (758.3 examples/sec; 0.169 sec/batch)
2016-10-15 02:38:11.027773: step 21460, loss = 0.85 (718.3 examples/sec; 0.178 sec/batch)
2016-10-15 02:38:12.794384: step 21470, loss = 0.82 (731.0 examples/sec; 0.175 sec/batch)
2016-10-15 02:38:14.554607: step 21480, loss = 0.86 (722.3 examples/sec; 0.177 sec/batch)
2016-10-15 02:38:16.320325: step 21490, loss = 0.79 (710.7 examples/sec; 0.180 sec/batch)
2016-10-15 02:38:18.070782: step 21500, loss = 0.70 (753.3 examples/sec; 0.170 sec/batch)
2016-10-15 02:38:20.010401: step 21510, loss = 0.77 (734.9 examples/sec; 0.174 sec/batch)
2016-10-15 02:38:21.767839: step 21520, loss = 0.73 (742.6 examples/sec; 0.172 sec/batch)
2016-10-15 02:38:23.537731: step 21530, loss = 0.81 (709.5 examples/sec; 0.180 sec/batch)
2016-10-15 02:38:25.301999: step 21540, loss = 0.83 (744.7 examples/sec; 0.172 sec/batch)
2016-10-15 02:38:27.063841: step 21550, loss = 0.88 (711.5 examples/sec; 0.180 sec/batch)
2016-10-15 02:38:28.823291: step 21560, loss = 1.06 (716.4 examples/sec; 0.179 sec/batch)
2016-10-15 02:38:30.578529: step 21570, loss = 0.79 (760.8 examples/sec; 0.168 sec/batch)
2016-10-15 02:38:32.343290: step 21580, loss = 0.75 (730.8 examples/sec; 0.175 sec/batch)
2016-10-15 02:38:34.097779: step 21590, loss = 1.01 (713.4 examples/sec; 0.179 sec/batch)
2016-10-15 02:38:35.854832: step 21600, loss = 0.83 (743.9 examples/sec; 0.172 sec/batch)
2016-10-15 02:38:37.803196: step 21610, loss = 0.99 (735.6 examples/sec; 0.174 sec/batch)
2016-10-15 02:38:39.562472: step 21620, loss = 1.03 (728.7 examples/sec; 0.176 sec/batch)
2016-10-15 02:38:41.331565: step 21630, loss = 0.78 (718.0 examples/sec; 0.178 sec/batch)
2016-10-15 02:38:43.090723: step 21640, loss = 0.82 (716.2 examples/sec; 0.179 sec/batch)
2016-10-15 02:38:44.857423: step 21650, loss = 0.84 (727.2 examples/sec; 0.176 sec/batch)
2016-10-15 02:38:46.616096: step 21660, loss = 0.85 (737.0 examples/sec; 0.174 sec/batch)
2016-10-15 02:38:48.382400: step 21670, loss = 0.85 (720.3 examples/sec; 0.178 sec/batch)
2016-10-15 02:38:50.150236: step 21680, loss = 0.76 (728.6 examples/sec; 0.176 sec/batch)
2016-10-15 02:38:51.917352: step 21690, loss = 0.75 (726.1 examples/sec; 0.176 sec/batch)
2016-10-15 02:38:53.677000: step 21700, loss = 1.03 (722.4 examples/sec; 0.177 sec/batch)
2016-10-15 02:38:55.617280: step 21710, loss = 0.80 (737.1 examples/sec; 0.174 sec/batch)
2016-10-15 02:38:57.384822: step 21720, loss = 0.70 (721.7 examples/sec; 0.177 sec/batch)
2016-10-15 02:38:59.146098: step 21730, loss = 0.99 (732.4 examples/sec; 0.175 sec/batch)
2016-10-15 02:39:00.917534: step 21740, loss = 0.90 (711.8 examples/sec; 0.180 sec/batch)
2016-10-15 02:39:02.679186: step 21750, loss = 0.82 (715.7 examples/sec; 0.179 sec/batch)
2016-10-15 02:39:04.434181: step 21760, loss = 0.90 (739.2 examples/sec; 0.173 sec/batch)
2016-10-15 02:39:06.201052: step 21770, loss = 0.93 (728.8 examples/sec; 0.176 sec/batch)
2016-10-15 02:39:07.960172: step 21780, loss = 0.85 (721.3 examples/sec; 0.177 sec/batch)
2016-10-15 02:39:09.727186: step 21790, loss = 0.87 (708.3 examples/sec; 0.181 sec/batch)
2016-10-15 02:39:11.487782: step 21800, loss = 0.95 (741.0 examples/sec; 0.173 sec/batch)
2016-10-15 02:39:13.430617: step 21810, loss = 0.93 (730.7 examples/sec; 0.175 sec/batch)
2016-10-15 02:39:15.185648: step 21820, loss = 0.78 (748.1 examples/sec; 0.171 sec/batch)
2016-10-15 02:39:16.954907: step 21830, loss = 1.00 (719.3 examples/sec; 0.178 sec/batch)
2016-10-15 02:39:18.717379: step 21840, loss = 0.69 (726.9 examples/sec; 0.176 sec/batch)
2016-10-15 02:39:20.479410: step 21850, loss = 0.83 (715.1 examples/sec; 0.179 sec/batch)
2016-10-15 02:39:22.236536: step 21860, loss = 0.81 (742.3 examples/sec; 0.172 sec/batch)
2016-10-15 02:39:24.006987: step 21870, loss = 0.89 (696.6 examples/sec; 0.184 sec/batch)
2016-10-15 02:39:25.759204: step 21880, loss = 0.88 (737.8 examples/sec; 0.173 sec/batch)
2016-10-15 02:39:27.512262: step 21890, loss = 0.88 (752.4 examples/sec; 0.170 sec/batch)
2016-10-15 02:39:29.285107: step 21900, loss = 0.99 (712.2 examples/sec; 0.180 sec/batch)
2016-10-15 02:39:31.236630: step 21910, loss = 0.77 (719.8 examples/sec; 0.178 sec/batch)
2016-10-15 02:39:32.994700: step 21920, loss = 0.74 (749.2 examples/sec; 0.171 sec/batch)
2016-10-15 02:39:34.753751: step 21930, loss = 0.82 (772.5 examples/sec; 0.166 sec/batch)
2016-10-15 02:39:36.512704: step 21940, loss = 0.82 (717.6 examples/sec; 0.178 sec/batch)
2016-10-15 02:39:38.270134: step 21950, loss = 0.76 (727.1 examples/sec; 0.176 sec/batch)
2016-10-15 02:39:40.035005: step 21960, loss = 0.95 (722.5 examples/sec; 0.177 sec/batch)
2016-10-15 02:39:41.797709: step 21970, loss = 1.00 (695.4 examples/sec; 0.184 sec/batch)
2016-10-15 02:39:43.549518: step 21980, loss = 0.72 (737.8 examples/sec; 0.173 sec/batch)
2016-10-15 02:39:45.305443: step 21990, loss = 0.85 (728.8 examples/sec; 0.176 sec/batch)
2016-10-15 02:39:47.066584: step 22000, loss = 0.82 (723.4 examples/sec; 0.177 sec/batch)
2016-10-15 02:39:49.515805: step 22010, loss = 0.81 (741.1 examples/sec; 0.173 sec/batch)
2016-10-15 02:39:51.275510: step 22020, loss = 0.83 (744.5 examples/sec; 0.172 sec/batch)
2016-10-15 02:39:53.032555: step 22030, loss = 0.94 (710.2 examples/sec; 0.180 sec/batch)
2016-10-15 02:39:54.796913: step 22040, loss = 0.77 (721.2 examples/sec; 0.177 sec/batch)
2016-10-15 02:39:56.551373: step 22050, loss = 0.88 (732.1 examples/sec; 0.175 sec/batch)
2016-10-15 02:39:58.314014: step 22060, loss = 0.77 (744.7 examples/sec; 0.172 sec/batch)
2016-10-15 02:40:00.074469: step 22070, loss = 0.93 (753.7 examples/sec; 0.170 sec/batch)
2016-10-15 02:40:01.841123: step 22080, loss = 0.81 (698.7 examples/sec; 0.183 sec/batch)
2016-10-15 02:40:03.624897: step 22090, loss = 0.85 (746.0 examples/sec; 0.172 sec/batch)
2016-10-15 02:40:05.389697: step 22100, loss = 0.94 (733.9 examples/sec; 0.174 sec/batch)
2016-10-15 02:40:07.338973: step 22110, loss = 0.74 (722.3 examples/sec; 0.177 sec/batch)
2016-10-15 02:40:09.107723: step 22120, loss = 0.87 (706.1 examples/sec; 0.181 sec/batch)
2016-10-15 02:40:10.866164: step 22130, loss = 0.93 (708.6 examples/sec; 0.181 sec/batch)
2016-10-15 02:40:12.623699: step 22140, loss = 0.82 (723.9 examples/sec; 0.177 sec/batch)
2016-10-15 02:40:14.389480: step 22150, loss = 0.75 (720.1 examples/sec; 0.178 sec/batch)
2016-10-15 02:40:16.150476: step 22160, loss = 0.84 (745.3 examples/sec; 0.172 sec/batch)
2016-10-15 02:40:17.918454: step 22170, loss = 0.64 (734.6 examples/sec; 0.174 sec/batch)
2016-10-15 02:40:19.682000: step 22180, loss = 0.87 (735.7 examples/sec; 0.174 sec/batch)
2016-10-15 02:40:21.437267: step 22190, loss = 0.82 (727.9 examples/sec; 0.176 sec/batch)
2016-10-15 02:40:23.204905: step 22200, loss = 0.74 (727.3 examples/sec; 0.176 sec/batch)
2016-10-15 02:40:25.145512: step 22210, loss = 0.96 (719.4 examples/sec; 0.178 sec/batch)
2016-10-15 02:40:26.906981: step 22220, loss = 0.83 (731.2 examples/sec; 0.175 sec/batch)
2016-10-15 02:40:28.671466: step 22230, loss = 1.10 (749.6 examples/sec; 0.171 sec/batch)
2016-10-15 02:40:30.433234: step 22240, loss = 0.81 (707.0 examples/sec; 0.181 sec/batch)
2016-10-15 02:40:32.192168: step 22250, loss = 0.78 (744.6 examples/sec; 0.172 sec/batch)
2016-10-15 02:40:33.952520: step 22260, loss = 0.83 (730.7 examples/sec; 0.175 sec/batch)
2016-10-15 02:40:35.713760: step 22270, loss = 0.71 (748.2 examples/sec; 0.171 sec/batch)
2016-10-15 02:40:37.472838: step 22280, loss = 0.81 (739.6 examples/sec; 0.173 sec/batch)
2016-10-15 02:40:39.233943: step 22290, loss = 0.82 (747.3 examples/sec; 0.171 sec/batch)
2016-10-15 02:40:40.995044: step 22300, loss = 0.77 (729.7 examples/sec; 0.175 sec/batch)
2016-10-15 02:40:42.944894: step 22310, loss = 0.89 (720.5 examples/sec; 0.178 sec/batch)
2016-10-15 02:40:44.706016: step 22320, loss = 1.02 (717.9 examples/sec; 0.178 sec/batch)
2016-10-15 02:40:46.466258: step 22330, loss = 0.82 (727.5 examples/sec; 0.176 sec/batch)
2016-10-15 02:40:48.224942: step 22340, loss = 0.85 (735.4 examples/sec; 0.174 sec/batch)
2016-10-15 02:40:49.989847: step 22350, loss = 1.02 (707.1 examples/sec; 0.181 sec/batch)
2016-10-15 02:40:51.739595: step 22360, loss = 0.75 (736.5 examples/sec; 0.174 sec/batch)
2016-10-15 02:40:53.508784: step 22370, loss = 0.88 (743.8 examples/sec; 0.172 sec/batch)
2016-10-15 02:40:55.269883: step 22380, loss = 0.76 (698.2 examples/sec; 0.183 sec/batch)
2016-10-15 02:40:57.023377: step 22390, loss = 0.72 (729.1 examples/sec; 0.176 sec/batch)
2016-10-15 02:40:58.781090: step 22400, loss = 0.77 (738.0 examples/sec; 0.173 sec/batch)
2016-10-15 02:41:00.723435: step 22410, loss = 0.80 (728.2 examples/sec; 0.176 sec/batch)
2016-10-15 02:41:02.481400: step 22420, loss = 1.02 (725.7 examples/sec; 0.176 sec/batch)
2016-10-15 02:41:04.241931: step 22430, loss = 1.01 (717.5 examples/sec; 0.178 sec/batch)
2016-10-15 02:41:06.005356: step 22440, loss = 0.84 (733.6 examples/sec; 0.174 sec/batch)
2016-10-15 02:41:07.761991: step 22450, loss = 1.10 (723.6 examples/sec; 0.177 sec/batch)
2016-10-15 02:41:09.523263: step 22460, loss = 0.76 (727.9 examples/sec; 0.176 sec/batch)
2016-10-15 02:41:11.280745: step 22470, loss = 0.87 (753.7 examples/sec; 0.170 sec/batch)
2016-10-15 02:41:13.044401: step 22480, loss = 0.81 (705.6 examples/sec; 0.181 sec/batch)
2016-10-15 02:41:14.810687: step 22490, loss = 0.82 (721.9 examples/sec; 0.177 sec/batch)
2016-10-15 02:41:16.567922: step 22500, loss = 0.79 (723.8 examples/sec; 0.177 sec/batch)
2016-10-15 02:41:18.515035: step 22510, loss = 0.80 (729.4 examples/sec; 0.175 sec/batch)
2016-10-15 02:41:20.275727: step 22520, loss = 0.66 (723.4 examples/sec; 0.177 sec/batch)
2016-10-15 02:41:22.043902: step 22530, loss = 0.95 (728.6 examples/sec; 0.176 sec/batch)
2016-10-15 02:41:23.797770: step 22540, loss = 0.66 (740.1 examples/sec; 0.173 sec/batch)
2016-10-15 02:41:25.575656: step 22550, loss = 0.75 (709.5 examples/sec; 0.180 sec/batch)
2016-10-15 02:41:27.323403: step 22560, loss = 0.80 (729.6 examples/sec; 0.175 sec/batch)
2016-10-15 02:41:29.075560: step 22570, loss = 0.68 (725.3 examples/sec; 0.176 sec/batch)
2016-10-15 02:41:30.835607: step 22580, loss = 0.88 (734.9 examples/sec; 0.174 sec/batch)
2016-10-15 02:41:32.593740: step 22590, loss = 0.86 (725.0 examples/sec; 0.177 sec/batch)
2016-10-15 02:41:34.350065: step 22600, loss = 0.83 (744.3 examples/sec; 0.172 sec/batch)
2016-10-15 02:41:36.287794: step 22610, loss = 0.65 (738.5 examples/sec; 0.173 sec/batch)
2016-10-15 02:41:38.049960: step 22620, loss = 0.92 (717.9 examples/sec; 0.178 sec/batch)
2016-10-15 02:41:39.807599: step 22630, loss = 0.70 (718.7 examples/sec; 0.178 sec/batch)
2016-10-15 02:41:41.563132: step 22640, loss = 0.75 (760.9 examples/sec; 0.168 sec/batch)
2016-10-15 02:41:43.321810: step 22650, loss = 0.91 (726.7 examples/sec; 0.176 sec/batch)
2016-10-15 02:41:45.085682: step 22660, loss = 0.69 (729.3 examples/sec; 0.176 sec/batch)
2016-10-15 02:41:46.837711: step 22670, loss = 0.99 (734.8 examples/sec; 0.174 sec/batch)
2016-10-15 02:41:48.595443: step 22680, loss = 0.74 (713.3 examples/sec; 0.179 sec/batch)
2016-10-15 02:41:50.349050: step 22690, loss = 0.95 (721.2 examples/sec; 0.177 sec/batch)
2016-10-15 02:41:52.102286: step 22700, loss = 0.63 (759.3 examples/sec; 0.169 sec/batch)
2016-10-15 02:41:54.056839: step 22710, loss = 0.99 (733.1 examples/sec; 0.175 sec/batch)
2016-10-15 02:41:55.818219: step 22720, loss = 0.92 (720.2 examples/sec; 0.178 sec/batch)
2016-10-15 02:41:57.574940: step 22730, loss = 0.76 (724.3 examples/sec; 0.177 sec/batch)
2016-10-15 02:41:59.333710: step 22740, loss = 0.65 (729.1 examples/sec; 0.176 sec/batch)
2016-10-15 02:42:01.096757: step 22750, loss = 0.83 (743.7 examples/sec; 0.172 sec/batch)
2016-10-15 02:42:02.868424: step 22760, loss = 0.74 (716.1 examples/sec; 0.179 sec/batch)
2016-10-15 02:42:04.629390: step 22770, loss = 0.85 (730.6 examples/sec; 0.175 sec/batch)
2016-10-15 02:42:06.380873: step 22780, loss = 0.84 (751.6 examples/sec; 0.170 sec/batch)
2016-10-15 02:42:08.140327: step 22790, loss = 0.89 (692.7 examples/sec; 0.185 sec/batch)
2016-10-15 02:42:09.899985: step 22800, loss = 0.89 (708.7 examples/sec; 0.181 sec/batch)
2016-10-15 02:42:11.836600: step 22810, loss = 0.76 (728.2 examples/sec; 0.176 sec/batch)
2016-10-15 02:42:13.592763: step 22820, loss = 0.71 (751.2 examples/sec; 0.170 sec/batch)
2016-10-15 02:42:15.355791: step 22830, loss = 0.71 (727.0 examples/sec; 0.176 sec/batch)
2016-10-15 02:42:17.116341: step 22840, loss = 0.94 (724.7 examples/sec; 0.177 sec/batch)
2016-10-15 02:42:18.863085: step 22850, loss = 0.82 (740.8 examples/sec; 0.173 sec/batch)
2016-10-15 02:42:20.618371: step 22860, loss = 0.76 (716.0 examples/sec; 0.179 sec/batch)
2016-10-15 02:42:22.368902: step 22870, loss = 0.63 (717.3 examples/sec; 0.178 sec/batch)
2016-10-15 02:42:24.130876: step 22880, loss = 0.84 (742.6 examples/sec; 0.172 sec/batch)
2016-10-15 02:42:25.888641: step 22890, loss = 0.83 (722.5 examples/sec; 0.177 sec/batch)
2016-10-15 02:42:27.644086: step 22900, loss = 0.91 (736.5 examples/sec; 0.174 sec/batch)
2016-10-15 02:42:29.600960: step 22910, loss = 0.95 (691.8 examples/sec; 0.185 sec/batch)
2016-10-15 02:42:31.355130: step 22920, loss = 0.83 (732.7 examples/sec; 0.175 sec/batch)
2016-10-15 02:42:33.115944: step 22930, loss = 0.70 (706.2 examples/sec; 0.181 sec/batch)
2016-10-15 02:42:34.872151: step 22940, loss = 0.72 (727.1 examples/sec; 0.176 sec/batch)
2016-10-15 02:42:36.642742: step 22950, loss = 0.79 (723.7 examples/sec; 0.177 sec/batch)
2016-10-15 02:42:38.395502: step 22960, loss = 0.89 (741.0 examples/sec; 0.173 sec/batch)
2016-10-15 02:42:40.155547: step 22970, loss = 0.83 (733.4 examples/sec; 0.175 sec/batch)
2016-10-15 02:42:41.914742: step 22980, loss = 0.72 (739.2 examples/sec; 0.173 sec/batch)
2016-10-15 02:42:43.669642: step 22990, loss = 0.86 (727.4 examples/sec; 0.176 sec/batch)
2016-10-15 02:42:45.429750: step 23000, loss = 0.68 (725.6 examples/sec; 0.176 sec/batch)
2016-10-15 02:42:47.930460: step 23010, loss = 0.82 (709.0 examples/sec; 0.181 sec/batch)
2016-10-15 02:42:49.682039: step 23020, loss = 0.88 (732.8 examples/sec; 0.175 sec/batch)
2016-10-15 02:42:51.452336: step 23030, loss = 1.04 (712.3 examples/sec; 0.180 sec/batch)
2016-10-15 02:42:53.211436: step 23040, loss = 0.72 (748.4 examples/sec; 0.171 sec/batch)
2016-10-15 02:42:54.965056: step 23050, loss = 0.74 (771.6 examples/sec; 0.166 sec/batch)
2016-10-15 02:42:56.731452: step 23060, loss = 0.83 (748.1 examples/sec; 0.171 sec/batch)
2016-10-15 02:42:58.491266: step 23070, loss = 0.74 (728.2 examples/sec; 0.176 sec/batch)
2016-10-15 02:43:00.246726: step 23080, loss = 0.77 (736.5 examples/sec; 0.174 sec/batch)
2016-10-15 02:43:02.015789: step 23090, loss = 0.85 (695.9 examples/sec; 0.184 sec/batch)
2016-10-15 02:43:03.781222: step 23100, loss = 0.98 (737.1 examples/sec; 0.174 sec/batch)
2016-10-15 02:43:05.730899: step 23110, loss = 0.84 (712.0 examples/sec; 0.180 sec/batch)
2016-10-15 02:43:07.489041: step 23120, loss = 0.81 (719.7 examples/sec; 0.178 sec/batch)
2016-10-15 02:43:09.255923: step 23130, loss = 0.85 (714.0 examples/sec; 0.179 sec/batch)
2016-10-15 02:43:11.007997: step 23140, loss = 0.88 (751.8 examples/sec; 0.170 sec/batch)
2016-10-15 02:43:12.765916: step 23150, loss = 0.89 (731.2 examples/sec; 0.175 sec/batch)
2016-10-15 02:43:14.529267: step 23160, loss = 0.84 (727.8 examples/sec; 0.176 sec/batch)
2016-10-15 02:43:16.286615: step 23170, loss = 0.79 (754.6 examples/sec; 0.170 sec/batch)
2016-10-15 02:43:18.051022: step 23180, loss = 0.81 (718.8 examples/sec; 0.178 sec/batch)
2016-10-15 02:43:19.808791: step 23190, loss = 0.79 (741.0 examples/sec; 0.173 sec/batch)
2016-10-15 02:43:21.565955: step 23200, loss = 0.98 (714.8 examples/sec; 0.179 sec/batch)
2016-10-15 02:43:23.507484: step 23210, loss = 0.81 (712.1 examples/sec; 0.180 sec/batch)
2016-10-15 02:43:25.268550: step 23220, loss = 0.74 (721.9 examples/sec; 0.177 sec/batch)
2016-10-15 02:43:27.027767: step 23230, loss = 0.91 (727.4 examples/sec; 0.176 sec/batch)
2016-10-15 02:43:28.787782: step 23240, loss = 0.85 (740.8 examples/sec; 0.173 sec/batch)
2016-10-15 02:43:30.552623: step 23250, loss = 0.79 (698.9 examples/sec; 0.183 sec/batch)
2016-10-15 02:43:32.305339: step 23260, loss = 0.76 (730.8 examples/sec; 0.175 sec/batch)
2016-10-15 02:43:34.069063: step 23270, loss = 0.86 (724.8 examples/sec; 0.177 sec/batch)
2016-10-15 02:43:35.820707: step 23280, loss = 0.91 (726.5 examples/sec; 0.176 sec/batch)
2016-10-15 02:43:37.576192: step 23290, loss = 0.74 (747.8 examples/sec; 0.171 sec/batch)
2016-10-15 02:43:39.338117: step 23300, loss = 0.72 (731.5 examples/sec; 0.175 sec/batch)
2016-10-15 02:43:41.297734: step 23310, loss = 0.98 (721.5 examples/sec; 0.177 sec/batch)
2016-10-15 02:43:43.060578: step 23320, loss = 0.84 (731.2 examples/sec; 0.175 sec/batch)
2016-10-15 02:43:44.825778: step 23330, loss = 0.77 (719.4 examples/sec; 0.178 sec/batch)
2016-10-15 02:43:46.581998: step 23340, loss = 0.73 (748.9 examples/sec; 0.171 sec/batch)
2016-10-15 02:43:48.343297: step 23350, loss = 0.76 (712.1 examples/sec; 0.180 sec/batch)
2016-10-15 02:43:50.104726: step 23360, loss = 0.90 (733.0 examples/sec; 0.175 sec/batch)
2016-10-15 02:43:51.863719: step 23370, loss = 0.77 (717.6 examples/sec; 0.178 sec/batch)
2016-10-15 02:43:53.639460: step 23380, loss = 0.83 (698.6 examples/sec; 0.183 sec/batch)
2016-10-15 02:43:55.390829: step 23390, loss = 0.68 (746.8 examples/sec; 0.171 sec/batch)
2016-10-15 02:43:57.154966: step 23400, loss = 0.89 (711.6 examples/sec; 0.180 sec/batch)
2016-10-15 02:43:59.136770: step 23410, loss = 0.85 (728.1 examples/sec; 0.176 sec/batch)
2016-10-15 02:44:00.901410: step 23420, loss = 0.72 (725.2 examples/sec; 0.176 sec/batch)
2016-10-15 02:44:02.664755: step 23430, loss = 0.80 (726.5 examples/sec; 0.176 sec/batch)
2016-10-15 02:44:04.428775: step 23440, loss = 0.99 (721.3 examples/sec; 0.177 sec/batch)
2016-10-15 02:44:06.189226: step 23450, loss = 0.73 (712.8 examples/sec; 0.180 sec/batch)
2016-10-15 02:44:07.945078: step 23460, loss = 0.86 (732.1 examples/sec; 0.175 sec/batch)
2016-10-15 02:44:09.704396: step 23470, loss = 0.90 (724.0 examples/sec; 0.177 sec/batch)
2016-10-15 02:44:11.463918: step 23480, loss = 0.79 (723.0 examples/sec; 0.177 sec/batch)
2016-10-15 02:44:13.220423: step 23490, loss = 0.94 (709.4 examples/sec; 0.180 sec/batch)
2016-10-15 02:44:14.981528: step 23500, loss = 0.89 (731.4 examples/sec; 0.175 sec/batch)
2016-10-15 02:44:16.922074: step 23510, loss = 0.71 (735.8 examples/sec; 0.174 sec/batch)
2016-10-15 02:44:18.684447: step 23520, loss = 0.76 (735.3 examples/sec; 0.174 sec/batch)
2016-10-15 02:44:20.444381: step 23530, loss = 0.68 (725.4 examples/sec; 0.176 sec/batch)
2016-10-15 02:44:22.199321: step 23540, loss = 0.85 (745.9 examples/sec; 0.172 sec/batch)
2016-10-15 02:44:23.963633: step 23550, loss = 0.74 (726.4 examples/sec; 0.176 sec/batch)
2016-10-15 02:44:25.736304: step 23560, loss = 0.87 (678.8 examples/sec; 0.189 sec/batch)
2016-10-15 02:44:27.489566: step 23570, loss = 0.81 (722.1 examples/sec; 0.177 sec/batch)
2016-10-15 02:44:29.255232: step 23580, loss = 0.80 (729.1 examples/sec; 0.176 sec/batch)
2016-10-15 02:44:31.011190: step 23590, loss = 0.89 (757.0 examples/sec; 0.169 sec/batch)
2016-10-15 02:44:32.781626: step 23600, loss = 0.93 (729.9 examples/sec; 0.175 sec/batch)
2016-10-15 02:44:34.737107: step 23610, loss = 0.81 (722.9 examples/sec; 0.177 sec/batch)
2016-10-15 02:44:36.492514: step 23620, loss = 0.76 (752.3 examples/sec; 0.170 sec/batch)
2016-10-15 02:44:38.259565: step 23630, loss = 0.79 (708.8 examples/sec; 0.181 sec/batch)
2016-10-15 02:44:40.015033: step 23640, loss = 0.84 (736.4 examples/sec; 0.174 sec/batch)
2016-10-15 02:44:41.777451: step 23650, loss = 0.76 (728.6 examples/sec; 0.176 sec/batch)
2016-10-15 02:44:43.539138: step 23660, loss = 0.83 (699.8 examples/sec; 0.183 sec/batch)
2016-10-15 02:44:45.289723: step 23670, loss = 0.70 (710.7 examples/sec; 0.180 sec/batch)
2016-10-15 02:44:47.045945: step 23680, loss = 0.65 (753.3 examples/sec; 0.170 sec/batch)
2016-10-15 02:44:48.800026: step 23690, loss = 0.72 (724.6 examples/sec; 0.177 sec/batch)
2016-10-15 02:44:50.561308: step 23700, loss = 0.74 (744.0 examples/sec; 0.172 sec/batch)
2016-10-15 02:44:52.498614: step 23710, loss = 0.88 (719.2 examples/sec; 0.178 sec/batch)
2016-10-15 02:44:54.255562: step 23720, loss = 0.89 (727.9 examples/sec; 0.176 sec/batch)
2016-10-15 02:44:56.013554: step 23730, loss = 0.71 (726.2 examples/sec; 0.176 sec/batch)
2016-10-15 02:44:57.772064: step 23740, loss = 0.70 (726.8 examples/sec; 0.176 sec/batch)
2016-10-15 02:44:59.531071: step 23750, loss = 0.84 (719.7 examples/sec; 0.178 sec/batch)
2016-10-15 02:45:01.285918: step 23760, loss = 0.76 (740.7 examples/sec; 0.173 sec/batch)
2016-10-15 02:45:03.064202: step 23770, loss = 0.79 (729.8 examples/sec; 0.175 sec/batch)
2016-10-15 02:45:04.828385: step 23780, loss = 0.76 (729.4 examples/sec; 0.175 sec/batch)
2016-10-15 02:45:06.585833: step 23790, loss = 0.75 (733.3 examples/sec; 0.175 sec/batch)
2016-10-15 02:45:08.331951: step 23800, loss = 0.82 (730.6 examples/sec; 0.175 sec/batch)
2016-10-15 02:45:10.292547: step 23810, loss = 0.70 (712.5 examples/sec; 0.180 sec/batch)
2016-10-15 02:45:12.046869: step 23820, loss = 0.74 (758.2 examples/sec; 0.169 sec/batch)
2016-10-15 02:45:13.820149: step 23830, loss = 0.69 (719.1 examples/sec; 0.178 sec/batch)
2016-10-15 02:45:15.581546: step 23840, loss = 0.86 (725.1 examples/sec; 0.177 sec/batch)
2016-10-15 02:45:17.355907: step 23850, loss = 0.71 (689.9 examples/sec; 0.186 sec/batch)
2016-10-15 02:45:19.110655: step 23860, loss = 0.69 (730.1 examples/sec; 0.175 sec/batch)
2016-10-15 02:45:20.872526: step 23870, loss = 0.83 (745.7 examples/sec; 0.172 sec/batch)
2016-10-15 02:45:22.640668: step 23880, loss = 0.93 (721.0 examples/sec; 0.178 sec/batch)
2016-10-15 02:45:24.408620: step 23890, loss = 0.66 (700.2 examples/sec; 0.183 sec/batch)
2016-10-15 02:45:26.168528: step 23900, loss = 0.69 (733.2 examples/sec; 0.175 sec/batch)
2016-10-15 02:45:28.115152: step 23910, loss = 0.90 (725.4 examples/sec; 0.176 sec/batch)
2016-10-15 02:45:29.872391: step 23920, loss = 0.92 (733.1 examples/sec; 0.175 sec/batch)
2016-10-15 02:45:31.630994: step 23930, loss = 0.73 (736.7 examples/sec; 0.174 sec/batch)
2016-10-15 02:45:33.399974: step 23940, loss = 0.83 (723.5 examples/sec; 0.177 sec/batch)
2016-10-15 02:45:35.159879: step 23950, loss = 0.70 (702.0 examples/sec; 0.182 sec/batch)
2016-10-15 02:45:36.923443: step 23960, loss = 0.87 (713.5 examples/sec; 0.179 sec/batch)
2016-10-15 02:45:38.674834: step 23970, loss = 0.80 (715.6 examples/sec; 0.179 sec/batch)
2016-10-15 02:45:40.427916: step 23980, loss = 0.87 (736.6 examples/sec; 0.174 sec/batch)
2016-10-15 02:45:42.186714: step 23990, loss = 0.66 (782.7 examples/sec; 0.164 sec/batch)
2016-10-15 02:45:43.947603: step 24000, loss = 0.75 (747.4 examples/sec; 0.171 sec/batch)
2016-10-15 02:45:46.477489: step 24010, loss = 0.80 (722.5 examples/sec; 0.177 sec/batch)
2016-10-15 02:45:48.234843: step 24020, loss = 0.75 (755.4 examples/sec; 0.169 sec/batch)
2016-10-15 02:45:49.999465: step 24030, loss = 0.86 (728.8 examples/sec; 0.176 sec/batch)
2016-10-15 02:45:51.756118: step 24040, loss = 0.70 (719.5 examples/sec; 0.178 sec/batch)
2016-10-15 02:45:53.510373: step 24050, loss = 0.74 (745.6 examples/sec; 0.172 sec/batch)
2016-10-15 02:45:55.276909: step 24060, loss = 0.76 (740.5 examples/sec; 0.173 sec/batch)
2016-10-15 02:45:57.034361: step 24070, loss = 0.65 (722.3 examples/sec; 0.177 sec/batch)
2016-10-15 02:45:58.791534: step 24080, loss = 0.73 (732.7 examples/sec; 0.175 sec/batch)
2016-10-15 02:46:00.555815: step 24090, loss = 0.78 (740.7 examples/sec; 0.173 sec/batch)
2016-10-15 02:46:02.316124: step 24100, loss = 0.87 (709.3 examples/sec; 0.180 sec/batch)
2016-10-15 02:46:04.258589: step 24110, loss = 0.73 (734.8 examples/sec; 0.174 sec/batch)
2016-10-15 02:46:06.024682: step 24120, loss = 0.83 (730.0 examples/sec; 0.175 sec/batch)
2016-10-15 02:46:07.786581: step 24130, loss = 0.84 (720.3 examples/sec; 0.178 sec/batch)
2016-10-15 02:46:09.556317: step 24140, loss = 0.71 (695.6 examples/sec; 0.184 sec/batch)
2016-10-15 02:46:11.317339: step 24150, loss = 0.86 (738.1 examples/sec; 0.173 sec/batch)
2016-10-15 02:46:13.082209: step 24160, loss = 0.79 (725.9 examples/sec; 0.176 sec/batch)
2016-10-15 02:46:14.841806: step 24170, loss = 0.69 (725.3 examples/sec; 0.176 sec/batch)
2016-10-15 02:46:16.607677: step 24180, loss = 0.85 (725.9 examples/sec; 0.176 sec/batch)
2016-10-15 02:46:18.372787: step 24190, loss = 0.87 (733.4 examples/sec; 0.175 sec/batch)
2016-10-15 02:46:20.129733: step 24200, loss = 0.75 (713.7 examples/sec; 0.179 sec/batch)
2016-10-15 02:46:22.083488: step 24210, loss = 0.59 (738.9 examples/sec; 0.173 sec/batch)
2016-10-15 02:46:23.842717: step 24220, loss = 0.68 (702.5 examples/sec; 0.182 sec/batch)
2016-10-15 02:46:25.597230: step 24230, loss = 0.70 (733.3 examples/sec; 0.175 sec/batch)
2016-10-15 02:46:27.356635: step 24240, loss = 0.78 (733.2 examples/sec; 0.175 sec/batch)
2016-10-15 02:46:29.119112: step 24250, loss = 0.88 (725.6 examples/sec; 0.176 sec/batch)
2016-10-15 02:46:30.880106: step 24260, loss = 0.64 (721.7 examples/sec; 0.177 sec/batch)
2016-10-15 02:46:32.640381: step 24270, loss = 0.71 (740.5 examples/sec; 0.173 sec/batch)
2016-10-15 02:46:34.403562: step 24280, loss = 0.68 (734.0 examples/sec; 0.174 sec/batch)
2016-10-15 02:46:36.170859: step 24290, loss = 0.71 (720.6 examples/sec; 0.178 sec/batch)
2016-10-15 02:46:37.932922: step 24300, loss = 0.73 (744.4 examples/sec; 0.172 sec/batch)
2016-10-15 02:46:39.884294: step 24310, loss = 0.66 (715.8 examples/sec; 0.179 sec/batch)
2016-10-15 02:46:41.644385: step 24320, loss = 0.76 (705.2 examples/sec; 0.182 sec/batch)
2016-10-15 02:46:43.396155: step 24330, loss = 0.87 (723.5 examples/sec; 0.177 sec/batch)
2016-10-15 02:46:45.160779: step 24340, loss = 0.81 (717.2 examples/sec; 0.178 sec/batch)
2016-10-15 02:46:46.911260: step 24350, loss = 0.74 (722.3 examples/sec; 0.177 sec/batch)
2016-10-15 02:46:48.671303: step 24360, loss = 0.78 (705.2 examples/sec; 0.182 sec/batch)
2016-10-15 02:46:50.431714: step 24370, loss = 0.80 (727.1 examples/sec; 0.176 sec/batch)
2016-10-15 02:46:52.193281: step 24380, loss = 0.79 (723.1 examples/sec; 0.177 sec/batch)
2016-10-15 02:46:53.957854: step 24390, loss = 0.79 (696.9 examples/sec; 0.184 sec/batch)
2016-10-15 02:46:55.706792: step 24400, loss = 0.86 (741.5 examples/sec; 0.173 sec/batch)
2016-10-15 02:46:57.649955: step 24410, loss = 0.79 (709.9 examples/sec; 0.180 sec/batch)
2016-10-15 02:46:59.407930: step 24420, loss = 0.74 (712.5 examples/sec; 0.180 sec/batch)
2016-10-15 02:47:01.158756: step 24430, loss = 0.94 (721.4 examples/sec; 0.177 sec/batch)
2016-10-15 02:47:02.919082: step 24440, loss = 0.82 (724.9 examples/sec; 0.177 sec/batch)
2016-10-15 02:47:04.687477: step 24450, loss = 0.84 (729.7 examples/sec; 0.175 sec/batch)
2016-10-15 02:47:06.452956: step 24460, loss = 0.80 (701.4 examples/sec; 0.182 sec/batch)
2016-10-15 02:47:08.217556: step 24470, loss = 0.67 (710.5 examples/sec; 0.180 sec/batch)
2016-10-15 02:47:09.975101: step 24480, loss = 0.83 (716.9 examples/sec; 0.179 sec/batch)
2016-10-15 02:47:11.739282: step 24490, loss = 0.88 (715.9 examples/sec; 0.179 sec/batch)
2016-10-15 02:47:13.498740: step 24500, loss = 0.70 (729.6 examples/sec; 0.175 sec/batch)
2016-10-15 02:47:15.449912: step 24510, loss = 0.77 (716.1 examples/sec; 0.179 sec/batch)
2016-10-15 02:47:17.216186: step 24520, loss = 0.86 (718.2 examples/sec; 0.178 sec/batch)
2016-10-15 02:47:18.969485: step 24530, loss = 0.83 (744.8 examples/sec; 0.172 sec/batch)
2016-10-15 02:47:20.728352: step 24540, loss = 0.77 (701.8 examples/sec; 0.182 sec/batch)
2016-10-15 02:47:22.491511: step 24550, loss = 0.89 (719.2 examples/sec; 0.178 sec/batch)
2016-10-15 02:47:24.248604: step 24560, loss = 0.85 (700.7 examples/sec; 0.183 sec/batch)
2016-10-15 02:47:26.008024: step 24570, loss = 1.15 (721.0 examples/sec; 0.178 sec/batch)
2016-10-15 02:47:27.766194: step 24580, loss = 0.79 (727.3 examples/sec; 0.176 sec/batch)
2016-10-15 02:47:29.527295: step 24590, loss = 0.78 (742.5 examples/sec; 0.172 sec/batch)
2016-10-15 02:47:31.291510: step 24600, loss = 0.93 (724.9 examples/sec; 0.177 sec/batch)
2016-10-15 02:47:33.247656: step 24610, loss = 0.77 (753.8 examples/sec; 0.170 sec/batch)
2016-10-15 02:47:35.016626: step 24620, loss = 0.85 (729.3 examples/sec; 0.176 sec/batch)
2016-10-15 02:47:36.778151: step 24630, loss = 0.75 (726.9 examples/sec; 0.176 sec/batch)
2016-10-15 02:47:38.542023: step 24640, loss = 0.91 (724.3 examples/sec; 0.177 sec/batch)
2016-10-15 02:47:40.305470: step 24650, loss = 1.02 (706.4 examples/sec; 0.181 sec/batch)
2016-10-15 02:47:42.065715: step 24660, loss = 0.92 (722.6 examples/sec; 0.177 sec/batch)
2016-10-15 02:47:43.820381: step 24670, loss = 0.70 (712.3 examples/sec; 0.180 sec/batch)
2016-10-15 02:47:45.585917: step 24680, loss = 0.60 (721.2 examples/sec; 0.177 sec/batch)
2016-10-15 02:47:47.343862: step 24690, loss = 0.94 (736.1 examples/sec; 0.174 sec/batch)
2016-10-15 02:47:49.110063: step 24700, loss = 0.77 (732.2 examples/sec; 0.175 sec/batch)
2016-10-15 02:47:51.067668: step 24710, loss = 0.89 (743.8 examples/sec; 0.172 sec/batch)
2016-10-15 02:47:52.835167: step 24720, loss = 0.85 (715.4 examples/sec; 0.179 sec/batch)
2016-10-15 02:47:54.602056: step 24730, loss = 0.91 (726.4 examples/sec; 0.176 sec/batch)
2016-10-15 02:47:56.362495: step 24740, loss = 0.84 (726.4 examples/sec; 0.176 sec/batch)
2016-10-15 02:47:58.131777: step 24750, loss = 0.75 (722.0 examples/sec; 0.177 sec/batch)
2016-10-15 02:47:59.892260: step 24760, loss = 0.93 (725.8 examples/sec; 0.176 sec/batch)
2016-10-15 02:48:01.669281: step 24770, loss = 0.69 (693.5 examples/sec; 0.185 sec/batch)
2016-10-15 02:48:03.420215: step 24780, loss = 0.84 (729.1 examples/sec; 0.176 sec/batch)
2016-10-15 02:48:05.178280: step 24790, loss = 0.79 (750.4 examples/sec; 0.171 sec/batch)
2016-10-15 02:48:06.950616: step 24800, loss = 0.93 (723.8 examples/sec; 0.177 sec/batch)
2016-10-15 02:48:08.902825: step 24810, loss = 0.78 (730.8 examples/sec; 0.175 sec/batch)
2016-10-15 02:48:10.668321: step 24820, loss = 0.85 (724.6 examples/sec; 0.177 sec/batch)
2016-10-15 02:48:12.439269: step 24830, loss = 0.78 (721.9 examples/sec; 0.177 sec/batch)
2016-10-15 02:48:14.201866: step 24840, loss = 0.80 (721.9 examples/sec; 0.177 sec/batch)
2016-10-15 02:48:15.962564: step 24850, loss = 0.92 (728.2 examples/sec; 0.176 sec/batch)
2016-10-15 02:48:17.722765: step 24860, loss = 0.85 (722.8 examples/sec; 0.177 sec/batch)
2016-10-15 02:48:19.482130: step 24870, loss = 0.75 (710.5 examples/sec; 0.180 sec/batch)
2016-10-15 02:48:21.238797: step 24880, loss = 0.76 (725.0 examples/sec; 0.177 sec/batch)
2016-10-15 02:48:22.992188: step 24890, loss = 0.86 (754.7 examples/sec; 0.170 sec/batch)
2016-10-15 02:48:24.746966: step 24900, loss = 0.79 (737.0 examples/sec; 0.174 sec/batch)
2016-10-15 02:48:26.696438: step 24910, loss = 0.69 (759.2 examples/sec; 0.169 sec/batch)
2016-10-15 02:48:28.464848: step 24920, loss = 0.84 (707.3 examples/sec; 0.181 sec/batch)
2016-10-15 02:48:30.219751: step 24930, loss = 0.71 (730.4 examples/sec; 0.175 sec/batch)
2016-10-15 02:48:31.977699: step 24940, loss = 0.93 (739.2 examples/sec; 0.173 sec/batch)
2016-10-15 02:48:33.738841: step 24950, loss = 0.89 (706.0 examples/sec; 0.181 sec/batch)
2016-10-15 02:48:35.497918: step 24960, loss = 0.71 (696.2 examples/sec; 0.184 sec/batch)
2016-10-15 02:48:37.258771: step 24970, loss = 0.68 (700.6 examples/sec; 0.183 sec/batch)
2016-10-15 02:48:39.011309: step 24980, loss = 0.82 (738.3 examples/sec; 0.173 sec/batch)
2016-10-15 02:48:40.779244: step 24990, loss = 0.84 (714.2 examples/sec; 0.179 sec/batch)
2016-10-15 02:48:42.533018: step 25000, loss = 0.84 (737.4 examples/sec; 0.174 sec/batch)
2016-10-15 02:48:45.083032: step 25010, loss = 0.90 (692.4 examples/sec; 0.185 sec/batch)
2016-10-15 02:48:46.836381: step 25020, loss = 0.75 (713.6 examples/sec; 0.179 sec/batch)
2016-10-15 02:48:48.595557: step 25030, loss = 0.81 (736.9 examples/sec; 0.174 sec/batch)
2016-10-15 02:48:50.350176: step 25040, loss = 0.69 (738.9 examples/sec; 0.173 sec/batch)
2016-10-15 02:48:52.104716: step 25050, loss = 0.84 (736.4 examples/sec; 0.174 sec/batch)
2016-10-15 02:48:53.867944: step 25060, loss = 0.71 (722.5 examples/sec; 0.177 sec/batch)
2016-10-15 02:48:55.635301: step 25070, loss = 0.83 (712.3 examples/sec; 0.180 sec/batch)
2016-10-15 02:48:57.387253: step 25080, loss = 0.82 (740.4 examples/sec; 0.173 sec/batch)
2016-10-15 02:48:59.148451: step 25090, loss = 0.74 (716.8 examples/sec; 0.179 sec/batch)
2016-10-15 02:49:00.907090: step 25100, loss = 0.78 (715.9 examples/sec; 0.179 sec/batch)
2016-10-15 02:49:02.850343: step 25110, loss = 0.79 (727.3 examples/sec; 0.176 sec/batch)
2016-10-15 02:49:04.614692: step 25120, loss = 0.57 (718.6 examples/sec; 0.178 sec/batch)
2016-10-15 02:49:06.383899: step 25130, loss = 0.80 (713.0 examples/sec; 0.180 sec/batch)
2016-10-15 02:49:08.143531: step 25140, loss = 0.64 (736.0 examples/sec; 0.174 sec/batch)
2016-10-15 02:49:09.899657: step 25150, loss = 0.78 (763.6 examples/sec; 0.168 sec/batch)
2016-10-15 02:49:11.661102: step 25160, loss = 0.80 (709.5 examples/sec; 0.180 sec/batch)
2016-10-15 02:49:13.427597: step 25170, loss = 0.78 (700.6 examples/sec; 0.183 sec/batch)
2016-10-15 02:49:15.177928: step 25180, loss = 0.84 (725.2 examples/sec; 0.176 sec/batch)
2016-10-15 02:49:16.933250: step 25190, loss = 0.60 (744.7 examples/sec; 0.172 sec/batch)
2016-10-15 02:49:18.689237: step 25200, loss = 0.72 (734.9 examples/sec; 0.174 sec/batch)
2016-10-15 02:49:20.640131: step 25210, loss = 0.75 (691.1 examples/sec; 0.185 sec/batch)
2016-10-15 02:49:22.397410: step 25220, loss = 0.79 (722.0 examples/sec; 0.177 sec/batch)
2016-10-15 02:49:24.152932: step 25230, loss = 0.77 (745.3 examples/sec; 0.172 sec/batch)
2016-10-15 02:49:25.917096: step 25240, loss = 0.79 (740.5 examples/sec; 0.173 sec/batch)
2016-10-15 02:49:27.673966: step 25250, loss = 0.87 (718.7 examples/sec; 0.178 sec/batch)
2016-10-15 02:49:29.428125: step 25260, loss = 0.79 (739.9 examples/sec; 0.173 sec/batch)
2016-10-15 02:49:31.180736: step 25270, loss = 0.79 (730.7 examples/sec; 0.175 sec/batch)
2016-10-15 02:49:32.942409: step 25280, loss = 0.97 (735.9 examples/sec; 0.174 sec/batch)
2016-10-15 02:49:34.707373: step 25290, loss = 0.79 (719.1 examples/sec; 0.178 sec/batch)
2016-10-15 02:49:36.456869: step 25300, loss = 0.77 (713.1 examples/sec; 0.180 sec/batch)
2016-10-15 02:49:38.387019: step 25310, loss = 0.78 (731.7 examples/sec; 0.175 sec/batch)
2016-10-15 02:49:40.156100: step 25320, loss = 0.72 (695.1 examples/sec; 0.184 sec/batch)
2016-10-15 02:49:41.919656: step 25330, loss = 0.80 (729.5 examples/sec; 0.175 sec/batch)
2016-10-15 02:49:43.684866: step 25340, loss = 0.66 (735.1 examples/sec; 0.174 sec/batch)
2016-10-15 02:49:45.435617: step 25350, loss = 0.80 (705.5 examples/sec; 0.181 sec/batch)
2016-10-15 02:49:47.197398: step 25360, loss = 0.74 (717.5 examples/sec; 0.178 sec/batch)
2016-10-15 02:49:48.961018: step 25370, loss = 0.79 (744.5 examples/sec; 0.172 sec/batch)
2016-10-15 02:49:50.720973: step 25380, loss = 0.84 (720.7 examples/sec; 0.178 sec/batch)
2016-10-15 02:49:52.490774: step 25390, loss = 0.75 (707.4 examples/sec; 0.181 sec/batch)
2016-10-15 02:49:54.249891: step 25400, loss = 0.77 (765.9 examples/sec; 0.167 sec/batch)
2016-10-15 02:49:56.207619: step 25410, loss = 0.62 (733.8 examples/sec; 0.174 sec/batch)
2016-10-15 02:49:57.976144: step 25420, loss = 0.81 (713.3 examples/sec; 0.179 sec/batch)
2016-10-15 02:49:59.735940: step 25430, loss = 0.90 (707.9 examples/sec; 0.181 sec/batch)
2016-10-15 02:50:01.507334: step 25440, loss = 0.68 (712.9 examples/sec; 0.180 sec/batch)
2016-10-15 02:50:03.290055: step 25450, loss = 0.85 (748.4 examples/sec; 0.171 sec/batch)
2016-10-15 02:50:05.055975: step 25460, loss = 0.83 (719.7 examples/sec; 0.178 sec/batch)
2016-10-15 02:50:06.823949: step 25470, loss = 0.87 (705.8 examples/sec; 0.181 sec/batch)
2016-10-15 02:50:08.591801: step 25480, loss = 0.67 (718.6 examples/sec; 0.178 sec/batch)
2016-10-15 02:50:10.354239: step 25490, loss = 0.92 (725.4 examples/sec; 0.176 sec/batch)
2016-10-15 02:50:12.116696: step 25500, loss = 0.77 (717.6 examples/sec; 0.178 sec/batch)
2016-10-15 02:50:14.063458: step 25510, loss = 0.77 (699.6 examples/sec; 0.183 sec/batch)
2016-10-15 02:50:15.830203: step 25520, loss = 0.77 (715.5 examples/sec; 0.179 sec/batch)
2016-10-15 02:50:17.604498: step 25530, loss = 0.71 (729.8 examples/sec; 0.175 sec/batch)
2016-10-15 02:50:19.363299: step 25540, loss = 0.78 (717.4 examples/sec; 0.178 sec/batch)
2016-10-15 02:50:21.130257: step 25550, loss = 0.76 (706.6 examples/sec; 0.181 sec/batch)
2016-10-15 02:50:22.879055: step 25560, loss = 0.77 (739.9 examples/sec; 0.173 sec/batch)
2016-10-15 02:50:24.651316: step 25570, loss = 0.97 (703.3 examples/sec; 0.182 sec/batch)
2016-10-15 02:50:26.407520: step 25580, loss = 0.82 (759.1 examples/sec; 0.169 sec/batch)
2016-10-15 02:50:28.164367: step 25590, loss = 0.71 (737.2 examples/sec; 0.174 sec/batch)
2016-10-15 02:50:29.918655: step 25600, loss = 0.84 (708.6 examples/sec; 0.181 sec/batch)
2016-10-15 02:50:31.869102: step 25610, loss = 0.74 (734.5 examples/sec; 0.174 sec/batch)
2016-10-15 02:50:33.634334: step 25620, loss = 0.84 (734.3 examples/sec; 0.174 sec/batch)
2016-10-15 02:50:35.401842: step 25630, loss = 0.80 (711.0 examples/sec; 0.180 sec/batch)
2016-10-15 02:50:37.154755: step 25640, loss = 0.74 (727.6 examples/sec; 0.176 sec/batch)
2016-10-15 02:50:38.922611: step 25650, loss = 0.80 (710.1 examples/sec; 0.180 sec/batch)
2016-10-15 02:50:40.678895: step 25660, loss = 0.77 (732.1 examples/sec; 0.175 sec/batch)
2016-10-15 02:50:42.438173: step 25670, loss = 0.91 (729.6 examples/sec; 0.175 sec/batch)
2016-10-15 02:50:44.195586: step 25680, loss = 0.82 (740.7 examples/sec; 0.173 sec/batch)
2016-10-15 02:50:45.951681: step 25690, loss = 0.70 (740.3 examples/sec; 0.173 sec/batch)
2016-10-15 02:50:47.712552: step 25700, loss = 0.85 (710.8 examples/sec; 0.180 sec/batch)
2016-10-15 02:50:49.660117: step 25710, loss = 0.83 (729.5 examples/sec; 0.175 sec/batch)
2016-10-15 02:50:51.417128: step 25720, loss = 0.93 (746.7 examples/sec; 0.171 sec/batch)
2016-10-15 02:50:53.180346: step 25730, loss = 1.02 (725.3 examples/sec; 0.176 sec/batch)
2016-10-15 02:50:54.938173: step 25740, loss = 0.91 (742.5 examples/sec; 0.172 sec/batch)
2016-10-15 02:50:56.702763: step 25750, loss = 0.75 (744.2 examples/sec; 0.172 sec/batch)
2016-10-15 02:50:58.470660: step 25760, loss = 0.76 (706.2 examples/sec; 0.181 sec/batch)
2016-10-15 02:51:00.236460: step 25770, loss = 0.79 (710.9 examples/sec; 0.180 sec/batch)
2016-10-15 02:51:02.010776: step 25780, loss = 0.77 (730.4 examples/sec; 0.175 sec/batch)
2016-10-15 02:51:03.774690: step 25790, loss = 0.68 (708.2 examples/sec; 0.181 sec/batch)
2016-10-15 02:51:05.537121: step 25800, loss = 0.76 (727.8 examples/sec; 0.176 sec/batch)
2016-10-15 02:51:07.477369: step 25810, loss = 0.93 (727.8 examples/sec; 0.176 sec/batch)
2016-10-15 02:51:09.240100: step 25820, loss = 0.82 (716.1 examples/sec; 0.179 sec/batch)
2016-10-15 02:51:10.993399: step 25830, loss = 0.82 (739.8 examples/sec; 0.173 sec/batch)
2016-10-15 02:51:12.763937: step 25840, loss = 0.78 (731.5 examples/sec; 0.175 sec/batch)
2016-10-15 02:51:14.526839: step 25850, loss = 0.68 (735.2 examples/sec; 0.174 sec/batch)
2016-10-15 02:51:16.282434: step 25860, loss = 0.74 (723.8 examples/sec; 0.177 sec/batch)
2016-10-15 02:51:18.050713: step 25870, loss = 0.83 (708.2 examples/sec; 0.181 sec/batch)
2016-10-15 02:51:19.805288: step 25880, loss = 0.90 (741.4 examples/sec; 0.173 sec/batch)
2016-10-15 02:51:21.569335: step 25890, loss = 0.78 (720.7 examples/sec; 0.178 sec/batch)
2016-10-15 02:51:23.323440: step 25900, loss = 0.90 (741.6 examples/sec; 0.173 sec/batch)
2016-10-15 02:51:25.262712: step 25910, loss = 0.76 (759.0 examples/sec; 0.169 sec/batch)
2016-10-15 02:51:27.034796: step 25920, loss = 0.75 (734.9 examples/sec; 0.174 sec/batch)
2016-10-15 02:51:28.793173: step 25930, loss = 0.82 (722.4 examples/sec; 0.177 sec/batch)
2016-10-15 02:51:30.557261: step 25940, loss = 0.64 (706.4 examples/sec; 0.181 sec/batch)
2016-10-15 02:51:32.310796: step 25950, loss = 0.84 (733.8 examples/sec; 0.174 sec/batch)
2016-10-15 02:51:34.064311: step 25960, loss = 0.82 (731.7 examples/sec; 0.175 sec/batch)
2016-10-15 02:51:35.821261: step 25970, loss = 0.78 (711.1 examples/sec; 0.180 sec/batch)
2016-10-15 02:51:37.575672: step 25980, loss = 0.88 (724.1 examples/sec; 0.177 sec/batch)
2016-10-15 02:51:39.329691: step 25990, loss = 0.67 (753.6 examples/sec; 0.170 sec/batch)
2016-10-15 02:51:41.095354: step 26000, loss = 0.76 (725.1 examples/sec; 0.177 sec/batch)
2016-10-15 02:51:43.662532: step 26010, loss = 1.06 (728.5 examples/sec; 0.176 sec/batch)
2016-10-15 02:51:45.423977: step 26020, loss = 0.85 (727.1 examples/sec; 0.176 sec/batch)
2016-10-15 02:51:47.187225: step 26030, loss = 0.94 (722.5 examples/sec; 0.177 sec/batch)
2016-10-15 02:51:48.937499: step 26040, loss = 0.80 (736.6 examples/sec; 0.174 sec/batch)
2016-10-15 02:51:50.696980: step 26050, loss = 0.77 (722.8 examples/sec; 0.177 sec/batch)
2016-10-15 02:51:52.452918: step 26060, loss = 0.84 (719.6 examples/sec; 0.178 sec/batch)
2016-10-15 02:51:54.212627: step 26070, loss = 0.81 (719.4 examples/sec; 0.178 sec/batch)
2016-10-15 02:51:55.982139: step 26080, loss = 0.86 (763.0 examples/sec; 0.168 sec/batch)
2016-10-15 02:51:57.745881: step 26090, loss = 0.93 (720.5 examples/sec; 0.178 sec/batch)
2016-10-15 02:51:59.501733: step 26100, loss = 0.98 (715.5 examples/sec; 0.179 sec/batch)
2016-10-15 02:52:01.454353: step 26110, loss = 0.96 (733.0 examples/sec; 0.175 sec/batch)
2016-10-15 02:52:03.210245: step 26120, loss = 0.69 (712.7 examples/sec; 0.180 sec/batch)
2016-10-15 02:52:04.974006: step 26130, loss = 0.81 (734.5 examples/sec; 0.174 sec/batch)
2016-10-15 02:52:06.734879: step 26140, loss = 0.99 (735.0 examples/sec; 0.174 sec/batch)
2016-10-15 02:52:08.499683: step 26150, loss = 0.94 (712.3 examples/sec; 0.180 sec/batch)
2016-10-15 02:52:10.256392: step 26160, loss = 0.88 (741.1 examples/sec; 0.173 sec/batch)
2016-10-15 02:52:12.021054: step 26170, loss = 0.83 (709.5 examples/sec; 0.180 sec/batch)
2016-10-15 02:52:13.785358: step 26180, loss = 0.75 (728.5 examples/sec; 0.176 sec/batch)
2016-10-15 02:52:15.554129: step 26190, loss = 0.80 (713.9 examples/sec; 0.179 sec/batch)
2016-10-15 02:52:17.318542: step 26200, loss = 0.74 (747.9 examples/sec; 0.171 sec/batch)
2016-10-15 02:52:19.275035: step 26210, loss = 0.64 (682.1 examples/sec; 0.188 sec/batch)
2016-10-15 02:52:21.034644: step 26220, loss = 0.88 (719.2 examples/sec; 0.178 sec/batch)
2016-10-15 02:52:22.797327: step 26230, loss = 0.70 (732.9 examples/sec; 0.175 sec/batch)
2016-10-15 02:52:24.569353: step 26240, loss = 0.78 (684.0 examples/sec; 0.187 sec/batch)
2016-10-15 02:52:26.319144: step 26250, loss = 0.84 (751.9 examples/sec; 0.170 sec/batch)
2016-10-15 02:52:28.080194: step 26260, loss = 0.82 (737.1 examples/sec; 0.174 sec/batch)
2016-10-15 02:52:29.852456: step 26270, loss = 0.95 (738.1 examples/sec; 0.173 sec/batch)
2016-10-15 02:52:31.614858: step 26280, loss = 0.74 (711.7 examples/sec; 0.180 sec/batch)
2016-10-15 02:52:33.386067: step 26290, loss = 0.71 (712.0 examples/sec; 0.180 sec/batch)
2016-10-15 02:52:35.139294: step 26300, loss = 0.77 (722.5 examples/sec; 0.177 sec/batch)
2016-10-15 02:52:37.081717: step 26310, loss = 0.74 (740.0 examples/sec; 0.173 sec/batch)
2016-10-15 02:52:38.831489: step 26320, loss = 0.86 (735.6 examples/sec; 0.174 sec/batch)
2016-10-15 02:52:40.597561: step 26330, loss = 0.60 (726.2 examples/sec; 0.176 sec/batch)
2016-10-15 02:52:42.357554: step 26340, loss = 0.73 (732.3 examples/sec; 0.175 sec/batch)
2016-10-15 02:52:44.117793: step 26350, loss = 0.75 (712.6 examples/sec; 0.180 sec/batch)
2016-10-15 02:52:45.887302: step 26360, loss = 0.85 (717.7 examples/sec; 0.178 sec/batch)
2016-10-15 02:52:47.647536: step 26370, loss = 0.89 (720.5 examples/sec; 0.178 sec/batch)
2016-10-15 02:52:49.399917: step 26380, loss = 0.91 (720.2 examples/sec; 0.178 sec/batch)
2016-10-15 02:52:51.167508: step 26390, loss = 0.68 (704.1 examples/sec; 0.182 sec/batch)
2016-10-15 02:52:52.924806: step 26400, loss = 0.74 (711.2 examples/sec; 0.180 sec/batch)
2016-10-15 02:52:54.864074: step 26410, loss = 0.70 (744.9 examples/sec; 0.172 sec/batch)
2016-10-15 02:52:56.627399: step 26420, loss = 0.87 (717.2 examples/sec; 0.178 sec/batch)
2016-10-15 02:52:58.395851: step 26430, loss = 0.72 (735.5 examples/sec; 0.174 sec/batch)
2016-10-15 02:53:00.152120: step 26440, loss = 0.78 (739.6 examples/sec; 0.173 sec/batch)
2016-10-15 02:53:01.906737: step 26450, loss = 0.73 (728.1 examples/sec; 0.176 sec/batch)
2016-10-15 02:53:03.658605: step 26460, loss = 0.74 (732.0 examples/sec; 0.175 sec/batch)
2016-10-15 02:53:05.411123: step 26470, loss = 0.81 (747.0 examples/sec; 0.171 sec/batch)
2016-10-15 02:53:07.166409: step 26480, loss = 0.87 (719.9 examples/sec; 0.178 sec/batch)
2016-10-15 02:53:08.926384: step 26490, loss = 0.88 (719.4 examples/sec; 0.178 sec/batch)
2016-10-15 02:53:10.688591: step 26500, loss = 0.88 (737.7 examples/sec; 0.174 sec/batch)
2016-10-15 02:53:12.638082: step 26510, loss = 0.85 (703.0 examples/sec; 0.182 sec/batch)
2016-10-15 02:53:14.396982: step 26520, loss = 0.88 (734.8 examples/sec; 0.174 sec/batch)
2016-10-15 02:53:16.153240: step 26530, loss = 0.82 (735.7 examples/sec; 0.174 sec/batch)
2016-10-15 02:53:17.919963: step 26540, loss = 0.67 (752.6 examples/sec; 0.170 sec/batch)
2016-10-15 02:53:19.680834: step 26550, loss = 0.76 (731.6 examples/sec; 0.175 sec/batch)
2016-10-15 02:53:21.433668: step 26560, loss = 0.68 (737.5 examples/sec; 0.174 sec/batch)
2016-10-15 02:53:23.193082: step 26570, loss = 0.81 (735.5 examples/sec; 0.174 sec/batch)
2016-10-15 02:53:24.946465: step 26580, loss = 0.64 (714.3 examples/sec; 0.179 sec/batch)
2016-10-15 02:53:26.700551: step 26590, loss = 0.72 (728.7 examples/sec; 0.176 sec/batch)
2016-10-15 02:53:28.469412: step 26600, loss = 0.90 (707.0 examples/sec; 0.181 sec/batch)
2016-10-15 02:53:30.407934: step 26610, loss = 0.89 (709.4 examples/sec; 0.180 sec/batch)
2016-10-15 02:53:32.159278: step 26620, loss = 0.61 (710.8 examples/sec; 0.180 sec/batch)
2016-10-15 02:53:33.917750: step 26630, loss = 0.86 (734.1 examples/sec; 0.174 sec/batch)
2016-10-15 02:53:35.678447: step 26640, loss = 0.82 (752.6 examples/sec; 0.170 sec/batch)
2016-10-15 02:53:37.437454: step 26650, loss = 0.71 (738.2 examples/sec; 0.173 sec/batch)
2016-10-15 02:53:39.198134: step 26660, loss = 0.74 (710.6 examples/sec; 0.180 sec/batch)
2016-10-15 02:53:40.959784: step 26670, loss = 0.79 (724.9 examples/sec; 0.177 sec/batch)
2016-10-15 02:53:42.715265: step 26680, loss = 0.69 (741.8 examples/sec; 0.173 sec/batch)
2016-10-15 02:53:44.468152: step 26690, loss = 0.84 (738.1 examples/sec; 0.173 sec/batch)
2016-10-15 02:53:46.232521: step 26700, loss = 0.68 (735.6 examples/sec; 0.174 sec/batch)
2016-10-15 02:53:48.174283: step 26710, loss = 0.97 (741.2 examples/sec; 0.173 sec/batch)
2016-10-15 02:53:49.937341: step 26720, loss = 0.80 (736.4 examples/sec; 0.174 sec/batch)
2016-10-15 02:53:51.700461: step 26730, loss = 0.57 (696.1 examples/sec; 0.184 sec/batch)
2016-10-15 02:53:53.452141: step 26740, loss = 0.82 (707.4 examples/sec; 0.181 sec/batch)
2016-10-15 02:53:55.204921: step 26750, loss = 0.68 (731.1 examples/sec; 0.175 sec/batch)
2016-10-15 02:53:56.965384: step 26760, loss = 0.82 (731.5 examples/sec; 0.175 sec/batch)
2016-10-15 02:53:58.718721: step 26770, loss = 0.72 (738.5 examples/sec; 0.173 sec/batch)
2016-10-15 02:54:00.482717: step 26780, loss = 0.83 (724.6 examples/sec; 0.177 sec/batch)
2016-10-15 02:54:02.237005: step 26790, loss = 0.78 (736.6 examples/sec; 0.174 sec/batch)
2016-10-15 02:54:03.991790: step 26800, loss = 0.86 (741.7 examples/sec; 0.173 sec/batch)
2016-10-15 02:54:05.938756: step 26810, loss = 0.75 (750.7 examples/sec; 0.171 sec/batch)
2016-10-15 02:54:07.703062: step 26820, loss = 0.78 (730.6 examples/sec; 0.175 sec/batch)
2016-10-15 02:54:09.466442: step 26830, loss = 0.87 (721.8 examples/sec; 0.177 sec/batch)
2016-10-15 02:54:11.222410: step 26840, loss = 0.80 (714.4 examples/sec; 0.179 sec/batch)
2016-10-15 02:54:12.993111: step 26850, loss = 0.83 (723.9 examples/sec; 0.177 sec/batch)
2016-10-15 02:54:14.751494: step 26860, loss = 0.82 (732.0 examples/sec; 0.175 sec/batch)
2016-10-15 02:54:16.518603: step 26870, loss = 0.70 (730.1 examples/sec; 0.175 sec/batch)
2016-10-15 02:54:18.283350: step 26880, loss = 0.87 (741.8 examples/sec; 0.173 sec/batch)
2016-10-15 02:54:20.041910: step 26890, loss = 0.73 (730.1 examples/sec; 0.175 sec/batch)
2016-10-15 02:54:21.801961: step 26900, loss = 0.86 (733.9 examples/sec; 0.174 sec/batch)
2016-10-15 02:54:23.759628: step 26910, loss = 0.69 (728.6 examples/sec; 0.176 sec/batch)
2016-10-15 02:54:25.530610: step 26920, loss = 0.74 (716.9 examples/sec; 0.179 sec/batch)
2016-10-15 02:54:27.287782: step 26930, loss = 0.85 (703.2 examples/sec; 0.182 sec/batch)
2016-10-15 02:54:29.041437: step 26940, loss = 0.88 (726.8 examples/sec; 0.176 sec/batch)
2016-10-15 02:54:30.797595: step 26950, loss = 0.79 (744.5 examples/sec; 0.172 sec/batch)
2016-10-15 02:54:32.552396: step 26960, loss = 0.71 (728.0 examples/sec; 0.176 sec/batch)
2016-10-15 02:54:34.315533: step 26970, loss = 0.82 (716.7 examples/sec; 0.179 sec/batch)
2016-10-15 02:54:36.079469: step 26980, loss = 0.98 (699.0 examples/sec; 0.183 sec/batch)
2016-10-15 02:54:37.829941: step 26990, loss = 0.70 (741.2 examples/sec; 0.173 sec/batch)
2016-10-15 02:54:39.578278: step 27000, loss = 0.67 (741.0 examples/sec; 0.173 sec/batch)
2016-10-15 02:54:42.114658: step 27010, loss = 0.63 (712.9 examples/sec; 0.180 sec/batch)
2016-10-15 02:54:43.876376: step 27020, loss = 0.71 (739.0 examples/sec; 0.173 sec/batch)
2016-10-15 02:54:45.642869: step 27030, loss = 0.69 (731.3 examples/sec; 0.175 sec/batch)
2016-10-15 02:54:47.409054: step 27040, loss = 0.89 (709.0 examples/sec; 0.181 sec/batch)
2016-10-15 02:54:49.162991: step 27050, loss = 0.86 (731.1 examples/sec; 0.175 sec/batch)
2016-10-15 02:54:50.917884: step 27060, loss = 0.75 (743.3 examples/sec; 0.172 sec/batch)
2016-10-15 02:54:52.696034: step 27070, loss = 0.86 (677.6 examples/sec; 0.189 sec/batch)
2016-10-15 02:54:54.436466: step 27080, loss = 0.99 (758.0 examples/sec; 0.169 sec/batch)
2016-10-15 02:54:56.202167: step 27090, loss = 0.80 (710.0 examples/sec; 0.180 sec/batch)
2016-10-15 02:54:57.966201: step 27100, loss = 0.68 (742.3 examples/sec; 0.172 sec/batch)
2016-10-15 02:54:59.916931: step 27110, loss = 0.81 (765.6 examples/sec; 0.167 sec/batch)
2016-10-15 02:55:01.681695: step 27120, loss = 0.73 (738.3 examples/sec; 0.173 sec/batch)
2016-10-15 02:55:03.458639: step 27130, loss = 0.64 (738.5 examples/sec; 0.173 sec/batch)
2016-10-15 02:55:05.223958: step 27140, loss = 0.79 (720.4 examples/sec; 0.178 sec/batch)
2016-10-15 02:55:06.982393: step 27150, loss = 0.79 (720.8 examples/sec; 0.178 sec/batch)
2016-10-15 02:55:08.746911: step 27160, loss = 0.86 (743.8 examples/sec; 0.172 sec/batch)
2016-10-15 02:55:10.501435: step 27170, loss = 0.78 (756.6 examples/sec; 0.169 sec/batch)
2016-10-15 02:55:12.267158: step 27180, loss = 0.93 (734.8 examples/sec; 0.174 sec/batch)
2016-10-15 02:55:14.043406: step 27190, loss = 0.74 (686.7 examples/sec; 0.186 sec/batch)
2016-10-15 02:55:15.791035: step 27200, loss = 0.67 (738.7 examples/sec; 0.173 sec/batch)
2016-10-15 02:55:17.741735: step 27210, loss = 0.75 (734.7 examples/sec; 0.174 sec/batch)
2016-10-15 02:55:19.505703: step 27220, loss = 0.75 (693.4 examples/sec; 0.185 sec/batch)
2016-10-15 02:55:21.259323: step 27230, loss = 0.63 (730.9 examples/sec; 0.175 sec/batch)
2016-10-15 02:55:23.021043: step 27240, loss = 0.73 (698.5 examples/sec; 0.183 sec/batch)
2016-10-15 02:55:24.771397: step 27250, loss = 0.82 (727.2 examples/sec; 0.176 sec/batch)
2016-10-15 02:55:26.530422: step 27260, loss = 0.80 (724.7 examples/sec; 0.177 sec/batch)
2016-10-15 02:55:28.302937: step 27270, loss = 0.71 (726.8 examples/sec; 0.176 sec/batch)
2016-10-15 02:55:30.057242: step 27280, loss = 0.83 (731.3 examples/sec; 0.175 sec/batch)
2016-10-15 02:55:31.815561: step 27290, loss = 0.77 (710.7 examples/sec; 0.180 sec/batch)
2016-10-15 02:55:33.578148: step 27300, loss = 0.89 (724.9 examples/sec; 0.177 sec/batch)
2016-10-15 02:55:35.529884: step 27310, loss = 0.84 (708.8 examples/sec; 0.181 sec/batch)
2016-10-15 02:55:37.285558: step 27320, loss = 0.83 (741.3 examples/sec; 0.173 sec/batch)
2016-10-15 02:55:39.035029: step 27330, loss = 0.73 (727.0 examples/sec; 0.176 sec/batch)
2016-10-15 02:55:40.801390: step 27340, loss = 0.79 (724.6 examples/sec; 0.177 sec/batch)
2016-10-15 02:55:42.563754: step 27350, loss = 0.89 (729.0 examples/sec; 0.176 sec/batch)
2016-10-15 02:55:44.320866: step 27360, loss = 0.84 (714.7 examples/sec; 0.179 sec/batch)
2016-10-15 02:55:46.082713: step 27370, loss = 0.91 (718.2 examples/sec; 0.178 sec/batch)
2016-10-15 02:55:47.851857: step 27380, loss = 0.68 (715.0 examples/sec; 0.179 sec/batch)
2016-10-15 02:55:49.610958: step 27390, loss = 0.88 (718.8 examples/sec; 0.178 sec/batch)
2016-10-15 02:55:51.365389: step 27400, loss = 0.78 (715.4 examples/sec; 0.179 sec/batch)
2016-10-15 02:55:53.324064: step 27410, loss = 0.77 (728.0 examples/sec; 0.176 sec/batch)
2016-10-15 02:55:55.081190: step 27420, loss = 0.90 (764.1 examples/sec; 0.168 sec/batch)
2016-10-15 02:55:56.836087: step 27430, loss = 0.86 (726.6 examples/sec; 0.176 sec/batch)
2016-10-15 02:55:58.604427: step 27440, loss = 0.73 (732.3 examples/sec; 0.175 sec/batch)
2016-10-15 02:56:00.368975: step 27450, loss = 0.65 (721.6 examples/sec; 0.177 sec/batch)
2016-10-15 02:56:02.142206: step 27460, loss = 0.88 (721.7 examples/sec; 0.177 sec/batch)
2016-10-15 02:56:03.900314: step 27470, loss = 0.96 (727.8 examples/sec; 0.176 sec/batch)
2016-10-15 02:56:05.667364: step 27480, loss = 0.75 (734.9 examples/sec; 0.174 sec/batch)
2016-10-15 02:56:07.432814: step 27490, loss = 0.76 (735.4 examples/sec; 0.174 sec/batch)
2016-10-15 02:56:09.190125: step 27500, loss = 0.98 (737.0 examples/sec; 0.174 sec/batch)
2016-10-15 02:56:11.144536: step 27510, loss = 0.70 (701.4 examples/sec; 0.182 sec/batch)
2016-10-15 02:56:12.903600: step 27520, loss = 0.71 (720.2 examples/sec; 0.178 sec/batch)
2016-10-15 02:56:14.662608: step 27530, loss = 0.80 (739.1 examples/sec; 0.173 sec/batch)
2016-10-15 02:56:16.434051: step 27540, loss = 0.76 (738.1 examples/sec; 0.173 sec/batch)
2016-10-15 02:56:18.192762: step 27550, loss = 0.89 (732.2 examples/sec; 0.175 sec/batch)
2016-10-15 02:56:19.952600: step 27560, loss = 0.81 (732.8 examples/sec; 0.175 sec/batch)
2016-10-15 02:56:21.711292: step 27570, loss = 0.77 (704.5 examples/sec; 0.182 sec/batch)
2016-10-15 02:56:23.463003: step 27580, loss = 0.87 (731.1 examples/sec; 0.175 sec/batch)
2016-10-15 02:56:25.215733: step 27590, loss = 0.96 (751.4 examples/sec; 0.170 sec/batch)
2016-10-15 02:56:26.969987: step 27600, loss = 0.90 (731.2 examples/sec; 0.175 sec/batch)
2016-10-15 02:56:28.916935: step 27610, loss = 0.82 (750.5 examples/sec; 0.171 sec/batch)
2016-10-15 02:56:30.679563: step 27620, loss = 1.03 (715.7 examples/sec; 0.179 sec/batch)
2016-10-15 02:56:32.436127: step 27630, loss = 0.84 (737.9 examples/sec; 0.173 sec/batch)
2016-10-15 02:56:34.203604: step 27640, loss = 0.92 (732.2 examples/sec; 0.175 sec/batch)
2016-10-15 02:56:35.958759: step 27650, loss = 0.76 (719.9 examples/sec; 0.178 sec/batch)
2016-10-15 02:56:37.721029: step 27660, loss = 0.68 (733.4 examples/sec; 0.175 sec/batch)
2016-10-15 02:56:39.466993: step 27670, loss = 0.77 (736.6 examples/sec; 0.174 sec/batch)
2016-10-15 02:56:41.223611: step 27680, loss = 0.93 (731.8 examples/sec; 0.175 sec/batch)
2016-10-15 02:56:42.976202: step 27690, loss = 0.87 (725.2 examples/sec; 0.177 sec/batch)
2016-10-15 02:56:44.735170: step 27700, loss = 0.58 (739.8 examples/sec; 0.173 sec/batch)
2016-10-15 02:56:46.681566: step 27710, loss = 0.68 (691.8 examples/sec; 0.185 sec/batch)
2016-10-15 02:56:48.438601: step 27720, loss = 0.80 (737.2 examples/sec; 0.174 sec/batch)
2016-10-15 02:56:50.203688: step 27730, loss = 0.73 (724.0 examples/sec; 0.177 sec/batch)
2016-10-15 02:56:51.956288: step 27740, loss = 0.85 (743.1 examples/sec; 0.172 sec/batch)
2016-10-15 02:56:53.719000: step 27750, loss = 0.78 (746.3 examples/sec; 0.172 sec/batch)
2016-10-15 02:56:55.475160: step 27760, loss = 0.66 (747.2 examples/sec; 0.171 sec/batch)
2016-10-15 02:56:57.235164: step 27770, loss = 0.77 (728.5 examples/sec; 0.176 sec/batch)
2016-10-15 02:56:58.991737: step 27780, loss = 0.87 (727.6 examples/sec; 0.176 sec/batch)
2016-10-15 02:57:00.753194: step 27790, loss = 0.94 (731.8 examples/sec; 0.175 sec/batch)
2016-10-15 02:57:02.515515: step 27800, loss = 0.93 (726.2 examples/sec; 0.176 sec/batch)
2016-10-15 02:57:04.457880: step 27810, loss = 0.78 (747.5 examples/sec; 0.171 sec/batch)
2016-10-15 02:57:06.216987: step 27820, loss = 0.75 (757.7 examples/sec; 0.169 sec/batch)
2016-10-15 02:57:07.976039: step 27830, loss = 0.83 (723.7 examples/sec; 0.177 sec/batch)
2016-10-15 02:57:09.734165: step 27840, loss = 0.77 (744.6 examples/sec; 0.172 sec/batch)
2016-10-15 02:57:11.497269: step 27850, loss = 0.82 (730.4 examples/sec; 0.175 sec/batch)
2016-10-15 02:57:13.258234: step 27860, loss = 0.59 (725.5 examples/sec; 0.176 sec/batch)
2016-10-15 02:57:15.019972: step 27870, loss = 0.87 (758.5 examples/sec; 0.169 sec/batch)
2016-10-15 02:57:16.787911: step 27880, loss = 0.98 (723.7 examples/sec; 0.177 sec/batch)
2016-10-15 02:57:18.554157: step 27890, loss = 0.76 (708.5 examples/sec; 0.181 sec/batch)
2016-10-15 02:57:20.310162: step 27900, loss = 0.75 (732.0 examples/sec; 0.175 sec/batch)
2016-10-15 02:57:22.256163: step 27910, loss = 0.75 (732.2 examples/sec; 0.175 sec/batch)
2016-10-15 02:57:24.014889: step 27920, loss = 0.85 (716.9 examples/sec; 0.179 sec/batch)
2016-10-15 02:57:25.777037: step 27930, loss = 0.77 (739.9 examples/sec; 0.173 sec/batch)
2016-10-15 02:57:27.545835: step 27940, loss = 0.80 (713.8 examples/sec; 0.179 sec/batch)
2016-10-15 02:57:29.302403: step 27950, loss = 0.79 (744.7 examples/sec; 0.172 sec/batch)
2016-10-15 02:57:31.063229: step 27960, loss = 0.69 (725.4 examples/sec; 0.176 sec/batch)
2016-10-15 02:57:32.827451: step 27970, loss = 0.78 (717.7 examples/sec; 0.178 sec/batch)
2016-10-15 02:57:34.593957: step 27980, loss = 0.89 (726.8 examples/sec; 0.176 sec/batch)
2016-10-15 02:57:36.341259: step 27990, loss = 0.85 (742.5 examples/sec; 0.172 sec/batch)
2016-10-15 02:57:38.096042: step 28000, loss = 0.84 (737.5 examples/sec; 0.174 sec/batch)
2016-10-15 02:57:40.603012: step 28010, loss = 0.81 (693.8 examples/sec; 0.185 sec/batch)
2016-10-15 02:57:42.357856: step 28020, loss = 0.92 (728.6 examples/sec; 0.176 sec/batch)
2016-10-15 02:57:44.125450: step 28030, loss = 0.73 (695.6 examples/sec; 0.184 sec/batch)
2016-10-15 02:57:45.877837: step 28040, loss = 0.79 (786.7 examples/sec; 0.163 sec/batch)
2016-10-15 02:57:47.648546: step 28050, loss = 0.83 (707.4 examples/sec; 0.181 sec/batch)
2016-10-15 02:57:49.404051: step 28060, loss = 0.84 (776.6 examples/sec; 0.165 sec/batch)
2016-10-15 02:57:51.167690: step 28070, loss = 0.79 (745.8 examples/sec; 0.172 sec/batch)
2016-10-15 02:57:52.934103: step 28080, loss = 0.95 (737.4 examples/sec; 0.174 sec/batch)
2016-10-15 02:57:54.703610: step 28090, loss = 0.73 (720.0 examples/sec; 0.178 sec/batch)
2016-10-15 02:57:56.462903: step 28100, loss = 0.67 (728.0 examples/sec; 0.176 sec/batch)
2016-10-15 02:57:58.410661: step 28110, loss = 0.67 (743.3 examples/sec; 0.172 sec/batch)
2016-10-15 02:58:00.167234: step 28120, loss = 0.69 (699.5 examples/sec; 0.183 sec/batch)
2016-10-15 02:58:01.926608: step 28130, loss = 0.87 (734.1 examples/sec; 0.174 sec/batch)
2016-10-15 02:58:03.686368: step 28140, loss = 0.72 (699.2 examples/sec; 0.183 sec/batch)
2016-10-15 02:58:05.435072: step 28150, loss = 0.59 (743.1 examples/sec; 0.172 sec/batch)
2016-10-15 02:58:07.193646: step 28160, loss = 0.91 (725.6 examples/sec; 0.176 sec/batch)
2016-10-15 02:58:08.947182: step 28170, loss = 0.69 (721.4 examples/sec; 0.177 sec/batch)
2016-10-15 02:58:10.702656: step 28180, loss = 0.95 (733.9 examples/sec; 0.174 sec/batch)
2016-10-15 02:58:12.466241: step 28190, loss = 0.78 (737.8 examples/sec; 0.173 sec/batch)
2016-10-15 02:58:14.228157: step 28200, loss = 0.84 (707.0 examples/sec; 0.181 sec/batch)
2016-10-15 02:58:16.175613: step 28210, loss = 0.83 (718.3 examples/sec; 0.178 sec/batch)
2016-10-15 02:58:17.934418: step 28220, loss = 0.64 (718.5 examples/sec; 0.178 sec/batch)
2016-10-15 02:58:19.698006: step 28230, loss = 0.78 (731.0 examples/sec; 0.175 sec/batch)
2016-10-15 02:58:21.462261: step 28240, loss = 0.68 (740.2 examples/sec; 0.173 sec/batch)
2016-10-15 02:58:23.218881: step 28250, loss = 0.76 (731.9 examples/sec; 0.175 sec/batch)
2016-10-15 02:58:24.999950: step 28260, loss = 0.71 (720.3 examples/sec; 0.178 sec/batch)
2016-10-15 02:58:26.766754: step 28270, loss = 0.75 (697.7 examples/sec; 0.183 sec/batch)
2016-10-15 02:58:28.521864: step 28280, loss = 0.78 (740.6 examples/sec; 0.173 sec/batch)
2016-10-15 02:58:30.289663: step 28290, loss = 0.70 (736.4 examples/sec; 0.174 sec/batch)
2016-10-15 02:58:32.049829: step 28300, loss = 0.75 (709.2 examples/sec; 0.180 sec/batch)
2016-10-15 02:58:33.990120: step 28310, loss = 0.76 (698.1 examples/sec; 0.183 sec/batch)
2016-10-15 02:58:35.748740: step 28320, loss = 0.67 (728.4 examples/sec; 0.176 sec/batch)
2016-10-15 02:58:37.514616: step 28330, loss = 0.87 (722.2 examples/sec; 0.177 sec/batch)
2016-10-15 02:58:39.267489: step 28340, loss = 0.86 (730.8 examples/sec; 0.175 sec/batch)
2016-10-15 02:58:41.032728: step 28350, loss = 0.90 (775.8 examples/sec; 0.165 sec/batch)
2016-10-15 02:58:42.799522: step 28360, loss = 0.80 (757.5 examples/sec; 0.169 sec/batch)
2016-10-15 02:58:44.570580: step 28370, loss = 0.86 (717.7 examples/sec; 0.178 sec/batch)
2016-10-15 02:58:46.318761: step 28380, loss = 0.84 (744.7 examples/sec; 0.172 sec/batch)
2016-10-15 02:58:48.081007: step 28390, loss = 0.69 (709.1 examples/sec; 0.181 sec/batch)
2016-10-15 02:58:49.833619: step 28400, loss = 0.79 (723.5 examples/sec; 0.177 sec/batch)
2016-10-15 02:58:51.777054: step 28410, loss = 0.76 (722.0 examples/sec; 0.177 sec/batch)
2016-10-15 02:58:53.536619: step 28420, loss = 0.79 (720.4 examples/sec; 0.178 sec/batch)
2016-10-15 02:58:55.292787: step 28430, loss = 0.76 (725.0 examples/sec; 0.177 sec/batch)
2016-10-15 02:58:57.050288: step 28440, loss = 0.81 (719.5 examples/sec; 0.178 sec/batch)
2016-10-15 02:58:58.807376: step 28450, loss = 0.91 (719.3 examples/sec; 0.178 sec/batch)
2016-10-15 02:59:00.570955: step 28460, loss = 0.78 (745.8 examples/sec; 0.172 sec/batch)
2016-10-15 02:59:02.334915: step 28470, loss = 0.73 (728.8 examples/sec; 0.176 sec/batch)
2016-10-15 02:59:04.095590: step 28480, loss = 0.89 (725.6 examples/sec; 0.176 sec/batch)
2016-10-15 02:59:05.853097: step 28490, loss = 0.96 (723.1 examples/sec; 0.177 sec/batch)
2016-10-15 02:59:07.614585: step 28500, loss = 0.88 (719.0 examples/sec; 0.178 sec/batch)
2016-10-15 02:59:09.554236: step 28510, loss = 0.81 (715.1 examples/sec; 0.179 sec/batch)
2016-10-15 02:59:11.307941: step 28520, loss = 0.84 (739.9 examples/sec; 0.173 sec/batch)
2016-10-15 02:59:13.077588: step 28530, loss = 0.65 (716.1 examples/sec; 0.179 sec/batch)
2016-10-15 02:59:14.837878: step 28540, loss = 0.64 (722.8 examples/sec; 0.177 sec/batch)
2016-10-15 02:59:16.618666: step 28550, loss = 0.87 (708.2 examples/sec; 0.181 sec/batch)
2016-10-15 02:59:18.378364: step 28560, loss = 0.80 (749.5 examples/sec; 0.171 sec/batch)
2016-10-15 02:59:20.149346: step 28570, loss = 0.65 (715.1 examples/sec; 0.179 sec/batch)
2016-10-15 02:59:21.903101: step 28580, loss = 0.86 (745.2 examples/sec; 0.172 sec/batch)
2016-10-15 02:59:23.667486: step 28590, loss = 0.73 (721.1 examples/sec; 0.178 sec/batch)
2016-10-15 02:59:25.435335: step 28600, loss = 0.74 (715.7 examples/sec; 0.179 sec/batch)
2016-10-15 02:59:27.390583: step 28610, loss = 0.68 (728.8 examples/sec; 0.176 sec/batch)
2016-10-15 02:59:29.147679: step 28620, loss = 0.77 (719.1 examples/sec; 0.178 sec/batch)
2016-10-15 02:59:30.906935: step 28630, loss = 0.80 (707.5 examples/sec; 0.181 sec/batch)
2016-10-15 02:59:32.665703: step 28640, loss = 0.76 (733.8 examples/sec; 0.174 sec/batch)
2016-10-15 02:59:34.435074: step 28650, loss = 0.58 (725.5 examples/sec; 0.176 sec/batch)
2016-10-15 02:59:36.194236: step 28660, loss = 0.75 (724.1 examples/sec; 0.177 sec/batch)
2016-10-15 02:59:37.959688: step 28670, loss = 0.80 (699.1 examples/sec; 0.183 sec/batch)
2016-10-15 02:59:39.717026: step 28680, loss = 0.76 (712.3 examples/sec; 0.180 sec/batch)
2016-10-15 02:59:41.472162: step 28690, loss = 0.84 (724.5 examples/sec; 0.177 sec/batch)
2016-10-15 02:59:43.240009: step 28700, loss = 0.83 (725.5 examples/sec; 0.176 sec/batch)
2016-10-15 02:59:45.182072: step 28710, loss = 0.81 (728.4 examples/sec; 0.176 sec/batch)
2016-10-15 02:59:46.940174: step 28720, loss = 0.74 (753.6 examples/sec; 0.170 sec/batch)
2016-10-15 02:59:48.689574: step 28730, loss = 0.73 (737.4 examples/sec; 0.174 sec/batch)
2016-10-15 02:59:50.458749: step 28740, loss = 0.75 (718.1 examples/sec; 0.178 sec/batch)
2016-10-15 02:59:52.220475: step 28750, loss = 0.74 (728.9 examples/sec; 0.176 sec/batch)
2016-10-15 02:59:53.988394: step 28760, loss = 0.68 (716.9 examples/sec; 0.179 sec/batch)
2016-10-15 02:59:55.750261: step 28770, loss = 0.69 (703.6 examples/sec; 0.182 sec/batch)
2016-10-15 02:59:57.496879: step 28780, loss = 0.86 (736.3 examples/sec; 0.174 sec/batch)
2016-10-15 02:59:59.252331: step 28790, loss = 0.68 (735.4 examples/sec; 0.174 sec/batch)
2016-10-15 03:00:01.011319: step 28800, loss = 0.71 (716.3 examples/sec; 0.179 sec/batch)
2016-10-15 03:00:02.983407: step 28810, loss = 0.64 (741.1 examples/sec; 0.173 sec/batch)
2016-10-15 03:00:04.742831: step 28820, loss = 0.90 (712.3 examples/sec; 0.180 sec/batch)
2016-10-15 03:00:06.489494: step 28830, loss = 0.77 (734.7 examples/sec; 0.174 sec/batch)
2016-10-15 03:00:08.230707: step 28840, loss = 0.83 (742.2 examples/sec; 0.172 sec/batch)
2016-10-15 03:00:09.978945: step 28850, loss = 0.93 (741.8 examples/sec; 0.173 sec/batch)
2016-10-15 03:00:11.743073: step 28860, loss = 0.73 (710.6 examples/sec; 0.180 sec/batch)
2016-10-15 03:00:13.499599: step 28870, loss = 0.84 (742.2 examples/sec; 0.172 sec/batch)
2016-10-15 03:00:15.257995: step 28880, loss = 0.75 (728.5 examples/sec; 0.176 sec/batch)
2016-10-15 03:00:17.016725: step 28890, loss = 0.80 (723.7 examples/sec; 0.177 sec/batch)
2016-10-15 03:00:18.767120: step 28900, loss = 0.77 (729.1 examples/sec; 0.176 sec/batch)
2016-10-15 03:00:20.710300: step 28910, loss = 0.90 (721.7 examples/sec; 0.177 sec/batch)
2016-10-15 03:00:22.470194: step 28920, loss = 0.84 (709.2 examples/sec; 0.180 sec/batch)
2016-10-15 03:00:24.230394: step 28930, loss = 0.81 (710.6 examples/sec; 0.180 sec/batch)
2016-10-15 03:00:25.987500: step 28940, loss = 0.85 (725.5 examples/sec; 0.176 sec/batch)
2016-10-15 03:00:27.741488: step 28950, loss = 0.79 (751.5 examples/sec; 0.170 sec/batch)
2016-10-15 03:00:29.508187: step 28960, loss = 0.87 (727.0 examples/sec; 0.176 sec/batch)
2016-10-15 03:00:31.253943: step 28970, loss = 0.90 (720.2 examples/sec; 0.178 sec/batch)
2016-10-15 03:00:33.012809: step 28980, loss = 0.89 (735.0 examples/sec; 0.174 sec/batch)
2016-10-15 03:00:34.768130: step 28990, loss = 0.93 (731.6 examples/sec; 0.175 sec/batch)
2016-10-15 03:00:36.519972: step 29000, loss = 0.60 (711.2 examples/sec; 0.180 sec/batch)
2016-10-15 03:00:39.072224: step 29010, loss = 0.74 (725.3 examples/sec; 0.176 sec/batch)
2016-10-15 03:00:40.834394: step 29020, loss = 0.71 (721.1 examples/sec; 0.178 sec/batch)
2016-10-15 03:00:42.592483: step 29030, loss = 0.81 (723.8 examples/sec; 0.177 sec/batch)
2016-10-15 03:00:44.364312: step 29040, loss = 0.69 (721.7 examples/sec; 0.177 sec/batch)
2016-10-15 03:00:46.106452: step 29050, loss = 0.66 (765.6 examples/sec; 0.167 sec/batch)
2016-10-15 03:00:47.856508: step 29060, loss = 0.89 (728.5 examples/sec; 0.176 sec/batch)
2016-10-15 03:00:49.601416: step 29070, loss = 0.90 (735.9 examples/sec; 0.174 sec/batch)
2016-10-15 03:00:51.348645: step 29080, loss = 0.77 (718.4 examples/sec; 0.178 sec/batch)
2016-10-15 03:00:53.111817: step 29090, loss = 0.64 (729.7 examples/sec; 0.175 sec/batch)
2016-10-15 03:00:54.862382: step 29100, loss = 0.85 (726.8 examples/sec; 0.176 sec/batch)
2016-10-15 03:00:56.801216: step 29110, loss = 0.79 (741.7 examples/sec; 0.173 sec/batch)
2016-10-15 03:00:58.557623: step 29120, loss = 0.75 (740.7 examples/sec; 0.173 sec/batch)
2016-10-15 03:01:00.310172: step 29130, loss = 0.84 (712.4 examples/sec; 0.180 sec/batch)
2016-10-15 03:01:02.075601: step 29140, loss = 0.86 (739.4 examples/sec; 0.173 sec/batch)
2016-10-15 03:01:03.826278: step 29150, loss = 0.76 (733.2 examples/sec; 0.175 sec/batch)
2016-10-15 03:01:05.573288: step 29160, loss = 0.69 (733.9 examples/sec; 0.174 sec/batch)
2016-10-15 03:01:07.336748: step 29170, loss = 0.71 (705.7 examples/sec; 0.181 sec/batch)
2016-10-15 03:01:09.096357: step 29180, loss = 0.77 (726.1 examples/sec; 0.176 sec/batch)
2016-10-15 03:01:10.851407: step 29190, loss = 0.78 (728.7 examples/sec; 0.176 sec/batch)
2016-10-15 03:01:12.613486: step 29200, loss = 0.65 (723.2 examples/sec; 0.177 sec/batch)
2016-10-15 03:01:14.576895: step 29210, loss = 0.80 (738.8 examples/sec; 0.173 sec/batch)
2016-10-15 03:01:16.338320: step 29220, loss = 0.80 (742.9 examples/sec; 0.172 sec/batch)
2016-10-15 03:01:18.105827: step 29230, loss = 0.73 (729.7 examples/sec; 0.175 sec/batch)
2016-10-15 03:01:19.865746: step 29240, loss = 0.86 (734.0 examples/sec; 0.174 sec/batch)
2016-10-15 03:01:21.621115: step 29250, loss = 0.70 (746.1 examples/sec; 0.172 sec/batch)
2016-10-15 03:01:23.382154: step 29260, loss = 0.77 (720.8 examples/sec; 0.178 sec/batch)
2016-10-15 03:01:25.145978: step 29270, loss = 0.97 (718.7 examples/sec; 0.178 sec/batch)
2016-10-15 03:01:26.908429: step 29280, loss = 0.78 (720.7 examples/sec; 0.178 sec/batch)
2016-10-15 03:01:28.667311: step 29290, loss = 0.85 (715.2 examples/sec; 0.179 sec/batch)
2016-10-15 03:01:30.438108: step 29300, loss = 0.85 (744.2 examples/sec; 0.172 sec/batch)
2016-10-15 03:01:32.383564: step 29310, loss = 0.80 (708.5 examples/sec; 0.181 sec/batch)
2016-10-15 03:01:34.148887: step 29320, loss = 0.70 (727.8 examples/sec; 0.176 sec/batch)
2016-10-15 03:01:35.907475: step 29330, loss = 0.85 (735.1 examples/sec; 0.174 sec/batch)
2016-10-15 03:01:37.677469: step 29340, loss = 0.84 (734.3 examples/sec; 0.174 sec/batch)
2016-10-15 03:01:39.439241: step 29350, loss = 0.79 (713.4 examples/sec; 0.179 sec/batch)
2016-10-15 03:01:41.200901: step 29360, loss = 0.75 (714.2 examples/sec; 0.179 sec/batch)
2016-10-15 03:01:42.959765: step 29370, loss = 0.71 (738.3 examples/sec; 0.173 sec/batch)
2016-10-15 03:01:44.727660: step 29380, loss = 0.73 (710.4 examples/sec; 0.180 sec/batch)
2016-10-15 03:01:46.489837: step 29390, loss = 0.84 (693.5 examples/sec; 0.185 sec/batch)
2016-10-15 03:01:48.248899: step 29400, loss = 0.70 (731.7 examples/sec; 0.175 sec/batch)
2016-10-15 03:01:50.194565: step 29410, loss = 0.72 (718.7 examples/sec; 0.178 sec/batch)
2016-10-15 03:01:51.949548: step 29420, loss = 0.68 (739.7 examples/sec; 0.173 sec/batch)
2016-10-15 03:01:53.711902: step 29430, loss = 0.95 (721.6 examples/sec; 0.177 sec/batch)
2016-10-15 03:01:55.482052: step 29440, loss = 0.84 (740.2 examples/sec; 0.173 sec/batch)
2016-10-15 03:01:57.246213: step 29450, loss = 0.72 (734.2 examples/sec; 0.174 sec/batch)
2016-10-15 03:01:59.006315: step 29460, loss = 0.76 (730.9 examples/sec; 0.175 sec/batch)
2016-10-15 03:02:00.766131: step 29470, loss = 0.93 (728.8 examples/sec; 0.176 sec/batch)
2016-10-15 03:02:02.540023: step 29480, loss = 0.92 (705.8 examples/sec; 0.181 sec/batch)
2016-10-15 03:02:04.298257: step 29490, loss = 0.77 (729.8 examples/sec; 0.175 sec/batch)
2016-10-15 03:02:06.058331: step 29500, loss = 0.62 (734.8 examples/sec; 0.174 sec/batch)
2016-10-15 03:02:08.010144: step 29510, loss = 0.64 (701.7 examples/sec; 0.182 sec/batch)
2016-10-15 03:02:09.768133: step 29520, loss = 0.77 (753.6 examples/sec; 0.170 sec/batch)
2016-10-15 03:02:11.526293: step 29530, loss = 0.86 (725.6 examples/sec; 0.176 sec/batch)
2016-10-15 03:02:13.295117: step 29540, loss = 0.78 (728.1 examples/sec; 0.176 sec/batch)
2016-10-15 03:02:15.060348: step 29550, loss = 0.66 (733.3 examples/sec; 0.175 sec/batch)
2016-10-15 03:02:16.842356: step 29560, loss = 0.80 (716.0 examples/sec; 0.179 sec/batch)
2016-10-15 03:02:18.597301: step 29570, loss = 0.81 (737.4 examples/sec; 0.174 sec/batch)
2016-10-15 03:02:20.370882: step 29580, loss = 0.78 (718.6 examples/sec; 0.178 sec/batch)
2016-10-15 03:02:22.130629: step 29590, loss = 0.82 (722.2 examples/sec; 0.177 sec/batch)
2016-10-15 03:02:23.886999: step 29600, loss = 0.90 (725.2 examples/sec; 0.176 sec/batch)
2016-10-15 03:02:25.818316: step 29610, loss = 0.81 (761.0 examples/sec; 0.168 sec/batch)
2016-10-15 03:02:27.578202: step 29620, loss = 0.70 (724.4 examples/sec; 0.177 sec/batch)
2016-10-15 03:02:29.344323: step 29630, loss = 0.79 (731.0 examples/sec; 0.175 sec/batch)
2016-10-15 03:02:31.107676: step 29640, loss = 0.83 (713.3 examples/sec; 0.179 sec/batch)
2016-10-15 03:02:32.874327: step 29650, loss = 0.90 (737.7 examples/sec; 0.174 sec/batch)
2016-10-15 03:02:34.641187: step 29660, loss = 0.67 (744.1 examples/sec; 0.172 sec/batch)
2016-10-15 03:02:36.399523: step 29670, loss = 0.80 (725.9 examples/sec; 0.176 sec/batch)
2016-10-15 03:02:38.163325: step 29680, loss = 0.87 (720.8 examples/sec; 0.178 sec/batch)
2016-10-15 03:02:39.926311: step 29690, loss = 0.70 (727.1 examples/sec; 0.176 sec/batch)
2016-10-15 03:02:41.673673: step 29700, loss = 0.72 (739.3 examples/sec; 0.173 sec/batch)
2016-10-15 03:02:43.620316: step 29710, loss = 0.57 (718.5 examples/sec; 0.178 sec/batch)
2016-10-15 03:02:45.378068: step 29720, loss = 0.75 (735.0 examples/sec; 0.174 sec/batch)
2016-10-15 03:02:47.135794: step 29730, loss = 0.83 (709.6 examples/sec; 0.180 sec/batch)
2016-10-15 03:02:48.892850: step 29740, loss = 0.93 (725.1 examples/sec; 0.177 sec/batch)
2016-10-15 03:02:50.650864: step 29750, loss = 0.91 (719.8 examples/sec; 0.178 sec/batch)
2016-10-15 03:02:52.410492: step 29760, loss = 0.72 (707.9 examples/sec; 0.181 sec/batch)
2016-10-15 03:02:54.169322: step 29770, loss = 0.63 (716.3 examples/sec; 0.179 sec/batch)
2016-10-15 03:02:55.929395: step 29780, loss = 0.68 (715.7 examples/sec; 0.179 sec/batch)
2016-10-15 03:02:57.697659: step 29790, loss = 0.70 (698.7 examples/sec; 0.183 sec/batch)
2016-10-15 03:02:59.454427: step 29800, loss = 0.73 (715.3 examples/sec; 0.179 sec/batch)
2016-10-15 03:03:01.398344: step 29810, loss = 0.83 (724.6 examples/sec; 0.177 sec/batch)
2016-10-15 03:03:03.157079: step 29820, loss = 0.88 (720.4 examples/sec; 0.178 sec/batch)
2016-10-15 03:03:04.912668: step 29830, loss = 0.88 (747.0 examples/sec; 0.171 sec/batch)
2016-10-15 03:03:06.676582: step 29840, loss = 0.96 (748.8 examples/sec; 0.171 sec/batch)
2016-10-15 03:03:08.433267: step 29850, loss = 0.80 (739.1 examples/sec; 0.173 sec/batch)
2016-10-15 03:03:10.192549: step 29860, loss = 0.66 (716.2 examples/sec; 0.179 sec/batch)
2016-10-15 03:03:11.948773: step 29870, loss = 0.66 (717.0 examples/sec; 0.179 sec/batch)
2016-10-15 03:03:13.706791: step 29880, loss = 0.81 (735.9 examples/sec; 0.174 sec/batch)
2016-10-15 03:03:15.471981: step 29890, loss = 0.84 (697.7 examples/sec; 0.183 sec/batch)
2016-10-15 03:03:17.232661: step 29900, loss = 0.92 (723.9 examples/sec; 0.177 sec/batch)
2016-10-15 03:03:19.175760: step 29910, loss = 0.75 (746.3 examples/sec; 0.172 sec/batch)
2016-10-15 03:03:20.939668: step 29920, loss = 0.86 (747.0 examples/sec; 0.171 sec/batch)
2016-10-15 03:03:22.686681: step 29930, loss = 0.72 (733.2 examples/sec; 0.175 sec/batch)
2016-10-15 03:03:24.446098: step 29940, loss = 0.78 (738.7 examples/sec; 0.173 sec/batch)
2016-10-15 03:03:26.198240: step 29950, loss = 0.78 (730.4 examples/sec; 0.175 sec/batch)
2016-10-15 03:03:27.955445: step 29960, loss = 0.68 (725.5 examples/sec; 0.176 sec/batch)
2016-10-15 03:03:29.711562: step 29970, loss = 0.73 (733.4 examples/sec; 0.175 sec/batch)
2016-10-15 03:03:31.471625: step 29980, loss = 0.84 (731.5 examples/sec; 0.175 sec/batch)
2016-10-15 03:03:33.227488: step 29990, loss = 0.80 (757.8 examples/sec; 0.169 sec/batch)
2016-10-15 03:03:34.993603: step 30000, loss = 0.81 (739.0 examples/sec; 0.173 sec/batch)
2016-10-15 03:03:37.508407: step 30010, loss = 0.64 (729.5 examples/sec; 0.175 sec/batch)
2016-10-15 03:03:39.268092: step 30020, loss = 0.75 (720.4 examples/sec; 0.178 sec/batch)
2016-10-15 03:03:41.029442: step 30030, loss = 0.79 (725.1 examples/sec; 0.177 sec/batch)
2016-10-15 03:03:42.784712: step 30040, loss = 0.69 (732.7 examples/sec; 0.175 sec/batch)
2016-10-15 03:03:44.557296: step 30050, loss = 0.69 (718.8 examples/sec; 0.178 sec/batch)
2016-10-15 03:03:46.310480: step 30060, loss = 0.94 (748.7 examples/sec; 0.171 sec/batch)
2016-10-15 03:03:48.068538: step 30070, loss = 0.71 (736.5 examples/sec; 0.174 sec/batch)
2016-10-15 03:03:49.831791: step 30080, loss = 0.69 (713.6 examples/sec; 0.179 sec/batch)
2016-10-15 03:03:51.583643: step 30090, loss = 0.79 (738.4 examples/sec; 0.173 sec/batch)
2016-10-15 03:03:53.350386: step 30100, loss = 0.82 (720.0 examples/sec; 0.178 sec/batch)
2016-10-15 03:03:55.293097: step 30110, loss = 0.68 (724.4 examples/sec; 0.177 sec/batch)
2016-10-15 03:03:57.065541: step 30120, loss = 0.58 (729.8 examples/sec; 0.175 sec/batch)
2016-10-15 03:03:58.829195: step 30130, loss = 0.62 (715.1 examples/sec; 0.179 sec/batch)
2016-10-15 03:04:00.586367: step 30140, loss = 0.65 (743.3 examples/sec; 0.172 sec/batch)
2016-10-15 03:04:02.344375: step 30150, loss = 0.78 (713.3 examples/sec; 0.179 sec/batch)
2016-10-15 03:04:04.124687: step 30160, loss = 0.76 (711.5 examples/sec; 0.180 sec/batch)
2016-10-15 03:04:05.877249: step 30170, loss = 0.71 (733.1 examples/sec; 0.175 sec/batch)
2016-10-15 03:04:07.632151: step 30180, loss = 0.77 (750.7 examples/sec; 0.170 sec/batch)
2016-10-15 03:04:09.389069: step 30190, loss = 0.79 (725.6 examples/sec; 0.176 sec/batch)
2016-10-15 03:04:11.145071: step 30200, loss = 0.79 (723.9 examples/sec; 0.177 sec/batch)
2016-10-15 03:04:13.084625: step 30210, loss = 0.79 (715.8 examples/sec; 0.179 sec/batch)
2016-10-15 03:04:14.843420: step 30220, loss = 0.93 (738.5 examples/sec; 0.173 sec/batch)
2016-10-15 03:04:16.615797: step 30230, loss = 0.82 (721.7 examples/sec; 0.177 sec/batch)
2016-10-15 03:04:18.372836: step 30240, loss = 0.92 (731.5 examples/sec; 0.175 sec/batch)
2016-10-15 03:04:20.147754: step 30250, loss = 0.60 (701.0 examples/sec; 0.183 sec/batch)
2016-10-15 03:04:21.904362: step 30260, loss = 0.80 (698.1 examples/sec; 0.183 sec/batch)
2016-10-15 03:04:23.651020: step 30270, loss = 0.77 (726.9 examples/sec; 0.176 sec/batch)
2016-10-15 03:04:25.408521: step 30280, loss = 0.74 (734.4 examples/sec; 0.174 sec/batch)
2016-10-15 03:04:27.162029: step 30290, loss = 0.70 (764.1 examples/sec; 0.168 sec/batch)
2016-10-15 03:04:28.920733: step 30300, loss = 0.65 (719.4 examples/sec; 0.178 sec/batch)
2016-10-15 03:04:30.878167: step 30310, loss = 0.74 (726.6 examples/sec; 0.176 sec/batch)
2016-10-15 03:04:32.635928: step 30320, loss = 0.79 (717.5 examples/sec; 0.178 sec/batch)
2016-10-15 03:04:34.389276: step 30330, loss = 0.66 (716.1 examples/sec; 0.179 sec/batch)
2016-10-15 03:04:36.148008: step 30340, loss = 0.80 (742.4 examples/sec; 0.172 sec/batch)
2016-10-15 03:04:37.902192: step 30350, loss = 0.90 (717.3 examples/sec; 0.178 sec/batch)
2016-10-15 03:04:39.660556: step 30360, loss = 0.73 (750.0 examples/sec; 0.171 sec/batch)
2016-10-15 03:04:41.418145: step 30370, loss = 0.85 (742.5 examples/sec; 0.172 sec/batch)
2016-10-15 03:04:43.181365: step 30380, loss = 0.84 (706.5 examples/sec; 0.181 sec/batch)
2016-10-15 03:04:44.924987: step 30390, loss = 0.76 (730.9 examples/sec; 0.175 sec/batch)
2016-10-15 03:04:46.684015: step 30400, loss = 0.87 (728.6 examples/sec; 0.176 sec/batch)
2016-10-15 03:04:48.615486: step 30410, loss = 0.75 (752.3 examples/sec; 0.170 sec/batch)
2016-10-15 03:04:50.370649: step 30420, loss = 0.87 (720.7 examples/sec; 0.178 sec/batch)
2016-10-15 03:04:52.133614: step 30430, loss = 0.64 (727.1 examples/sec; 0.176 sec/batch)
2016-10-15 03:04:53.898530: step 30440, loss = 0.68 (702.9 examples/sec; 0.182 sec/batch)
2016-10-15 03:04:55.649034: step 30450, loss = 0.60 (729.1 examples/sec; 0.176 sec/batch)
2016-10-15 03:04:57.409491: step 30460, loss = 0.84 (736.6 examples/sec; 0.174 sec/batch)
2016-10-15 03:04:59.171326: step 30470, loss = 0.72 (729.5 examples/sec; 0.175 sec/batch)
2016-10-15 03:05:00.937178: step 30480, loss = 0.70 (734.3 examples/sec; 0.174 sec/batch)
2016-10-15 03:05:02.719294: step 30490, loss = 0.73 (718.1 examples/sec; 0.178 sec/batch)
2016-10-15 03:05:04.479699: step 30500, loss = 0.70 (747.6 examples/sec; 0.171 sec/batch)
2016-10-15 03:05:06.424371: step 30510, loss = 0.78 (750.5 examples/sec; 0.171 sec/batch)
2016-10-15 03:05:08.196409: step 30520, loss = 0.77 (723.7 examples/sec; 0.177 sec/batch)
2016-10-15 03:05:09.961925: step 30530, loss = 0.83 (711.4 examples/sec; 0.180 sec/batch)
2016-10-15 03:05:11.721029: step 30540, loss = 0.72 (724.9 examples/sec; 0.177 sec/batch)
2016-10-15 03:05:13.484433: step 30550, loss = 0.71 (750.4 examples/sec; 0.171 sec/batch)
2016-10-15 03:05:15.244968: step 30560, loss = 0.81 (734.5 examples/sec; 0.174 sec/batch)
2016-10-15 03:05:16.994865: step 30570, loss = 0.77 (770.3 examples/sec; 0.166 sec/batch)
2016-10-15 03:05:18.762052: step 30580, loss = 0.77 (717.2 examples/sec; 0.178 sec/batch)
2016-10-15 03:05:20.516254: step 30590, loss = 0.89 (730.5 examples/sec; 0.175 sec/batch)
2016-10-15 03:05:22.271096: step 30600, loss = 0.74 (745.4 examples/sec; 0.172 sec/batch)
2016-10-15 03:05:24.234648: step 30610, loss = 0.65 (719.5 examples/sec; 0.178 sec/batch)
2016-10-15 03:05:25.998359: step 30620, loss = 0.84 (737.1 examples/sec; 0.174 sec/batch)
2016-10-15 03:05:27.759356: step 30630, loss = 0.70 (734.7 examples/sec; 0.174 sec/batch)
2016-10-15 03:05:29.529821: step 30640, loss = 0.86 (704.1 examples/sec; 0.182 sec/batch)
2016-10-15 03:05:31.283336: step 30650, loss = 0.71 (726.3 examples/sec; 0.176 sec/batch)
2016-10-15 03:05:33.044312: step 30660, loss = 0.77 (715.2 examples/sec; 0.179 sec/batch)
2016-10-15 03:05:34.796416: step 30670, loss = 0.81 (724.3 examples/sec; 0.177 sec/batch)
2016-10-15 03:05:36.550886: step 30680, loss = 0.87 (742.0 examples/sec; 0.173 sec/batch)
2016-10-15 03:05:38.318502: step 30690, loss = 0.77 (696.6 examples/sec; 0.184 sec/batch)
2016-10-15 03:05:40.064324: step 30700, loss = 0.96 (728.2 examples/sec; 0.176 sec/batch)
2016-10-15 03:05:42.017924: step 30710, loss = 0.81 (733.2 examples/sec; 0.175 sec/batch)
2016-10-15 03:05:43.779292: step 30720, loss = 0.70 (721.9 examples/sec; 0.177 sec/batch)
2016-10-15 03:05:45.540422: step 30730, loss = 0.72 (754.6 examples/sec; 0.170 sec/batch)
2016-10-15 03:05:47.301789: step 30740, loss = 0.78 (710.8 examples/sec; 0.180 sec/batch)
2016-10-15 03:05:49.058429: step 30750, loss = 0.78 (759.2 examples/sec; 0.169 sec/batch)
2016-10-15 03:05:50.805733: step 30760, loss = 0.76 (722.0 examples/sec; 0.177 sec/batch)
2016-10-15 03:05:52.574116: step 30770, loss = 0.75 (744.7 examples/sec; 0.172 sec/batch)
2016-10-15 03:05:54.341395: step 30780, loss = 0.88 (731.2 examples/sec; 0.175 sec/batch)
2016-10-15 03:05:56.098473: step 30790, loss = 0.93 (716.3 examples/sec; 0.179 sec/batch)
2016-10-15 03:05:57.851038: step 30800, loss = 0.79 (739.7 examples/sec; 0.173 sec/batch)
2016-10-15 03:05:59.788723: step 30810, loss = 0.85 (742.2 examples/sec; 0.172 sec/batch)
2016-10-15 03:06:01.546880: step 30820, loss = 0.88 (738.0 examples/sec; 0.173 sec/batch)
2016-10-15 03:06:03.318647: step 30830, loss = 0.80 (707.9 examples/sec; 0.181 sec/batch)
2016-10-15 03:06:05.066282: step 30840, loss = 0.95 (723.2 examples/sec; 0.177 sec/batch)
2016-10-15 03:06:06.821569: step 30850, loss = 0.70 (730.8 examples/sec; 0.175 sec/batch)
2016-10-15 03:06:08.582333: step 30860, loss = 0.74 (701.3 examples/sec; 0.183 sec/batch)
2016-10-15 03:06:10.339575: step 30870, loss = 0.73 (709.7 examples/sec; 0.180 sec/batch)
2016-10-15 03:06:12.089907: step 30880, loss = 0.87 (731.6 examples/sec; 0.175 sec/batch)
2016-10-15 03:06:13.845608: step 30890, loss = 0.80 (736.4 examples/sec; 0.174 sec/batch)
2016-10-15 03:06:15.608878: step 30900, loss = 0.78 (714.8 examples/sec; 0.179 sec/batch)
2016-10-15 03:06:17.553221: step 30910, loss = 0.67 (740.0 examples/sec; 0.173 sec/batch)
2016-10-15 03:06:19.314588: step 30920, loss = 0.66 (727.0 examples/sec; 0.176 sec/batch)
2016-10-15 03:06:21.079829: step 30930, loss = 0.86 (737.2 examples/sec; 0.174 sec/batch)
2016-10-15 03:06:22.845346: step 30940, loss = 0.75 (724.4 examples/sec; 0.177 sec/batch)
2016-10-15 03:06:24.620705: step 30950, loss = 0.93 (734.5 examples/sec; 0.174 sec/batch)
2016-10-15 03:06:26.378537: step 30960, loss = 0.81 (738.6 examples/sec; 0.173 sec/batch)
2016-10-15 03:06:28.144157: step 30970, loss = 0.79 (729.7 examples/sec; 0.175 sec/batch)
2016-10-15 03:06:29.899029: step 30980, loss = 0.85 (731.4 examples/sec; 0.175 sec/batch)
2016-10-15 03:06:31.656906: step 30990, loss = 0.82 (712.0 examples/sec; 0.180 sec/batch)
2016-10-15 03:06:33.421143: step 31000, loss = 0.94 (724.2 examples/sec; 0.177 sec/batch)
2016-10-15 03:06:35.843945: step 31010, loss = 0.67 (746.1 examples/sec; 0.172 sec/batch)
2016-10-15 03:06:37.604141: step 31020, loss = 0.78 (758.1 examples/sec; 0.169 sec/batch)
2016-10-15 03:06:39.364366: step 31030, loss = 0.85 (722.6 examples/sec; 0.177 sec/batch)
2016-10-15 03:06:41.122120: step 31040, loss = 0.70 (724.4 examples/sec; 0.177 sec/batch)
2016-10-15 03:06:42.874737: step 31050, loss = 0.83 (727.4 examples/sec; 0.176 sec/batch)
2016-10-15 03:06:44.633789: step 31060, loss = 0.80 (727.1 examples/sec; 0.176 sec/batch)
2016-10-15 03:06:46.388108: step 31070, loss = 0.72 (731.0 examples/sec; 0.175 sec/batch)
2016-10-15 03:06:48.139945: step 31080, loss = 0.88 (738.7 examples/sec; 0.173 sec/batch)
2016-10-15 03:06:49.890035: step 31090, loss = 0.64 (741.1 examples/sec; 0.173 sec/batch)
2016-10-15 03:06:51.646595: step 31100, loss = 0.69 (747.2 examples/sec; 0.171 sec/batch)
2016-10-15 03:06:53.582859: step 31110, loss = 0.60 (730.4 examples/sec; 0.175 sec/batch)
2016-10-15 03:06:55.337854: step 31120, loss = 0.80 (743.7 examples/sec; 0.172 sec/batch)
2016-10-15 03:06:57.106284: step 31130, loss = 0.88 (703.8 examples/sec; 0.182 sec/batch)
2016-10-15 03:06:58.864895: step 31140, loss = 0.76 (762.5 examples/sec; 0.168 sec/batch)
2016-10-15 03:07:00.625197: step 31150, loss = 0.81 (714.4 examples/sec; 0.179 sec/batch)
2016-10-15 03:07:02.380258: step 31160, loss = 0.84 (702.8 examples/sec; 0.182 sec/batch)
2016-10-15 03:07:04.134367: step 31170, loss = 0.78 (752.7 examples/sec; 0.170 sec/batch)
2016-10-15 03:07:05.886305: step 31180, loss = 0.73 (732.3 examples/sec; 0.175 sec/batch)
2016-10-15 03:07:07.641756: step 31190, loss = 0.80 (715.9 examples/sec; 0.179 sec/batch)
2016-10-15 03:07:09.400576: step 31200, loss = 0.75 (699.5 examples/sec; 0.183 sec/batch)
2016-10-15 03:07:11.339881: step 31210, loss = 0.76 (699.3 examples/sec; 0.183 sec/batch)
2016-10-15 03:07:13.096681: step 31220, loss = 0.78 (733.5 examples/sec; 0.175 sec/batch)
2016-10-15 03:07:14.851946: step 31230, loss = 0.75 (727.2 examples/sec; 0.176 sec/batch)
2016-10-15 03:07:16.608564: step 31240, loss = 0.91 (727.2 examples/sec; 0.176 sec/batch)
2016-10-15 03:07:18.374546: step 31250, loss = 0.76 (733.5 examples/sec; 0.175 sec/batch)
2016-10-15 03:07:20.139178: step 31260, loss = 0.79 (711.6 examples/sec; 0.180 sec/batch)
2016-10-15 03:07:21.897884: step 31270, loss = 0.89 (734.8 examples/sec; 0.174 sec/batch)
2016-10-15 03:07:23.652145: step 31280, loss = 0.70 (736.9 examples/sec; 0.174 sec/batch)
2016-10-15 03:07:25.417812: step 31290, loss = 0.69 (737.4 examples/sec; 0.174 sec/batch)
2016-10-15 03:07:27.173994: step 31300, loss = 0.72 (737.8 examples/sec; 0.173 sec/batch)
2016-10-15 03:07:29.156955: step 31310, loss = 0.79 (734.9 examples/sec; 0.174 sec/batch)
2016-10-15 03:07:30.914027: step 31320, loss = 0.62 (727.6 examples/sec; 0.176 sec/batch)
2016-10-15 03:07:32.684944: step 31330, loss = 1.00 (712.9 examples/sec; 0.180 sec/batch)
2016-10-15 03:07:34.435234: step 31340, loss = 0.75 (725.4 examples/sec; 0.176 sec/batch)
2016-10-15 03:07:36.196342: step 31350, loss = 0.83 (743.6 examples/sec; 0.172 sec/batch)
2016-10-15 03:07:37.959104: step 31360, loss = 0.70 (732.8 examples/sec; 0.175 sec/batch)
2016-10-15 03:07:39.715028: step 31370, loss = 0.78 (734.8 examples/sec; 0.174 sec/batch)
2016-10-15 03:07:41.471403: step 31380, loss = 0.60 (757.7 examples/sec; 0.169 sec/batch)
2016-10-15 03:07:43.224014: step 31390, loss = 0.85 (724.3 examples/sec; 0.177 sec/batch)
2016-10-15 03:07:44.986474: step 31400, loss = 0.89 (742.7 examples/sec; 0.172 sec/batch)
2016-10-15 03:07:46.941341: step 31410, loss = 0.57 (720.5 examples/sec; 0.178 sec/batch)
2016-10-15 03:07:48.704183: step 31420, loss = 0.86 (728.5 examples/sec; 0.176 sec/batch)
2016-10-15 03:07:50.456374: step 31430, loss = 0.96 (729.8 examples/sec; 0.175 sec/batch)
2016-10-15 03:07:52.204424: step 31440, loss = 0.81 (758.9 examples/sec; 0.169 sec/batch)
2016-10-15 03:07:53.959328: step 31450, loss = 0.71 (732.4 examples/sec; 0.175 sec/batch)
2016-10-15 03:07:55.724153: step 31460, loss = 0.78 (717.6 examples/sec; 0.178 sec/batch)
2016-10-15 03:07:57.476204: step 31470, loss = 0.75 (710.5 examples/sec; 0.180 sec/batch)
2016-10-15 03:07:59.233098: step 31480, loss = 0.77 (720.5 examples/sec; 0.178 sec/batch)
2016-10-15 03:08:00.993041: step 31490, loss = 0.80 (733.9 examples/sec; 0.174 sec/batch)
2016-10-15 03:08:02.768314: step 31500, loss = 0.89 (684.6 examples/sec; 0.187 sec/batch)
2016-10-15 03:08:04.705306: step 31510, loss = 0.69 (728.8 examples/sec; 0.176 sec/batch)
2016-10-15 03:08:06.471181: step 31520, loss = 1.00 (743.2 examples/sec; 0.172 sec/batch)
2016-10-15 03:08:08.231217: step 31530, loss = 0.79 (701.3 examples/sec; 0.183 sec/batch)
2016-10-15 03:08:09.980010: step 31540, loss = 0.74 (751.4 examples/sec; 0.170 sec/batch)
2016-10-15 03:08:11.741018: step 31550, loss = 0.84 (721.0 examples/sec; 0.178 sec/batch)
2016-10-15 03:08:13.500123: step 31560, loss = 0.77 (737.5 examples/sec; 0.174 sec/batch)
2016-10-15 03:08:15.255936: step 31570, loss = 0.70 (711.3 examples/sec; 0.180 sec/batch)
2016-10-15 03:08:17.005917: step 31580, loss = 0.84 (727.6 examples/sec; 0.176 sec/batch)
2016-10-15 03:08:18.759201: step 31590, loss = 0.80 (730.7 examples/sec; 0.175 sec/batch)
2016-10-15 03:08:20.510058: step 31600, loss = 0.76 (751.9 examples/sec; 0.170 sec/batch)
2016-10-15 03:08:22.457760: step 31610, loss = 0.71 (700.0 examples/sec; 0.183 sec/batch)
2016-10-15 03:08:24.215465: step 31620, loss = 0.82 (712.7 examples/sec; 0.180 sec/batch)
2016-10-15 03:08:25.977446: step 31630, loss = 0.77 (728.0 examples/sec; 0.176 sec/batch)
2016-10-15 03:08:27.730602: step 31640, loss = 0.82 (762.9 examples/sec; 0.168 sec/batch)
2016-10-15 03:08:29.492814: step 31650, loss = 0.74 (714.1 examples/sec; 0.179 sec/batch)
2016-10-15 03:08:31.261678: step 31660, loss = 0.62 (708.6 examples/sec; 0.181 sec/batch)
2016-10-15 03:08:33.032930: step 31670, loss = 0.75 (701.5 examples/sec; 0.182 sec/batch)
2016-10-15 03:08:34.785430: step 31680, loss = 0.71 (699.6 examples/sec; 0.183 sec/batch)
2016-10-15 03:08:36.554351: step 31690, loss = 0.84 (720.6 examples/sec; 0.178 sec/batch)
2016-10-15 03:08:38.311104: step 31700, loss = 0.74 (714.2 examples/sec; 0.179 sec/batch)
2016-10-15 03:08:40.249453: step 31710, loss = 0.64 (716.2 examples/sec; 0.179 sec/batch)
2016-10-15 03:08:42.022414: step 31720, loss = 0.85 (707.0 examples/sec; 0.181 sec/batch)
2016-10-15 03:08:43.789412: step 31730, loss = 0.66 (692.3 examples/sec; 0.185 sec/batch)
2016-10-15 03:08:45.545660: step 31740, loss = 0.89 (726.5 examples/sec; 0.176 sec/batch)
2016-10-15 03:08:47.303148: step 31750, loss = 0.77 (743.2 examples/sec; 0.172 sec/batch)
2016-10-15 03:08:49.070585: step 31760, loss = 0.88 (730.1 examples/sec; 0.175 sec/batch)
2016-10-15 03:08:50.820983: step 31770, loss = 0.75 (733.8 examples/sec; 0.174 sec/batch)
2016-10-15 03:08:52.584629: step 31780, loss = 0.80 (727.5 examples/sec; 0.176 sec/batch)
2016-10-15 03:08:54.359732: step 31790, loss = 0.80 (768.3 examples/sec; 0.167 sec/batch)
2016-10-15 03:08:56.124289: step 31800, loss = 0.70 (738.9 examples/sec; 0.173 sec/batch)
2016-10-15 03:08:58.065859: step 31810, loss = 0.75 (726.3 examples/sec; 0.176 sec/batch)
2016-10-15 03:08:59.823683: step 31820, loss = 0.79 (718.2 examples/sec; 0.178 sec/batch)
2016-10-15 03:09:01.599379: step 31830, loss = 0.86 (696.7 examples/sec; 0.184 sec/batch)
2016-10-15 03:09:03.343238: step 31840, loss = 0.96 (745.2 examples/sec; 0.172 sec/batch)
2016-10-15 03:09:05.101358: step 31850, loss = 0.70 (741.5 examples/sec; 0.173 sec/batch)
2016-10-15 03:09:06.862635: step 31860, loss = 0.78 (721.3 examples/sec; 0.177 sec/batch)
2016-10-15 03:09:08.619979: step 31870, loss = 0.87 (709.5 examples/sec; 0.180 sec/batch)
2016-10-15 03:09:10.378462: step 31880, loss = 0.79 (715.9 examples/sec; 0.179 sec/batch)
2016-10-15 03:09:12.136479: step 31890, loss = 0.75 (735.0 examples/sec; 0.174 sec/batch)
2016-10-15 03:09:13.907674: step 31900, loss = 0.74 (749.7 examples/sec; 0.171 sec/batch)
2016-10-15 03:09:15.848390: step 31910, loss = 0.89 (770.3 examples/sec; 0.166 sec/batch)
2016-10-15 03:09:17.619371: step 31920, loss = 0.92 (741.5 examples/sec; 0.173 sec/batch)
2016-10-15 03:09:19.383816: step 31930, loss = 0.81 (729.1 examples/sec; 0.176 sec/batch)
2016-10-15 03:09:21.144441: step 31940, loss = 0.74 (740.2 examples/sec; 0.173 sec/batch)
2016-10-15 03:09:22.905106: step 31950, loss = 0.68 (716.3 examples/sec; 0.179 sec/batch)
2016-10-15 03:09:24.665641: step 31960, loss = 0.90 (723.4 examples/sec; 0.177 sec/batch)
2016-10-15 03:09:26.425088: step 31970, loss = 0.70 (713.4 examples/sec; 0.179 sec/batch)
2016-10-15 03:09:28.186792: step 31980, loss = 0.80 (722.9 examples/sec; 0.177 sec/batch)
2016-10-15 03:09:29.950443: step 31990, loss = 0.78 (741.9 examples/sec; 0.173 sec/batch)
2016-10-15 03:09:31.706874: step 32000, loss = 0.81 (717.1 examples/sec; 0.178 sec/batch)
2016-10-15 03:09:34.259706: step 32010, loss = 0.86 (717.2 examples/sec; 0.178 sec/batch)
2016-10-15 03:09:36.029473: step 32020, loss = 0.86 (736.2 examples/sec; 0.174 sec/batch)
2016-10-15 03:09:37.795235: step 32030, loss = 0.83 (732.2 examples/sec; 0.175 sec/batch)
2016-10-15 03:09:39.551845: step 32040, loss = 0.86 (736.3 examples/sec; 0.174 sec/batch)
2016-10-15 03:09:41.315793: step 32050, loss = 0.73 (708.1 examples/sec; 0.181 sec/batch)
2016-10-15 03:09:43.081206: step 32060, loss = 0.72 (687.9 examples/sec; 0.186 sec/batch)
2016-10-15 03:09:44.836635: step 32070, loss = 1.00 (713.9 examples/sec; 0.179 sec/batch)
2016-10-15 03:09:46.604630: step 32080, loss = 0.84 (719.5 examples/sec; 0.178 sec/batch)
2016-10-15 03:09:48.367043: step 32090, loss = 0.84 (712.8 examples/sec; 0.180 sec/batch)
2016-10-15 03:09:50.121459: step 32100, loss = 0.71 (747.7 examples/sec; 0.171 sec/batch)
2016-10-15 03:09:52.067859: step 32110, loss = 0.86 (698.2 examples/sec; 0.183 sec/batch)
2016-10-15 03:09:53.827435: step 32120, loss = 0.81 (722.1 examples/sec; 0.177 sec/batch)
2016-10-15 03:09:55.587379: step 32130, loss = 0.92 (716.6 examples/sec; 0.179 sec/batch)
2016-10-15 03:09:57.345626: step 32140, loss = 0.66 (734.2 examples/sec; 0.174 sec/batch)
2016-10-15 03:09:59.108130: step 32150, loss = 0.61 (716.8 examples/sec; 0.179 sec/batch)
2016-10-15 03:10:00.872360: step 32160, loss = 0.73 (730.0 examples/sec; 0.175 sec/batch)
2016-10-15 03:10:02.664998: step 32170, loss = 0.76 (701.1 examples/sec; 0.183 sec/batch)
2016-10-15 03:10:04.429228: step 32180, loss = 0.85 (708.8 examples/sec; 0.181 sec/batch)
2016-10-15 03:10:06.190667: step 32190, loss = 0.76 (692.7 examples/sec; 0.185 sec/batch)
2016-10-15 03:10:07.949474: step 32200, loss = 0.73 (714.6 examples/sec; 0.179 sec/batch)
2016-10-15 03:10:09.893576: step 32210, loss = 0.88 (732.5 examples/sec; 0.175 sec/batch)
2016-10-15 03:10:11.647028: step 32220, loss = 0.77 (737.8 examples/sec; 0.173 sec/batch)
2016-10-15 03:10:13.410466: step 32230, loss = 0.82 (710.9 examples/sec; 0.180 sec/batch)
2016-10-15 03:10:15.164439: step 32240, loss = 0.88 (747.3 examples/sec; 0.171 sec/batch)
2016-10-15 03:10:16.928635: step 32250, loss = 0.64 (732.1 examples/sec; 0.175 sec/batch)
2016-10-15 03:10:18.691581: step 32260, loss = 0.71 (715.8 examples/sec; 0.179 sec/batch)
2016-10-15 03:10:20.454647: step 32270, loss = 0.88 (703.9 examples/sec; 0.182 sec/batch)
2016-10-15 03:10:22.202255: step 32280, loss = 0.80 (746.8 examples/sec; 0.171 sec/batch)
2016-10-15 03:10:23.971586: step 32290, loss = 0.61 (736.7 examples/sec; 0.174 sec/batch)
2016-10-15 03:10:25.734634: step 32300, loss = 0.81 (724.8 examples/sec; 0.177 sec/batch)
2016-10-15 03:10:27.684236: step 32310, loss = 0.83 (732.9 examples/sec; 0.175 sec/batch)
2016-10-15 03:10:29.446734: step 32320, loss = 0.85 (719.8 examples/sec; 0.178 sec/batch)
2016-10-15 03:10:31.208428: step 32330, loss = 0.96 (753.0 examples/sec; 0.170 sec/batch)
2016-10-15 03:10:32.973166: step 32340, loss = 0.71 (709.6 examples/sec; 0.180 sec/batch)
2016-10-15 03:10:34.733900: step 32350, loss = 0.80 (721.5 examples/sec; 0.177 sec/batch)
2016-10-15 03:10:36.489933: step 32360, loss = 0.71 (731.6 examples/sec; 0.175 sec/batch)
2016-10-15 03:10:38.255640: step 32370, loss = 0.83 (688.3 examples/sec; 0.186 sec/batch)
2016-10-15 03:10:40.006185: step 32380, loss = 0.71 (712.0 examples/sec; 0.180 sec/batch)
2016-10-15 03:10:41.768707: step 32390, loss = 0.86 (718.5 examples/sec; 0.178 sec/batch)
2016-10-15 03:10:43.530813: step 32400, loss = 0.65 (733.5 examples/sec; 0.175 sec/batch)
2016-10-15 03:10:45.467220: step 32410, loss = 0.78 (751.7 examples/sec; 0.170 sec/batch)
2016-10-15 03:10:47.233342: step 32420, loss = 0.72 (710.9 examples/sec; 0.180 sec/batch)
2016-10-15 03:10:48.987024: step 32430, loss = 0.75 (729.5 examples/sec; 0.175 sec/batch)
2016-10-15 03:10:50.750118: step 32440, loss = 0.84 (735.0 examples/sec; 0.174 sec/batch)
2016-10-15 03:10:52.515973: step 32450, loss = 0.65 (727.0 examples/sec; 0.176 sec/batch)
2016-10-15 03:10:54.285441: step 32460, loss = 0.98 (746.4 examples/sec; 0.171 sec/batch)
2016-10-15 03:10:56.041892: step 32470, loss = 0.70 (716.5 examples/sec; 0.179 sec/batch)
2016-10-15 03:10:57.793939: step 32480, loss = 0.78 (723.3 examples/sec; 0.177 sec/batch)
2016-10-15 03:10:59.558133: step 32490, loss = 0.84 (704.0 examples/sec; 0.182 sec/batch)
2016-10-15 03:11:01.316169: step 32500, loss = 0.80 (700.1 examples/sec; 0.183 sec/batch)
2016-10-15 03:11:03.265249: step 32510, loss = 0.82 (707.0 examples/sec; 0.181 sec/batch)
2016-10-15 03:11:05.025628: step 32520, loss = 0.74 (723.0 examples/sec; 0.177 sec/batch)
2016-10-15 03:11:06.787551: step 32530, loss = 0.77 (711.5 examples/sec; 0.180 sec/batch)
2016-10-15 03:11:08.543454: step 32540, loss = 0.92 (726.1 examples/sec; 0.176 sec/batch)
2016-10-15 03:11:10.300267: step 32550, loss = 0.76 (739.7 examples/sec; 0.173 sec/batch)
2016-10-15 03:11:12.056139: step 32560, loss = 0.78 (720.8 examples/sec; 0.178 sec/batch)
2016-10-15 03:11:13.818309: step 32570, loss = 0.80 (720.1 examples/sec; 0.178 sec/batch)
2016-10-15 03:11:15.576117: step 32580, loss = 0.88 (744.1 examples/sec; 0.172 sec/batch)
2016-10-15 03:11:17.337365: step 32590, loss = 0.61 (730.7 examples/sec; 0.175 sec/batch)
2016-10-15 03:11:19.088561: step 32600, loss = 0.70 (727.9 examples/sec; 0.176 sec/batch)
2016-10-15 03:11:21.041355: step 32610, loss = 0.86 (740.5 examples/sec; 0.173 sec/batch)
2016-10-15 03:11:22.789199: step 32620, loss = 0.90 (742.8 examples/sec; 0.172 sec/batch)
2016-10-15 03:11:24.549289: step 32630, loss = 0.72 (717.9 examples/sec; 0.178 sec/batch)
2016-10-15 03:11:26.299166: step 32640, loss = 0.99 (724.4 examples/sec; 0.177 sec/batch)
2016-10-15 03:11:28.058741: step 32650, loss = 0.86 (724.4 examples/sec; 0.177 sec/batch)
2016-10-15 03:11:29.818553: step 32660, loss = 0.74 (697.3 examples/sec; 0.184 sec/batch)
2016-10-15 03:11:31.580948: step 32670, loss = 0.71 (741.1 examples/sec; 0.173 sec/batch)
2016-10-15 03:11:33.367822: step 32680, loss = 0.81 (711.3 examples/sec; 0.180 sec/batch)
2016-10-15 03:11:35.120059: step 32690, loss = 0.86 (742.3 examples/sec; 0.172 sec/batch)
2016-10-15 03:11:36.872791: step 32700, loss = 0.66 (731.8 examples/sec; 0.175 sec/batch)
2016-10-15 03:11:38.815437: step 32710, loss = 0.64 (736.5 examples/sec; 0.174 sec/batch)
2016-10-15 03:11:40.575939: step 32720, loss = 0.79 (716.4 examples/sec; 0.179 sec/batch)
2016-10-15 03:11:42.326967: step 32730, loss = 0.71 (735.6 examples/sec; 0.174 sec/batch)
2016-10-15 03:11:44.085510: step 32740, loss = 0.74 (730.6 examples/sec; 0.175 sec/batch)
2016-10-15 03:11:45.837014: step 32750, loss = 0.73 (741.4 examples/sec; 0.173 sec/batch)
2016-10-15 03:11:47.604745: step 32760, loss = 0.78 (718.1 examples/sec; 0.178 sec/batch)
2016-10-15 03:11:49.375327: step 32770, loss = 0.79 (689.9 examples/sec; 0.186 sec/batch)
2016-10-15 03:11:51.134534: step 32780, loss = 0.89 (711.7 examples/sec; 0.180 sec/batch)
2016-10-15 03:11:52.892547: step 32790, loss = 0.74 (719.9 examples/sec; 0.178 sec/batch)
2016-10-15 03:11:54.653433: step 32800, loss = 0.79 (711.4 examples/sec; 0.180 sec/batch)
2016-10-15 03:11:56.598266: step 32810, loss = 0.75 (745.9 examples/sec; 0.172 sec/batch)
2016-10-15 03:11:58.366925: step 32820, loss = 0.86 (735.1 examples/sec; 0.174 sec/batch)
2016-10-15 03:12:00.124946: step 32830, loss = 0.79 (724.4 examples/sec; 0.177 sec/batch)
2016-10-15 03:12:01.894383: step 32840, loss = 0.78 (712.9 examples/sec; 0.180 sec/batch)
2016-10-15 03:12:03.655241: step 32850, loss = 0.86 (721.0 examples/sec; 0.178 sec/batch)
2016-10-15 03:12:05.412126: step 32860, loss = 0.72 (756.8 examples/sec; 0.169 sec/batch)
2016-10-15 03:12:07.177385: step 32870, loss = 0.70 (708.9 examples/sec; 0.181 sec/batch)
2016-10-15 03:12:08.946686: step 32880, loss = 0.77 (745.0 examples/sec; 0.172 sec/batch)
2016-10-15 03:12:10.702232: step 32890, loss = 0.86 (729.1 examples/sec; 0.176 sec/batch)
2016-10-15 03:12:12.460211: step 32900, loss = 0.97 (730.9 examples/sec; 0.175 sec/batch)
2016-10-15 03:12:14.400616: step 32910, loss = 0.83 (734.4 examples/sec; 0.174 sec/batch)
2016-10-15 03:12:16.160942: step 32920, loss = 0.77 (723.9 examples/sec; 0.177 sec/batch)
2016-10-15 03:12:17.926972: step 32930, loss = 0.87 (718.2 examples/sec; 0.178 sec/batch)
2016-10-15 03:12:19.681415: step 32940, loss = 0.69 (741.2 examples/sec; 0.173 sec/batch)
2016-10-15 03:12:21.441231: step 32950, loss = 0.86 (734.7 examples/sec; 0.174 sec/batch)
2016-10-15 03:12:23.204857: step 32960, loss = 0.70 (744.8 examples/sec; 0.172 sec/batch)
2016-10-15 03:12:24.971174: step 32970, loss = 0.76 (718.3 examples/sec; 0.178 sec/batch)
2016-10-15 03:12:26.732965: step 32980, loss = 0.81 (724.6 examples/sec; 0.177 sec/batch)
2016-10-15 03:12:28.490844: step 32990, loss = 0.83 (758.7 examples/sec; 0.169 sec/batch)
2016-10-15 03:12:30.251494: step 33000, loss = 0.80 (734.8 examples/sec; 0.174 sec/batch)
2016-10-15 03:12:32.692213: step 33010, loss = 0.98 (763.3 examples/sec; 0.168 sec/batch)
2016-10-15 03:12:34.451692: step 33020, loss = 0.89 (714.4 examples/sec; 0.179 sec/batch)
2016-10-15 03:12:36.212283: step 33030, loss = 0.71 (725.3 examples/sec; 0.176 sec/batch)
2016-10-15 03:12:37.969543: step 33040, loss = 0.72 (734.7 examples/sec; 0.174 sec/batch)
2016-10-15 03:12:39.737359: step 33050, loss = 0.76 (706.0 examples/sec; 0.181 sec/batch)
2016-10-15 03:12:41.491446: step 33060, loss = 0.90 (757.2 examples/sec; 0.169 sec/batch)
2016-10-15 03:12:43.241773: step 33070, loss = 0.76 (728.9 examples/sec; 0.176 sec/batch)
2016-10-15 03:12:45.001467: step 33080, loss = 0.86 (730.5 examples/sec; 0.175 sec/batch)
2016-10-15 03:12:46.762698: step 33090, loss = 0.76 (749.9 examples/sec; 0.171 sec/batch)
2016-10-15 03:12:48.525149: step 33100, loss = 0.78 (732.8 examples/sec; 0.175 sec/batch)
2016-10-15 03:12:50.472689: step 33110, loss = 0.86 (734.0 examples/sec; 0.174 sec/batch)
2016-10-15 03:12:52.230783: step 33120, loss = 0.78 (777.5 examples/sec; 0.165 sec/batch)
2016-10-15 03:12:53.998567: step 33130, loss = 0.72 (715.9 examples/sec; 0.179 sec/batch)
2016-10-15 03:12:55.756391: step 33140, loss = 0.74 (735.8 examples/sec; 0.174 sec/batch)
2016-10-15 03:12:57.510472: step 33150, loss = 0.95 (734.7 examples/sec; 0.174 sec/batch)
2016-10-15 03:12:59.273127: step 33160, loss = 0.59 (736.0 examples/sec; 0.174 sec/batch)
2016-10-15 03:13:01.025887: step 33170, loss = 0.91 (734.2 examples/sec; 0.174 sec/batch)
2016-10-15 03:13:02.784076: step 33180, loss = 0.66 (724.9 examples/sec; 0.177 sec/batch)
2016-10-15 03:13:04.549332: step 33190, loss = 0.75 (745.9 examples/sec; 0.172 sec/batch)
2016-10-15 03:13:06.311375: step 33200, loss = 0.75 (728.5 examples/sec; 0.176 sec/batch)
2016-10-15 03:13:08.263346: step 33210, loss = 0.75 (717.3 examples/sec; 0.178 sec/batch)
2016-10-15 03:13:10.018025: step 33220, loss = 0.67 (750.5 examples/sec; 0.171 sec/batch)
2016-10-15 03:13:11.791127: step 33230, loss = 0.94 (684.7 examples/sec; 0.187 sec/batch)
2016-10-15 03:13:13.543357: step 33240, loss = 0.80 (723.5 examples/sec; 0.177 sec/batch)
2016-10-15 03:13:15.305251: step 33250, loss = 0.69 (757.3 examples/sec; 0.169 sec/batch)
2016-10-15 03:13:17.067432: step 33260, loss = 0.83 (706.7 examples/sec; 0.181 sec/batch)
2016-10-15 03:13:18.824161: step 33270, loss = 0.69 (709.7 examples/sec; 0.180 sec/batch)
2016-10-15 03:13:20.579145: step 33280, loss = 0.80 (725.8 examples/sec; 0.176 sec/batch)
2016-10-15 03:13:22.334104: step 33290, loss = 0.90 (725.6 examples/sec; 0.176 sec/batch)
2016-10-15 03:13:24.092631: step 33300, loss = 0.72 (736.5 examples/sec; 0.174 sec/batch)
2016-10-15 03:13:26.033695: step 33310, loss = 0.75 (719.0 examples/sec; 0.178 sec/batch)
2016-10-15 03:13:27.784347: step 33320, loss = 0.70 (746.8 examples/sec; 0.171 sec/batch)
2016-10-15 03:13:29.550710: step 33330, loss = 0.80 (725.0 examples/sec; 0.177 sec/batch)
2016-10-15 03:13:31.304265: step 33340, loss = 0.90 (748.1 examples/sec; 0.171 sec/batch)
2016-10-15 03:13:33.064749: step 33350, loss = 0.77 (750.3 examples/sec; 0.171 sec/batch)
2016-10-15 03:13:34.817849: step 33360, loss = 0.68 (730.1 examples/sec; 0.175 sec/batch)
2016-10-15 03:13:36.574927: step 33370, loss = 0.75 (742.2 examples/sec; 0.172 sec/batch)
2016-10-15 03:13:38.328913: step 33380, loss = 0.69 (746.6 examples/sec; 0.171 sec/batch)
2016-10-15 03:13:40.091858: step 33390, loss = 0.73 (723.3 examples/sec; 0.177 sec/batch)
2016-10-15 03:13:41.848305: step 33400, loss = 0.60 (734.9 examples/sec; 0.174 sec/batch)
2016-10-15 03:13:43.787176: step 33410, loss = 0.82 (712.2 examples/sec; 0.180 sec/batch)
2016-10-15 03:13:45.540450: step 33420, loss = 0.72 (748.3 examples/sec; 0.171 sec/batch)
2016-10-15 03:13:47.300069: step 33430, loss = 0.70 (714.5 examples/sec; 0.179 sec/batch)
2016-10-15 03:13:49.058478: step 33440, loss = 0.81 (704.9 examples/sec; 0.182 sec/batch)
2016-10-15 03:13:50.803955: step 33450, loss = 0.85 (773.3 examples/sec; 0.166 sec/batch)
2016-10-15 03:13:52.571084: step 33460, loss = 0.71 (723.6 examples/sec; 0.177 sec/batch)
2016-10-15 03:13:54.337189: step 33470, loss = 0.60 (723.3 examples/sec; 0.177 sec/batch)
2016-10-15 03:13:56.089044: step 33480, loss = 0.67 (722.0 examples/sec; 0.177 sec/batch)
2016-10-15 03:13:57.853821: step 33490, loss = 0.80 (738.0 examples/sec; 0.173 sec/batch)
2016-10-15 03:13:59.610145: step 33500, loss = 0.65 (743.3 examples/sec; 0.172 sec/batch)
2016-10-15 03:14:01.567133: step 33510, loss = 0.81 (711.0 examples/sec; 0.180 sec/batch)
2016-10-15 03:14:03.336190: step 33520, loss = 0.72 (702.2 examples/sec; 0.182 sec/batch)
2016-10-15 03:14:05.094628: step 33530, loss = 0.72 (737.8 examples/sec; 0.173 sec/batch)
2016-10-15 03:14:06.845140: step 33540, loss = 0.82 (752.4 examples/sec; 0.170 sec/batch)
2016-10-15 03:14:08.604388: step 33550, loss = 0.86 (727.9 examples/sec; 0.176 sec/batch)
2016-10-15 03:14:10.371434: step 33560, loss = 0.87 (734.2 examples/sec; 0.174 sec/batch)
2016-10-15 03:14:12.133979: step 33570, loss = 0.79 (739.6 examples/sec; 0.173 sec/batch)
2016-10-15 03:14:13.901171: step 33580, loss = 0.65 (732.1 examples/sec; 0.175 sec/batch)
2016-10-15 03:14:15.657455: step 33590, loss = 0.90 (715.2 examples/sec; 0.179 sec/batch)
2016-10-15 03:14:17.415535: step 33600, loss = 0.64 (726.2 examples/sec; 0.176 sec/batch)
2016-10-15 03:14:19.374395: step 33610, loss = 0.63 (729.3 examples/sec; 0.176 sec/batch)
2016-10-15 03:14:21.137192: step 33620, loss = 0.90 (729.9 examples/sec; 0.175 sec/batch)
2016-10-15 03:14:22.898484: step 33630, loss = 0.89 (731.1 examples/sec; 0.175 sec/batch)
2016-10-15 03:14:24.666586: step 33640, loss = 0.73 (713.2 examples/sec; 0.179 sec/batch)
2016-10-15 03:14:26.435993: step 33650, loss = 0.60 (729.9 examples/sec; 0.175 sec/batch)
2016-10-15 03:14:28.194926: step 33660, loss = 0.92 (742.8 examples/sec; 0.172 sec/batch)
2016-10-15 03:14:29.957957: step 33670, loss = 0.85 (725.8 examples/sec; 0.176 sec/batch)
2016-10-15 03:14:31.717328: step 33680, loss = 0.72 (709.1 examples/sec; 0.181 sec/batch)
2016-10-15 03:14:33.471896: step 33690, loss = 0.67 (745.8 examples/sec; 0.172 sec/batch)
2016-10-15 03:14:35.226306: step 33700, loss = 0.88 (727.5 examples/sec; 0.176 sec/batch)
2016-10-15 03:14:37.175766: step 33710, loss = 0.76 (743.3 examples/sec; 0.172 sec/batch)
2016-10-15 03:14:38.946878: step 33720, loss = 0.76 (703.4 examples/sec; 0.182 sec/batch)
2016-10-15 03:14:40.702784: step 33730, loss = 0.68 (737.7 examples/sec; 0.174 sec/batch)
2016-10-15 03:14:42.459836: step 33740, loss = 0.67 (732.1 examples/sec; 0.175 sec/batch)
2016-10-15 03:14:44.225275: step 33750, loss = 0.71 (738.1 examples/sec; 0.173 sec/batch)
2016-10-15 03:14:45.990580: step 33760, loss = 0.71 (733.4 examples/sec; 0.175 sec/batch)
2016-10-15 03:14:47.748485: step 33770, loss = 0.71 (719.7 examples/sec; 0.178 sec/batch)
2016-10-15 03:14:49.509646: step 33780, loss = 0.84 (721.1 examples/sec; 0.178 sec/batch)
2016-10-15 03:14:51.267672: step 33790, loss = 0.78 (737.5 examples/sec; 0.174 sec/batch)
2016-10-15 03:14:53.030874: step 33800, loss = 0.80 (715.1 examples/sec; 0.179 sec/batch)
2016-10-15 03:14:54.972690: step 33810, loss = 0.86 (715.8 examples/sec; 0.179 sec/batch)
2016-10-15 03:14:56.737405: step 33820, loss = 0.75 (722.6 examples/sec; 0.177 sec/batch)
2016-10-15 03:14:58.501062: step 33830, loss = 0.90 (714.0 examples/sec; 0.179 sec/batch)
2016-10-15 03:15:00.261782: step 33840, loss = 0.70 (731.0 examples/sec; 0.175 sec/batch)
2016-10-15 03:15:02.031898: step 33850, loss = 0.86 (755.5 examples/sec; 0.169 sec/batch)
2016-10-15 03:15:03.796884: step 33860, loss = 0.91 (719.8 examples/sec; 0.178 sec/batch)
2016-10-15 03:15:05.565105: step 33870, loss = 0.82 (725.1 examples/sec; 0.177 sec/batch)
2016-10-15 03:15:07.336982: step 33880, loss = 0.79 (731.2 examples/sec; 0.175 sec/batch)
2016-10-15 03:15:09.105105: step 33890, loss = 0.86 (772.9 examples/sec; 0.166 sec/batch)
2016-10-15 03:15:10.872591: step 33900, loss = 0.72 (736.2 examples/sec; 0.174 sec/batch)
2016-10-15 03:15:12.819332: step 33910, loss = 0.79 (726.0 examples/sec; 0.176 sec/batch)
2016-10-15 03:15:14.593148: step 33920, loss = 0.86 (734.0 examples/sec; 0.174 sec/batch)
2016-10-15 03:15:16.361972: step 33930, loss = 0.76 (734.1 examples/sec; 0.174 sec/batch)
2016-10-15 03:15:18.125780: step 33940, loss = 0.89 (717.1 examples/sec; 0.178 sec/batch)
2016-10-15 03:15:19.883308: step 33950, loss = 0.83 (736.4 examples/sec; 0.174 sec/batch)
2016-10-15 03:15:21.642182: step 33960, loss = 0.85 (727.5 examples/sec; 0.176 sec/batch)
2016-10-15 03:15:23.405839: step 33970, loss = 0.75 (775.7 examples/sec; 0.165 sec/batch)
2016-10-15 03:15:25.163148: step 33980, loss = 0.72 (734.2 examples/sec; 0.174 sec/batch)
2016-10-15 03:15:26.913860: step 33990, loss = 0.92 (729.0 examples/sec; 0.176 sec/batch)
2016-10-15 03:15:28.679094: step 34000, loss = 0.81 (714.5 examples/sec; 0.179 sec/batch)
2016-10-15 03:15:31.192354: step 34010, loss = 0.69 (708.7 examples/sec; 0.181 sec/batch)
2016-10-15 03:15:32.958179: step 34020, loss = 0.88 (738.2 examples/sec; 0.173 sec/batch)
2016-10-15 03:15:34.735043: step 34030, loss = 0.67 (721.3 examples/sec; 0.177 sec/batch)
2016-10-15 03:15:36.495830: step 34040, loss = 0.73 (723.0 examples/sec; 0.177 sec/batch)
2016-10-15 03:15:38.245558: step 34050, loss = 0.77 (719.8 examples/sec; 0.178 sec/batch)
2016-10-15 03:15:40.008722: step 34060, loss = 0.68 (757.9 examples/sec; 0.169 sec/batch)
2016-10-15 03:15:41.781120: step 34070, loss = 0.71 (710.8 examples/sec; 0.180 sec/batch)
2016-10-15 03:15:43.548010: step 34080, loss = 1.06 (722.0 examples/sec; 0.177 sec/batch)
2016-10-15 03:15:45.313537: step 34090, loss = 0.72 (711.8 examples/sec; 0.180 sec/batch)
2016-10-15 03:15:47.066225: step 34100, loss = 0.83 (724.1 examples/sec; 0.177 sec/batch)
2016-10-15 03:15:49.007071: step 34110, loss = 0.99 (724.2 examples/sec; 0.177 sec/batch)
2016-10-15 03:15:50.763734: step 34120, loss = 0.85 (737.5 examples/sec; 0.174 sec/batch)
2016-10-15 03:15:52.530387: step 34130, loss = 0.87 (701.5 examples/sec; 0.182 sec/batch)
2016-10-15 03:15:54.288068: step 34140, loss = 0.70 (709.1 examples/sec; 0.181 sec/batch)
2016-10-15 03:15:56.047729: step 34150, loss = 0.64 (738.8 examples/sec; 0.173 sec/batch)
2016-10-15 03:15:57.805102: step 34160, loss = 0.79 (725.2 examples/sec; 0.177 sec/batch)
2016-10-15 03:15:59.564745: step 34170, loss = 0.99 (750.2 examples/sec; 0.171 sec/batch)
2016-10-15 03:16:01.325379: step 34180, loss = 0.87 (717.4 examples/sec; 0.178 sec/batch)
2016-10-15 03:16:03.089343: step 34190, loss = 0.80 (724.5 examples/sec; 0.177 sec/batch)
2016-10-15 03:16:04.848728: step 34200, loss = 0.81 (739.8 examples/sec; 0.173 sec/batch)
2016-10-15 03:16:06.803564: step 34210, loss = 0.82 (698.9 examples/sec; 0.183 sec/batch)
2016-10-15 03:16:08.558883: step 34220, loss = 1.05 (729.8 examples/sec; 0.175 sec/batch)
2016-10-15 03:16:10.313632: step 34230, loss = 0.58 (720.1 examples/sec; 0.178 sec/batch)
2016-10-15 03:16:12.071764: step 34240, loss = 0.78 (719.6 examples/sec; 0.178 sec/batch)
2016-10-15 03:16:13.837556: step 34250, loss = 0.69 (718.6 examples/sec; 0.178 sec/batch)
2016-10-15 03:16:15.596372: step 34260, loss = 0.76 (724.0 examples/sec; 0.177 sec/batch)
2016-10-15 03:16:17.365939: step 34270, loss = 0.71 (723.1 examples/sec; 0.177 sec/batch)
2016-10-15 03:16:19.109846: step 34280, loss = 0.89 (735.8 examples/sec; 0.174 sec/batch)
2016-10-15 03:16:20.865739: step 34290, loss = 0.87 (742.3 examples/sec; 0.172 sec/batch)
2016-10-15 03:16:22.616878: step 34300, loss = 0.72 (753.3 examples/sec; 0.170 sec/batch)
2016-10-15 03:16:24.556847: step 34310, loss = 1.01 (738.2 examples/sec; 0.173 sec/batch)
2016-10-15 03:16:26.312672: step 34320, loss = 0.68 (727.3 examples/sec; 0.176 sec/batch)
2016-10-15 03:16:28.068864: step 34330, loss = 0.81 (731.7 examples/sec; 0.175 sec/batch)
2016-10-15 03:16:29.822344: step 34340, loss = 0.79 (717.9 examples/sec; 0.178 sec/batch)
2016-10-15 03:16:31.578937: step 34350, loss = 0.70 (712.5 examples/sec; 0.180 sec/batch)
2016-10-15 03:16:33.340907: step 34360, loss = 0.70 (700.4 examples/sec; 0.183 sec/batch)
2016-10-15 03:16:35.099281: step 34370, loss = 0.88 (739.7 examples/sec; 0.173 sec/batch)
2016-10-15 03:16:36.857104: step 34380, loss = 0.99 (760.6 examples/sec; 0.168 sec/batch)
2016-10-15 03:16:38.621208: step 34390, loss = 0.69 (725.1 examples/sec; 0.177 sec/batch)
2016-10-15 03:16:40.376483: step 34400, loss = 0.66 (747.1 examples/sec; 0.171 sec/batch)
2016-10-15 03:16:42.317701: step 34410, loss = 0.70 (731.7 examples/sec; 0.175 sec/batch)
2016-10-15 03:16:44.075722: step 34420, loss = 0.74 (741.2 examples/sec; 0.173 sec/batch)
2016-10-15 03:16:45.844028: step 34430, loss = 0.80 (685.9 examples/sec; 0.187 sec/batch)
2016-10-15 03:16:47.588470: step 34440, loss = 0.71 (741.4 examples/sec; 0.173 sec/batch)
2016-10-15 03:16:49.345468: step 34450, loss = 0.75 (717.1 examples/sec; 0.178 sec/batch)
2016-10-15 03:16:51.098603: step 34460, loss = 0.66 (730.6 examples/sec; 0.175 sec/batch)
2016-10-15 03:16:52.863824: step 34470, loss = 0.95 (766.8 examples/sec; 0.167 sec/batch)
2016-10-15 03:16:54.620907: step 34480, loss = 0.98 (749.8 examples/sec; 0.171 sec/batch)
2016-10-15 03:16:56.375939: step 34490, loss = 0.93 (747.1 examples/sec; 0.171 sec/batch)
2016-10-15 03:16:58.135326: step 34500, loss = 0.81 (740.6 examples/sec; 0.173 sec/batch)
2016-10-15 03:17:00.077934: step 34510, loss = 0.96 (746.2 examples/sec; 0.172 sec/batch)
2016-10-15 03:17:01.832877: step 34520, loss = 0.75 (721.3 examples/sec; 0.177 sec/batch)
2016-10-15 03:17:03.596374: step 34530, loss = 0.67 (728.5 examples/sec; 0.176 sec/batch)
2016-10-15 03:17:05.361586: step 34540, loss = 0.94 (722.0 examples/sec; 0.177 sec/batch)
2016-10-15 03:17:07.113816: step 34550, loss = 0.90 (737.5 examples/sec; 0.174 sec/batch)
2016-10-15 03:17:08.879105: step 34560, loss = 0.68 (721.8 examples/sec; 0.177 sec/batch)
2016-10-15 03:17:10.629849: step 34570, loss = 0.61 (740.9 examples/sec; 0.173 sec/batch)
2016-10-15 03:17:12.391367: step 34580, loss = 0.93 (725.5 examples/sec; 0.176 sec/batch)
2016-10-15 03:17:14.157869: step 34590, loss = 0.73 (725.3 examples/sec; 0.176 sec/batch)
2016-10-15 03:17:15.922675: step 34600, loss = 0.82 (743.2 examples/sec; 0.172 sec/batch)
2016-10-15 03:17:17.869815: step 34610, loss = 0.85 (738.2 examples/sec; 0.173 sec/batch)
2016-10-15 03:17:19.632403: step 34620, loss = 0.82 (728.6 examples/sec; 0.176 sec/batch)
2016-10-15 03:17:21.399417: step 34630, loss = 0.83 (731.2 examples/sec; 0.175 sec/batch)
2016-10-15 03:17:23.161442: step 34640, loss = 0.78 (713.8 examples/sec; 0.179 sec/batch)
2016-10-15 03:17:24.915897: step 34650, loss = 0.72 (749.3 examples/sec; 0.171 sec/batch)
2016-10-15 03:17:26.680766: step 34660, loss = 0.63 (716.5 examples/sec; 0.179 sec/batch)
2016-10-15 03:17:28.446453: step 34670, loss = 0.78 (724.6 examples/sec; 0.177 sec/batch)
2016-10-15 03:17:30.207910: step 34680, loss = 0.77 (711.2 examples/sec; 0.180 sec/batch)
2016-10-15 03:17:31.983361: step 34690, loss = 0.88 (691.0 examples/sec; 0.185 sec/batch)
2016-10-15 03:17:33.736211: step 34700, loss = 0.83 (729.0 examples/sec; 0.176 sec/batch)
2016-10-15 03:17:35.693947: step 34710, loss = 0.76 (700.1 examples/sec; 0.183 sec/batch)
2016-10-15 03:17:37.449008: step 34720, loss = 0.75 (719.1 examples/sec; 0.178 sec/batch)
2016-10-15 03:17:39.208026: step 34730, loss = 0.85 (712.7 examples/sec; 0.180 sec/batch)
2016-10-15 03:17:40.967314: step 34740, loss = 0.69 (735.3 examples/sec; 0.174 sec/batch)
2016-10-15 03:17:42.734891: step 34750, loss = 0.69 (694.0 examples/sec; 0.184 sec/batch)
2016-10-15 03:17:44.486722: step 34760, loss = 0.72 (727.3 examples/sec; 0.176 sec/batch)
2016-10-15 03:17:46.241707: step 34770, loss = 0.72 (750.0 examples/sec; 0.171 sec/batch)
2016-10-15 03:17:47.995630: step 34780, loss = 0.60 (740.4 examples/sec; 0.173 sec/batch)
2016-10-15 03:17:49.760267: step 34790, loss = 0.64 (725.0 examples/sec; 0.177 sec/batch)
2016-10-15 03:17:51.513961: step 34800, loss = 0.85 (744.0 examples/sec; 0.172 sec/batch)
2016-10-15 03:17:53.459824: step 34810, loss = 0.74 (724.5 examples/sec; 0.177 sec/batch)
2016-10-15 03:17:55.214551: step 34820, loss = 0.76 (740.4 examples/sec; 0.173 sec/batch)
2016-10-15 03:17:56.977001: step 34830, loss = 0.72 (724.7 examples/sec; 0.177 sec/batch)
2016-10-15 03:17:58.730468: step 34840, loss = 0.64 (749.1 examples/sec; 0.171 sec/batch)
2016-10-15 03:18:00.490488: step 34850, loss = 0.87 (724.1 examples/sec; 0.177 sec/batch)
2016-10-15 03:18:02.247060: step 34860, loss = 0.89 (722.5 examples/sec; 0.177 sec/batch)
2016-10-15 03:18:04.016750: step 34870, loss = 0.89 (716.4 examples/sec; 0.179 sec/batch)
2016-10-15 03:18:05.777789: step 34880, loss = 0.69 (722.2 examples/sec; 0.177 sec/batch)
2016-10-15 03:18:07.525253: step 34890, loss = 0.75 (722.5 examples/sec; 0.177 sec/batch)
2016-10-15 03:18:09.278354: step 34900, loss = 0.61 (707.6 examples/sec; 0.181 sec/batch)
2016-10-15 03:18:11.221336: step 34910, loss = 0.74 (722.0 examples/sec; 0.177 sec/batch)
2016-10-15 03:18:12.986079: step 34920, loss = 0.78 (720.5 examples/sec; 0.178 sec/batch)
2016-10-15 03:18:14.745303: step 34930, loss = 0.80 (740.5 examples/sec; 0.173 sec/batch)
2016-10-15 03:18:16.507247: step 34940, loss = 0.74 (710.4 examples/sec; 0.180 sec/batch)
2016-10-15 03:18:18.257949: step 34950, loss = 0.75 (736.5 examples/sec; 0.174 sec/batch)
2016-10-15 03:18:20.013691: step 34960, loss = 0.69 (733.4 examples/sec; 0.175 sec/batch)
2016-10-15 03:18:21.773883: step 34970, loss = 0.73 (740.9 examples/sec; 0.173 sec/batch)
2016-10-15 03:18:23.526548: step 34980, loss = 0.70 (721.3 examples/sec; 0.177 sec/batch)
2016-10-15 03:18:25.282115: step 34990, loss = 0.85 (750.0 examples/sec; 0.171 sec/batch)
2016-10-15 03:18:27.044477: step 35000, loss = 0.82 (736.1 examples/sec; 0.174 sec/batch)
2016-10-15 03:18:29.481855: step 35010, loss = 0.73 (737.2 examples/sec; 0.174 sec/batch)
2016-10-15 03:18:31.236153: step 35020, loss = 0.76 (738.0 examples/sec; 0.173 sec/batch)
2016-10-15 03:18:33.003085: step 35030, loss = 0.79 (718.4 examples/sec; 0.178 sec/batch)
2016-10-15 03:18:34.758884: step 35040, loss = 0.87 (710.7 examples/sec; 0.180 sec/batch)
2016-10-15 03:18:36.511448: step 35050, loss = 0.69 (737.0 examples/sec; 0.174 sec/batch)
2016-10-15 03:18:38.266297: step 35060, loss = 0.88 (733.2 examples/sec; 0.175 sec/batch)
2016-10-15 03:18:40.020525: step 35070, loss = 0.79 (728.8 examples/sec; 0.176 sec/batch)
2016-10-15 03:18:41.780867: step 35080, loss = 0.80 (741.2 examples/sec; 0.173 sec/batch)
2016-10-15 03:18:43.533515: step 35090, loss = 0.77 (723.0 examples/sec; 0.177 sec/batch)
2016-10-15 03:18:45.289375: step 35100, loss = 0.66 (713.2 examples/sec; 0.179 sec/batch)
2016-10-15 03:18:47.225741: step 35110, loss = 0.71 (714.7 examples/sec; 0.179 sec/batch)
2016-10-15 03:18:48.991867: step 35120, loss = 0.87 (737.5 examples/sec; 0.174 sec/batch)
2016-10-15 03:18:50.750122: step 35130, loss = 0.67 (738.4 examples/sec; 0.173 sec/batch)
2016-10-15 03:18:52.508342: step 35140, loss = 0.72 (738.7 examples/sec; 0.173 sec/batch)
2016-10-15 03:18:54.272010: step 35150, loss = 0.84 (735.2 examples/sec; 0.174 sec/batch)
2016-10-15 03:18:56.032095: step 35160, loss = 0.60 (742.2 examples/sec; 0.172 sec/batch)
2016-10-15 03:18:57.795269: step 35170, loss = 0.77 (715.8 examples/sec; 0.179 sec/batch)
2016-10-15 03:18:59.559354: step 35180, loss = 0.71 (704.7 examples/sec; 0.182 sec/batch)
2016-10-15 03:19:01.312456: step 35190, loss = 0.68 (722.9 examples/sec; 0.177 sec/batch)
2016-10-15 03:19:03.079955: step 35200, loss = 0.56 (698.0 examples/sec; 0.183 sec/batch)
2016-10-15 03:19:05.027279: step 35210, loss = 0.72 (705.8 examples/sec; 0.181 sec/batch)
2016-10-15 03:19:06.782975: step 35220, loss = 0.66 (748.8 examples/sec; 0.171 sec/batch)
2016-10-15 03:19:08.548448: step 35230, loss = 0.82 (725.7 examples/sec; 0.176 sec/batch)
2016-10-15 03:19:10.306333: step 35240, loss = 1.00 (736.4 examples/sec; 0.174 sec/batch)
2016-10-15 03:19:12.055915: step 35250, loss = 0.70 (736.0 examples/sec; 0.174 sec/batch)
2016-10-15 03:19:13.817759: step 35260, loss = 0.72 (711.2 examples/sec; 0.180 sec/batch)
2016-10-15 03:19:15.579027: step 35270, loss = 0.86 (717.9 examples/sec; 0.178 sec/batch)
2016-10-15 03:19:17.330488: step 35280, loss = 0.66 (729.3 examples/sec; 0.176 sec/batch)
2016-10-15 03:19:19.080062: step 35290, loss = 0.65 (745.9 examples/sec; 0.172 sec/batch)
2016-10-15 03:19:20.844932: step 35300, loss = 0.69 (715.7 examples/sec; 0.179 sec/batch)
2016-10-15 03:19:22.796949: step 35310, loss = 0.74 (708.2 examples/sec; 0.181 sec/batch)
2016-10-15 03:19:24.565130: step 35320, loss = 0.75 (740.9 examples/sec; 0.173 sec/batch)
2016-10-15 03:19:26.332582: step 35330, loss = 0.72 (748.0 examples/sec; 0.171 sec/batch)
2016-10-15 03:19:28.093623: step 35340, loss = 0.74 (734.2 examples/sec; 0.174 sec/batch)
2016-10-15 03:19:29.858160: step 35350, loss = 0.73 (742.6 examples/sec; 0.172 sec/batch)
2016-10-15 03:19:31.613478: step 35360, loss = 0.70 (706.4 examples/sec; 0.181 sec/batch)
2016-10-15 03:19:33.374844: step 35370, loss = 0.95 (722.0 examples/sec; 0.177 sec/batch)
2016-10-15 03:19:35.129823: step 35380, loss = 0.80 (731.1 examples/sec; 0.175 sec/batch)
2016-10-15 03:19:36.889320: step 35390, loss = 0.73 (713.8 examples/sec; 0.179 sec/batch)
2016-10-15 03:19:38.646750: step 35400, loss = 0.72 (726.1 examples/sec; 0.176 sec/batch)
2016-10-15 03:19:40.588682: step 35410, loss = 0.66 (737.7 examples/sec; 0.174 sec/batch)
2016-10-15 03:19:42.356982: step 35420, loss = 0.66 (710.4 examples/sec; 0.180 sec/batch)
2016-10-15 03:19:44.119550: step 35430, loss = 0.74 (685.4 examples/sec; 0.187 sec/batch)
2016-10-15 03:19:45.870660: step 35440, loss = 0.97 (709.1 examples/sec; 0.181 sec/batch)
2016-10-15 03:19:47.630820: step 35450, loss = 0.77 (707.7 examples/sec; 0.181 sec/batch)
2016-10-15 03:19:49.396132: step 35460, loss = 0.87 (697.9 examples/sec; 0.183 sec/batch)
2016-10-15 03:19:51.144329: step 35470, loss = 0.80 (731.7 examples/sec; 0.175 sec/batch)
2016-10-15 03:19:52.904543: step 35480, loss = 0.90 (739.6 examples/sec; 0.173 sec/batch)
2016-10-15 03:19:54.667818: step 35490, loss = 0.74 (713.7 examples/sec; 0.179 sec/batch)
2016-10-15 03:19:56.418376: step 35500, loss = 0.81 (742.8 examples/sec; 0.172 sec/batch)
2016-10-15 03:19:58.357590: step 35510, loss = 0.76 (712.2 examples/sec; 0.180 sec/batch)
2016-10-15 03:20:00.110405: step 35520, loss = 0.73 (733.8 examples/sec; 0.174 sec/batch)
2016-10-15 03:20:01.866089: step 35530, loss = 0.87 (731.3 examples/sec; 0.175 sec/batch)
2016-10-15 03:20:03.660369: step 35540, loss = 0.91 (722.2 examples/sec; 0.177 sec/batch)
2016-10-15 03:20:05.419337: step 35550, loss = 0.86 (722.1 examples/sec; 0.177 sec/batch)
2016-10-15 03:20:07.176266: step 35560, loss = 0.66 (731.6 examples/sec; 0.175 sec/batch)
2016-10-15 03:20:08.925793: step 35570, loss = 0.84 (741.9 examples/sec; 0.173 sec/batch)
2016-10-15 03:20:10.692117: step 35580, loss = 0.77 (732.3 examples/sec; 0.175 sec/batch)
2016-10-15 03:20:12.450032: step 35590, loss = 0.83 (724.6 examples/sec; 0.177 sec/batch)
2016-10-15 03:20:14.213156: step 35600, loss = 0.77 (696.9 examples/sec; 0.184 sec/batch)
2016-10-15 03:20:16.139479: step 35610, loss = 0.87 (733.5 examples/sec; 0.175 sec/batch)
2016-10-15 03:20:17.906095: step 35620, loss = 0.76 (711.3 examples/sec; 0.180 sec/batch)
2016-10-15 03:20:19.651904: step 35630, loss = 0.65 (738.2 examples/sec; 0.173 sec/batch)
2016-10-15 03:20:21.414913: step 35640, loss = 0.73 (731.4 examples/sec; 0.175 sec/batch)
2016-10-15 03:20:23.172572: step 35650, loss = 0.62 (718.7 examples/sec; 0.178 sec/batch)
2016-10-15 03:20:24.932093: step 35660, loss = 0.72 (739.0 examples/sec; 0.173 sec/batch)
2016-10-15 03:20:26.692860: step 35670, loss = 0.56 (732.7 examples/sec; 0.175 sec/batch)
2016-10-15 03:20:28.457734: step 35680, loss = 0.76 (753.1 examples/sec; 0.170 sec/batch)
2016-10-15 03:20:30.243661: step 35690, loss = 0.94 (732.6 examples/sec; 0.175 sec/batch)
2016-10-15 03:20:32.017402: step 35700, loss = 0.69 (724.3 examples/sec; 0.177 sec/batch)
2016-10-15 03:20:33.963493: step 35710, loss = 0.82 (732.9 examples/sec; 0.175 sec/batch)
2016-10-15 03:20:35.727899: step 35720, loss = 0.75 (718.3 examples/sec; 0.178 sec/batch)
2016-10-15 03:20:37.490629: step 35730, loss = 0.60 (726.7 examples/sec; 0.176 sec/batch)
2016-10-15 03:20:39.240922: step 35740, loss = 0.78 (714.8 examples/sec; 0.179 sec/batch)
2016-10-15 03:20:40.999990: step 35750, loss = 0.89 (755.4 examples/sec; 0.169 sec/batch)
2016-10-15 03:20:42.750897: step 35760, loss = 0.66 (754.6 examples/sec; 0.170 sec/batch)
2016-10-15 03:20:44.507380: step 35770, loss = 0.86 (717.8 examples/sec; 0.178 sec/batch)
2016-10-15 03:20:46.274922: step 35780, loss = 0.70 (701.8 examples/sec; 0.182 sec/batch)
2016-10-15 03:20:48.021877: step 35790, loss = 0.80 (745.3 examples/sec; 0.172 sec/batch)
2016-10-15 03:20:49.788440: step 35800, loss = 0.80 (713.6 examples/sec; 0.179 sec/batch)
2016-10-15 03:20:51.740150: step 35810, loss = 0.58 (749.9 examples/sec; 0.171 sec/batch)
2016-10-15 03:20:53.509017: step 35820, loss = 0.89 (713.8 examples/sec; 0.179 sec/batch)
2016-10-15 03:20:55.263682: step 35830, loss = 0.73 (730.2 examples/sec; 0.175 sec/batch)
2016-10-15 03:20:57.027827: step 35840, loss = 0.86 (727.6 examples/sec; 0.176 sec/batch)
2016-10-15 03:20:58.791980: step 35850, loss = 0.84 (743.9 examples/sec; 0.172 sec/batch)
2016-10-15 03:21:00.557805: step 35860, loss = 0.79 (722.4 examples/sec; 0.177 sec/batch)
2016-10-15 03:21:02.322941: step 35870, loss = 0.85 (731.8 examples/sec; 0.175 sec/batch)
2016-10-15 03:21:04.091918: step 35880, loss = 0.68 (699.5 examples/sec; 0.183 sec/batch)
2016-10-15 03:21:05.854625: step 35890, loss = 0.73 (699.7 examples/sec; 0.183 sec/batch)
2016-10-15 03:21:07.618440: step 35900, loss = 0.80 (698.5 examples/sec; 0.183 sec/batch)
2016-10-15 03:21:09.553564: step 35910, loss = 0.86 (745.3 examples/sec; 0.172 sec/batch)
2016-10-15 03:21:11.309215: step 35920, loss = 0.91 (737.4 examples/sec; 0.174 sec/batch)
2016-10-15 03:21:13.070132: step 35930, loss = 0.77 (712.4 examples/sec; 0.180 sec/batch)
2016-10-15 03:21:14.823489: step 35940, loss = 0.77 (722.7 examples/sec; 0.177 sec/batch)
2016-10-15 03:21:16.584323: step 35950, loss = 0.64 (731.2 examples/sec; 0.175 sec/batch)
2016-10-15 03:21:18.344866: step 35960, loss = 0.82 (716.9 examples/sec; 0.179 sec/batch)
2016-10-15 03:21:20.099242: step 35970, loss = 0.83 (734.1 examples/sec; 0.174 sec/batch)
2016-10-15 03:21:21.864813: step 35980, loss = 0.71 (733.9 examples/sec; 0.174 sec/batch)
2016-10-15 03:21:23.618440: step 35990, loss = 0.67 (733.2 examples/sec; 0.175 sec/batch)
2016-10-15 03:21:25.386831: step 36000, loss = 0.73 (719.5 examples/sec; 0.178 sec/batch)
2016-10-15 03:21:27.894048: step 36010, loss = 0.79 (720.8 examples/sec; 0.178 sec/batch)
2016-10-15 03:21:29.657596: step 36020, loss = 0.88 (714.2 examples/sec; 0.179 sec/batch)
2016-10-15 03:21:31.426449: step 36030, loss = 0.70 (692.9 examples/sec; 0.185 sec/batch)
2016-10-15 03:21:33.180454: step 36040, loss = 0.70 (717.0 examples/sec; 0.179 sec/batch)
2016-10-15 03:21:34.935977: step 36050, loss = 0.83 (713.9 examples/sec; 0.179 sec/batch)
2016-10-15 03:21:36.689117: step 36060, loss = 0.75 (718.8 examples/sec; 0.178 sec/batch)
2016-10-15 03:21:38.443511: step 36070, loss = 0.71 (731.3 examples/sec; 0.175 sec/batch)
2016-10-15 03:21:40.208251: step 36080, loss = 0.88 (724.6 examples/sec; 0.177 sec/batch)
2016-10-15 03:21:41.962791: step 36090, loss = 0.76 (724.5 examples/sec; 0.177 sec/batch)
2016-10-15 03:21:43.720451: step 36100, loss = 0.70 (733.8 examples/sec; 0.174 sec/batch)
2016-10-15 03:21:45.662030: step 36110, loss = 0.81 (737.6 examples/sec; 0.174 sec/batch)
2016-10-15 03:21:47.413804: step 36120, loss = 0.66 (742.6 examples/sec; 0.172 sec/batch)
2016-10-15 03:21:49.178998: step 36130, loss = 0.75 (742.7 examples/sec; 0.172 sec/batch)
2016-10-15 03:21:50.933318: step 36140, loss = 0.61 (752.9 examples/sec; 0.170 sec/batch)
2016-10-15 03:21:52.693865: step 36150, loss = 0.64 (737.5 examples/sec; 0.174 sec/batch)
2016-10-15 03:21:54.448688: step 36160, loss = 0.78 (733.3 examples/sec; 0.175 sec/batch)
2016-10-15 03:21:56.198963: step 36170, loss = 0.70 (724.9 examples/sec; 0.177 sec/batch)
2016-10-15 03:21:57.956132: step 36180, loss = 0.79 (728.3 examples/sec; 0.176 sec/batch)
2016-10-15 03:21:59.706043: step 36190, loss = 0.73 (766.1 examples/sec; 0.167 sec/batch)
2016-10-15 03:22:01.465946: step 36200, loss = 0.92 (728.3 examples/sec; 0.176 sec/batch)
2016-10-15 03:22:03.426813: step 36210, loss = 0.72 (710.2 examples/sec; 0.180 sec/batch)
2016-10-15 03:22:05.189113: step 36220, loss = 0.69 (734.7 examples/sec; 0.174 sec/batch)
2016-10-15 03:22:06.953826: step 36230, loss = 0.93 (752.3 examples/sec; 0.170 sec/batch)
2016-10-15 03:22:08.714147: step 36240, loss = 0.67 (716.5 examples/sec; 0.179 sec/batch)
2016-10-15 03:22:10.472656: step 36250, loss = 0.81 (729.3 examples/sec; 0.176 sec/batch)
2016-10-15 03:22:12.240791: step 36260, loss = 0.90 (724.6 examples/sec; 0.177 sec/batch)
2016-10-15 03:22:13.997992: step 36270, loss = 0.73 (739.6 examples/sec; 0.173 sec/batch)
2016-10-15 03:22:15.753997: step 36280, loss = 0.96 (749.6 examples/sec; 0.171 sec/batch)
2016-10-15 03:22:17.518732: step 36290, loss = 0.81 (712.8 examples/sec; 0.180 sec/batch)
2016-10-15 03:22:19.284108: step 36300, loss = 0.74 (697.7 examples/sec; 0.183 sec/batch)
2016-10-15 03:22:21.231298: step 36310, loss = 0.66 (721.9 examples/sec; 0.177 sec/batch)
2016-10-15 03:22:22.984161: step 36320, loss = 0.81 (716.8 examples/sec; 0.179 sec/batch)
2016-10-15 03:22:24.754476: step 36330, loss = 0.85 (707.0 examples/sec; 0.181 sec/batch)
2016-10-15 03:22:26.521975: step 36340, loss = 0.88 (679.1 examples/sec; 0.188 sec/batch)
2016-10-15 03:22:28.269697: step 36350, loss = 0.63 (750.0 examples/sec; 0.171 sec/batch)
2016-10-15 03:22:30.037788: step 36360, loss = 0.63 (693.6 examples/sec; 0.185 sec/batch)
2016-10-15 03:22:31.799867: step 36370, loss = 0.77 (697.3 examples/sec; 0.184 sec/batch)
2016-10-15 03:22:33.556039: step 36380, loss = 0.88 (716.2 examples/sec; 0.179 sec/batch)
2016-10-15 03:22:35.317028: step 36390, loss = 0.75 (720.2 examples/sec; 0.178 sec/batch)
2016-10-15 03:22:37.080432: step 36400, loss = 0.83 (699.4 examples/sec; 0.183 sec/batch)
2016-10-15 03:22:39.023344: step 36410, loss = 0.79 (725.2 examples/sec; 0.176 sec/batch)
2016-10-15 03:22:40.783181: step 36420, loss = 0.82 (724.6 examples/sec; 0.177 sec/batch)
2016-10-15 03:22:42.540013: step 36430, loss = 0.72 (733.7 examples/sec; 0.174 sec/batch)
2016-10-15 03:22:44.305000: step 36440, loss = 0.92 (775.4 examples/sec; 0.165 sec/batch)
2016-10-15 03:22:46.057244: step 36450, loss = 0.85 (737.9 examples/sec; 0.173 sec/batch)
2016-10-15 03:22:47.810182: step 36460, loss = 0.72 (729.3 examples/sec; 0.176 sec/batch)
2016-10-15 03:22:49.567064: step 36470, loss = 0.73 (714.0 examples/sec; 0.179 sec/batch)
2016-10-15 03:22:51.322101: step 36480, loss = 0.76 (730.5 examples/sec; 0.175 sec/batch)
2016-10-15 03:22:53.091238: step 36490, loss = 0.72 (699.9 examples/sec; 0.183 sec/batch)
2016-10-15 03:22:54.835900: step 36500, loss = 0.78 (740.4 examples/sec; 0.173 sec/batch)
2016-10-15 03:22:56.782888: step 36510, loss = 0.76 (818.2 examples/sec; 0.156 sec/batch)
2016-10-15 03:22:58.547491: step 36520, loss = 0.62 (705.6 examples/sec; 0.181 sec/batch)
2016-10-15 03:23:00.311573: step 36530, loss = 0.88 (722.9 examples/sec; 0.177 sec/batch)
2016-10-15 03:23:02.060544: step 36540, loss = 0.80 (743.3 examples/sec; 0.172 sec/batch)
2016-10-15 03:23:03.827548: step 36550, loss = 0.83 (726.1 examples/sec; 0.176 sec/batch)
2016-10-15 03:23:05.590638: step 36560, loss = 0.87 (723.8 examples/sec; 0.177 sec/batch)
2016-10-15 03:23:07.352178: step 36570, loss = 0.83 (737.8 examples/sec; 0.173 sec/batch)
2016-10-15 03:23:09.112323: step 36580, loss = 0.65 (748.6 examples/sec; 0.171 sec/batch)
2016-10-15 03:23:10.875546: step 36590, loss = 0.84 (737.2 examples/sec; 0.174 sec/batch)
2016-10-15 03:23:12.636690: step 36600, loss = 0.68 (737.5 examples/sec; 0.174 sec/batch)
2016-10-15 03:23:14.567925: step 36610, loss = 0.58 (740.5 examples/sec; 0.173 sec/batch)
2016-10-15 03:23:16.331683: step 36620, loss = 0.71 (719.6 examples/sec; 0.178 sec/batch)
2016-10-15 03:23:18.089472: step 36630, loss = 0.74 (753.1 examples/sec; 0.170 sec/batch)
2016-10-15 03:23:19.847782: step 36640, loss = 0.70 (726.1 examples/sec; 0.176 sec/batch)
2016-10-15 03:23:21.600150: step 36650, loss = 0.86 (726.3 examples/sec; 0.176 sec/batch)
2016-10-15 03:23:23.365650: step 36660, loss = 0.77 (752.6 examples/sec; 0.170 sec/batch)
2016-10-15 03:23:25.143763: step 36670, loss = 0.73 (705.7 examples/sec; 0.181 sec/batch)
2016-10-15 03:23:26.899267: step 36680, loss = 0.68 (714.8 examples/sec; 0.179 sec/batch)
2016-10-15 03:23:28.663690: step 36690, loss = 0.80 (728.2 examples/sec; 0.176 sec/batch)
2016-10-15 03:23:30.430264: step 36700, loss = 0.84 (723.2 examples/sec; 0.177 sec/batch)
2016-10-15 03:23:32.385131: step 36710, loss = 0.78 (709.2 examples/sec; 0.180 sec/batch)
2016-10-15 03:23:34.144454: step 36720, loss = 1.08 (708.0 examples/sec; 0.181 sec/batch)
2016-10-15 03:23:35.894518: step 36730, loss = 0.69 (732.3 examples/sec; 0.175 sec/batch)
2016-10-15 03:23:37.664668: step 36740, loss = 0.65 (715.0 examples/sec; 0.179 sec/batch)
2016-10-15 03:23:39.424715: step 36750, loss = 0.75 (717.8 examples/sec; 0.178 sec/batch)
2016-10-15 03:23:41.189254: step 36760, loss = 0.66 (716.0 examples/sec; 0.179 sec/batch)
2016-10-15 03:23:42.945280: step 36770, loss = 0.63 (745.6 examples/sec; 0.172 sec/batch)
2016-10-15 03:23:44.708446: step 36780, loss = 0.70 (742.0 examples/sec; 0.173 sec/batch)
2016-10-15 03:23:46.471407: step 36790, loss = 0.71 (708.6 examples/sec; 0.181 sec/batch)
2016-10-15 03:23:48.227685: step 36800, loss = 0.86 (732.8 examples/sec; 0.175 sec/batch)
2016-10-15 03:23:50.172338: step 36810, loss = 0.70 (746.1 examples/sec; 0.172 sec/batch)
2016-10-15 03:23:51.927481: step 36820, loss = 0.74 (714.0 examples/sec; 0.179 sec/batch)
2016-10-15 03:23:53.691027: step 36830, loss = 0.68 (708.2 examples/sec; 0.181 sec/batch)
2016-10-15 03:23:55.456005: step 36840, loss = 0.79 (717.1 examples/sec; 0.178 sec/batch)
2016-10-15 03:23:57.223638: step 36850, loss = 0.77 (719.1 examples/sec; 0.178 sec/batch)
2016-10-15 03:23:58.980850: step 36860, loss = 0.74 (727.3 examples/sec; 0.176 sec/batch)
2016-10-15 03:24:00.739748: step 36870, loss = 0.90 (718.4 examples/sec; 0.178 sec/batch)
2016-10-15 03:24:02.501575: step 36880, loss = 0.76 (719.9 examples/sec; 0.178 sec/batch)
2016-10-15 03:24:04.254511: step 36890, loss = 0.89 (713.2 examples/sec; 0.179 sec/batch)
2016-10-15 03:24:06.013437: step 36900, loss = 0.81 (732.9 examples/sec; 0.175 sec/batch)
2016-10-15 03:24:07.961590: step 36910, loss = 0.81 (757.4 examples/sec; 0.169 sec/batch)
2016-10-15 03:24:09.721943: step 36920, loss = 0.81 (725.9 examples/sec; 0.176 sec/batch)
2016-10-15 03:24:11.481426: step 36930, loss = 0.70 (713.1 examples/sec; 0.180 sec/batch)
2016-10-15 03:24:13.245714: step 36940, loss = 0.75 (723.4 examples/sec; 0.177 sec/batch)
2016-10-15 03:24:15.015487: step 36950, loss = 0.74 (707.7 examples/sec; 0.181 sec/batch)
2016-10-15 03:24:16.772743: step 36960, loss = 0.65 (742.0 examples/sec; 0.173 sec/batch)
2016-10-15 03:24:18.538465: step 36970, loss = 0.99 (723.3 examples/sec; 0.177 sec/batch)
2016-10-15 03:24:20.298687: step 36980, loss = 0.80 (725.4 examples/sec; 0.176 sec/batch)
2016-10-15 03:24:22.050055: step 36990, loss = 0.77 (717.8 examples/sec; 0.178 sec/batch)
2016-10-15 03:24:23.810115: step 37000, loss = 0.82 (724.2 examples/sec; 0.177 sec/batch)
2016-10-15 03:24:26.392982: step 37010, loss = 0.75 (703.4 examples/sec; 0.182 sec/batch)
2016-10-15 03:24:28.159723: step 37020, loss = 0.75 (715.9 examples/sec; 0.179 sec/batch)
2016-10-15 03:24:29.924713: step 37030, loss = 0.75 (730.5 examples/sec; 0.175 sec/batch)
2016-10-15 03:24:31.683898: step 37040, loss = 0.75 (763.1 examples/sec; 0.168 sec/batch)
2016-10-15 03:24:33.451848: step 37050, loss = 0.75 (692.1 examples/sec; 0.185 sec/batch)
2016-10-15 03:24:35.196668: step 37060, loss = 0.73 (736.1 examples/sec; 0.174 sec/batch)
2016-10-15 03:24:36.956993: step 37070, loss = 0.76 (725.3 examples/sec; 0.176 sec/batch)
2016-10-15 03:24:38.717928: step 37080, loss = 0.62 (718.1 examples/sec; 0.178 sec/batch)
2016-10-15 03:24:40.474277: step 37090, loss = 0.88 (749.7 examples/sec; 0.171 sec/batch)
2016-10-15 03:24:42.231172: step 37100, loss = 0.63 (729.8 examples/sec; 0.175 sec/batch)
2016-10-15 03:24:44.193114: step 37110, loss = 0.87 (740.3 examples/sec; 0.173 sec/batch)
2016-10-15 03:24:45.948687: step 37120, loss = 0.64 (740.6 examples/sec; 0.173 sec/batch)
2016-10-15 03:24:47.706216: step 37130, loss = 0.67 (721.4 examples/sec; 0.177 sec/batch)
2016-10-15 03:24:49.474039: step 37140, loss = 0.74 (769.3 examples/sec; 0.166 sec/batch)
2016-10-15 03:24:51.235349: step 37150, loss = 0.69 (719.7 examples/sec; 0.178 sec/batch)
2016-10-15 03:24:53.001801: step 37160, loss = 0.70 (715.4 examples/sec; 0.179 sec/batch)
2016-10-15 03:24:54.765937: step 37170, loss = 0.76 (723.9 examples/sec; 0.177 sec/batch)
2016-10-15 03:24:56.526374: step 37180, loss = 0.72 (721.9 examples/sec; 0.177 sec/batch)
2016-10-15 03:24:58.287262: step 37190, loss = 0.67 (731.0 examples/sec; 0.175 sec/batch)
2016-10-15 03:25:00.049677: step 37200, loss = 0.69 (726.4 examples/sec; 0.176 sec/batch)
2016-10-15 03:25:02.001236: step 37210, loss = 0.66 (684.2 examples/sec; 0.187 sec/batch)
2016-10-15 03:25:03.761845: step 37220, loss = 0.76 (738.9 examples/sec; 0.173 sec/batch)
2016-10-15 03:25:05.529715: step 37230, loss = 0.70 (736.0 examples/sec; 0.174 sec/batch)
2016-10-15 03:25:07.290590: step 37240, loss = 0.74 (748.8 examples/sec; 0.171 sec/batch)
2016-10-15 03:25:09.054370: step 37250, loss = 0.70 (731.6 examples/sec; 0.175 sec/batch)
2016-10-15 03:25:10.815857: step 37260, loss = 0.70 (728.2 examples/sec; 0.176 sec/batch)
2016-10-15 03:25:12.570909: step 37270, loss = 0.84 (754.9 examples/sec; 0.170 sec/batch)
2016-10-15 03:25:14.323975: step 37280, loss = 0.70 (716.1 examples/sec; 0.179 sec/batch)
2016-10-15 03:25:16.102183: step 37290, loss = 0.70 (691.2 examples/sec; 0.185 sec/batch)
2016-10-15 03:25:17.853680: step 37300, loss = 0.71 (722.9 examples/sec; 0.177 sec/batch)
2016-10-15 03:25:19.806644: step 37310, loss = 0.77 (688.1 examples/sec; 0.186 sec/batch)
2016-10-15 03:25:21.562601: step 37320, loss = 0.63 (727.0 examples/sec; 0.176 sec/batch)
2016-10-15 03:25:23.325246: step 37330, loss = 0.87 (731.9 examples/sec; 0.175 sec/batch)
2016-10-15 03:25:25.091216: step 37340, loss = 0.78 (731.7 examples/sec; 0.175 sec/batch)
2016-10-15 03:25:26.862496: step 37350, loss = 0.66 (698.4 examples/sec; 0.183 sec/batch)
2016-10-15 03:25:28.613514: step 37360, loss = 0.79 (732.3 examples/sec; 0.175 sec/batch)
2016-10-15 03:25:30.382805: step 37370, loss = 0.70 (744.8 examples/sec; 0.172 sec/batch)
2016-10-15 03:25:32.144003: step 37380, loss = 0.75 (723.2 examples/sec; 0.177 sec/batch)
2016-10-15 03:25:33.913818: step 37390, loss = 0.66 (712.9 examples/sec; 0.180 sec/batch)
2016-10-15 03:25:35.679140: step 37400, loss = 0.82 (723.1 examples/sec; 0.177 sec/batch)
2016-10-15 03:25:37.621489: step 37410, loss = 0.82 (735.5 examples/sec; 0.174 sec/batch)
2016-10-15 03:25:39.390075: step 37420, loss = 0.91 (728.2 examples/sec; 0.176 sec/batch)
2016-10-15 03:25:41.159890: step 37430, loss = 0.78 (736.5 examples/sec; 0.174 sec/batch)
2016-10-15 03:25:42.919683: step 37440, loss = 0.77 (715.6 examples/sec; 0.179 sec/batch)
2016-10-15 03:25:44.674194: step 37450, loss = 0.82 (726.1 examples/sec; 0.176 sec/batch)
2016-10-15 03:25:46.440391: step 37460, loss = 0.96 (736.5 examples/sec; 0.174 sec/batch)
2016-10-15 03:25:48.200016: step 37470, loss = 0.69 (742.3 examples/sec; 0.172 sec/batch)
2016-10-15 03:25:49.965177: step 37480, loss = 0.65 (726.6 examples/sec; 0.176 sec/batch)
2016-10-15 03:25:51.726161: step 37490, loss = 0.79 (729.9 examples/sec; 0.175 sec/batch)
2016-10-15 03:25:53.488320: step 37500, loss = 0.65 (737.8 examples/sec; 0.173 sec/batch)
2016-10-15 03:25:55.443792: step 37510, loss = 0.77 (707.4 examples/sec; 0.181 sec/batch)
2016-10-15 03:25:57.204008: step 37520, loss = 0.64 (722.0 examples/sec; 0.177 sec/batch)
2016-10-15 03:25:58.972528: step 37530, loss = 0.75 (755.0 examples/sec; 0.170 sec/batch)
2016-10-15 03:26:00.735327: step 37540, loss = 0.92 (719.6 examples/sec; 0.178 sec/batch)
2016-10-15 03:26:02.495263: step 37550, loss = 0.84 (737.2 examples/sec; 0.174 sec/batch)
2016-10-15 03:26:04.255472: step 37560, loss = 0.83 (714.0 examples/sec; 0.179 sec/batch)
2016-10-15 03:26:06.013597: step 37570, loss = 0.80 (765.9 examples/sec; 0.167 sec/batch)
2016-10-15 03:26:07.773263: step 37580, loss = 0.80 (701.4 examples/sec; 0.182 sec/batch)
2016-10-15 03:26:09.530718: step 37590, loss = 0.87 (724.4 examples/sec; 0.177 sec/batch)
2016-10-15 03:26:11.291740: step 37600, loss = 0.85 (731.3 examples/sec; 0.175 sec/batch)
2016-10-15 03:26:13.237558: step 37610, loss = 0.64 (716.0 examples/sec; 0.179 sec/batch)
2016-10-15 03:26:14.984488: step 37620, loss = 0.82 (756.7 examples/sec; 0.169 sec/batch)
2016-10-15 03:26:16.744202: step 37630, loss = 0.74 (735.4 examples/sec; 0.174 sec/batch)
2016-10-15 03:26:18.507322: step 37640, loss = 0.67 (704.6 examples/sec; 0.182 sec/batch)
2016-10-15 03:26:20.263424: step 37650, loss = 0.71 (737.0 examples/sec; 0.174 sec/batch)
2016-10-15 03:26:22.025477: step 37660, loss = 0.80 (713.6 examples/sec; 0.179 sec/batch)
2016-10-15 03:26:23.782845: step 37670, loss = 0.71 (737.8 examples/sec; 0.173 sec/batch)
2016-10-15 03:26:25.543190: step 37680, loss = 0.85 (724.3 examples/sec; 0.177 sec/batch)
2016-10-15 03:26:27.303639: step 37690, loss = 0.69 (722.7 examples/sec; 0.177 sec/batch)
2016-10-15 03:26:29.065401: step 37700, loss = 0.92 (741.3 examples/sec; 0.173 sec/batch)
2016-10-15 03:26:31.007740: step 37710, loss = 0.85 (734.0 examples/sec; 0.174 sec/batch)
2016-10-15 03:26:32.769647: step 37720, loss = 0.83 (731.8 examples/sec; 0.175 sec/batch)
2016-10-15 03:26:34.526391: step 37730, loss = 0.78 (713.2 examples/sec; 0.179 sec/batch)
2016-10-15 03:26:36.285239: step 37740, loss = 0.58 (732.1 examples/sec; 0.175 sec/batch)
2016-10-15 03:26:38.042835: step 37750, loss = 0.77 (731.3 examples/sec; 0.175 sec/batch)
2016-10-15 03:26:39.804050: step 37760, loss = 0.82 (738.8 examples/sec; 0.173 sec/batch)
2016-10-15 03:26:41.563673: step 37770, loss = 0.62 (722.0 examples/sec; 0.177 sec/batch)
2016-10-15 03:26:43.319162: step 37780, loss = 0.78 (723.7 examples/sec; 0.177 sec/batch)
2016-10-15 03:26:45.083126: step 37790, loss = 0.79 (730.1 examples/sec; 0.175 sec/batch)
2016-10-15 03:26:46.836983: step 37800, loss = 0.72 (771.5 examples/sec; 0.166 sec/batch)
2016-10-15 03:26:48.803755: step 37810, loss = 0.76 (722.4 examples/sec; 0.177 sec/batch)
2016-10-15 03:26:50.562185: step 37820, loss = 0.62 (723.7 examples/sec; 0.177 sec/batch)
2016-10-15 03:26:52.330094: step 37830, loss = 0.70 (717.2 examples/sec; 0.178 sec/batch)
2016-10-15 03:26:54.086973: step 37840, loss = 0.74 (733.2 examples/sec; 0.175 sec/batch)
2016-10-15 03:26:55.841574: step 37850, loss = 0.79 (736.9 examples/sec; 0.174 sec/batch)
2016-10-15 03:26:57.599346: step 37860, loss = 0.75 (753.0 examples/sec; 0.170 sec/batch)
2016-10-15 03:26:59.360504: step 37870, loss = 0.66 (713.4 examples/sec; 0.179 sec/batch)
2016-10-15 03:27:01.115368: step 37880, loss = 0.62 (724.8 examples/sec; 0.177 sec/batch)
2016-10-15 03:27:02.877098: step 37890, loss = 0.75 (730.5 examples/sec; 0.175 sec/batch)
2016-10-15 03:27:04.634685: step 37900, loss = 0.84 (727.8 examples/sec; 0.176 sec/batch)
2016-10-15 03:27:06.575984: step 37910, loss = 0.70 (699.7 examples/sec; 0.183 sec/batch)
2016-10-15 03:27:08.335731: step 37920, loss = 0.69 (707.8 examples/sec; 0.181 sec/batch)
2016-10-15 03:27:10.095413: step 37930, loss = 0.77 (727.2 examples/sec; 0.176 sec/batch)
2016-10-15 03:27:11.855675: step 37940, loss = 0.77 (701.2 examples/sec; 0.183 sec/batch)
2016-10-15 03:27:13.609891: step 37950, loss = 0.77 (724.8 examples/sec; 0.177 sec/batch)
2016-10-15 03:27:15.371472: step 37960, loss = 0.88 (713.3 examples/sec; 0.179 sec/batch)
2016-10-15 03:27:17.125113: step 37970, loss = 0.81 (749.6 examples/sec; 0.171 sec/batch)
2016-10-15 03:27:18.877282: step 37980, loss = 0.67 (735.2 examples/sec; 0.174 sec/batch)
2016-10-15 03:27:20.636967: step 37990, loss = 0.72 (723.2 examples/sec; 0.177 sec/batch)
2016-10-15 03:27:22.392088: step 38000, loss = 0.59 (735.5 examples/sec; 0.174 sec/batch)
2016-10-15 03:27:24.849809: step 38010, loss = 0.70 (712.7 examples/sec; 0.180 sec/batch)
2016-10-15 03:27:26.612533: step 38020, loss = 0.71 (715.1 examples/sec; 0.179 sec/batch)
2016-10-15 03:27:28.366827: step 38030, loss = 0.74 (722.3 examples/sec; 0.177 sec/batch)
2016-10-15 03:27:30.133960: step 38040, loss = 1.02 (701.4 examples/sec; 0.182 sec/batch)
2016-10-15 03:27:31.894002: step 38050, loss = 0.92 (762.0 examples/sec; 0.168 sec/batch)
2016-10-15 03:27:33.665090: step 38060, loss = 0.73 (714.1 examples/sec; 0.179 sec/batch)
2016-10-15 03:27:35.430861: step 38070, loss = 0.76 (702.4 examples/sec; 0.182 sec/batch)
2016-10-15 03:27:37.181200: step 38080, loss = 0.73 (723.6 examples/sec; 0.177 sec/batch)
2016-10-15 03:27:38.944461: step 38090, loss = 0.70 (701.8 examples/sec; 0.182 sec/batch)
2016-10-15 03:27:40.689266: step 38100, loss = 0.70 (771.2 examples/sec; 0.166 sec/batch)
2016-10-15 03:27:42.635241: step 38110, loss = 0.79 (718.4 examples/sec; 0.178 sec/batch)
2016-10-15 03:27:44.400801: step 38120, loss = 0.82 (699.3 examples/sec; 0.183 sec/batch)
2016-10-15 03:27:46.160189: step 38130, loss = 0.71 (685.6 examples/sec; 0.187 sec/batch)
2016-10-15 03:27:47.913907: step 38140, loss = 0.71 (721.5 examples/sec; 0.177 sec/batch)
2016-10-15 03:27:49.675490: step 38150, loss = 0.84 (736.8 examples/sec; 0.174 sec/batch)
2016-10-15 03:27:51.437929: step 38160, loss = 0.89 (717.9 examples/sec; 0.178 sec/batch)
2016-10-15 03:27:53.204491: step 38170, loss = 0.91 (742.2 examples/sec; 0.172 sec/batch)
2016-10-15 03:27:54.967790: step 38180, loss = 0.67 (707.7 examples/sec; 0.181 sec/batch)
2016-10-15 03:27:56.729364: step 38190, loss = 0.76 (737.5 examples/sec; 0.174 sec/batch)
2016-10-15 03:27:58.480922: step 38200, loss = 0.62 (724.1 examples/sec; 0.177 sec/batch)
2016-10-15 03:28:00.435333: step 38210, loss = 0.75 (715.2 examples/sec; 0.179 sec/batch)
2016-10-15 03:28:02.196843: step 38220, loss = 0.79 (741.9 examples/sec; 0.173 sec/batch)
2016-10-15 03:28:03.960377: step 38230, loss = 0.68 (727.2 examples/sec; 0.176 sec/batch)
2016-10-15 03:28:05.717914: step 38240, loss = 0.79 (714.9 examples/sec; 0.179 sec/batch)
2016-10-15 03:28:07.496758: step 38250, loss = 0.66 (706.7 examples/sec; 0.181 sec/batch)
2016-10-15 03:28:09.276678: step 38260, loss = 0.72 (752.7 examples/sec; 0.170 sec/batch)
2016-10-15 03:28:11.029223: step 38270, loss = 0.84 (725.0 examples/sec; 0.177 sec/batch)
2016-10-15 03:28:12.787809: step 38280, loss = 0.76 (715.6 examples/sec; 0.179 sec/batch)
2016-10-15 03:28:14.550748: step 38290, loss = 0.85 (719.1 examples/sec; 0.178 sec/batch)
2016-10-15 03:28:16.314923: step 38300, loss = 0.79 (716.8 examples/sec; 0.179 sec/batch)
2016-10-15 03:28:18.260063: step 38310, loss = 0.83 (730.5 examples/sec; 0.175 sec/batch)
2016-10-15 03:28:20.013302: step 38320, loss = 0.78 (743.6 examples/sec; 0.172 sec/batch)
2016-10-15 03:28:21.776810: step 38330, loss = 0.85 (722.1 examples/sec; 0.177 sec/batch)
2016-10-15 03:28:23.539312: step 38340, loss = 0.78 (704.1 examples/sec; 0.182 sec/batch)
2016-10-15 03:28:25.299900: step 38350, loss = 0.72 (716.5 examples/sec; 0.179 sec/batch)
2016-10-15 03:28:27.063962: step 38360, loss = 0.65 (728.1 examples/sec; 0.176 sec/batch)
2016-10-15 03:28:28.830400: step 38370, loss = 0.72 (720.2 examples/sec; 0.178 sec/batch)
2016-10-15 03:28:30.601240: step 38380, loss = 0.76 (723.0 examples/sec; 0.177 sec/batch)
2016-10-15 03:28:32.359216: step 38390, loss = 0.67 (731.2 examples/sec; 0.175 sec/batch)
2016-10-15 03:28:34.129717: step 38400, loss = 0.89 (742.3 examples/sec; 0.172 sec/batch)
2016-10-15 03:28:36.073784: step 38410, loss = 0.99 (711.9 examples/sec; 0.180 sec/batch)
2016-10-15 03:28:37.834862: step 38420, loss = 0.86 (733.8 examples/sec; 0.174 sec/batch)
2016-10-15 03:28:39.601591: step 38430, loss = 0.74 (725.1 examples/sec; 0.177 sec/batch)
2016-10-15 03:28:41.356212: step 38440, loss = 0.88 (746.2 examples/sec; 0.172 sec/batch)
2016-10-15 03:28:43.118361: step 38450, loss = 0.77 (742.5 examples/sec; 0.172 sec/batch)
2016-10-15 03:28:44.890406: step 38460, loss = 0.76 (719.0 examples/sec; 0.178 sec/batch)
2016-10-15 03:28:46.646464: step 38470, loss = 0.76 (710.8 examples/sec; 0.180 sec/batch)
2016-10-15 03:28:48.413606: step 38480, loss = 0.75 (749.0 examples/sec; 0.171 sec/batch)
2016-10-15 03:28:50.183835: step 38490, loss = 0.75 (726.2 examples/sec; 0.176 sec/batch)
2016-10-15 03:28:51.947691: step 38500, loss = 0.80 (750.0 examples/sec; 0.171 sec/batch)
2016-10-15 03:28:53.900733: step 38510, loss = 0.81 (739.9 examples/sec; 0.173 sec/batch)
2016-10-15 03:28:55.658457: step 38520, loss = 0.83 (736.3 examples/sec; 0.174 sec/batch)
2016-10-15 03:28:57.421604: step 38530, loss = 0.70 (721.0 examples/sec; 0.178 sec/batch)
2016-10-15 03:28:59.174215: step 38540, loss = 0.73 (732.8 examples/sec; 0.175 sec/batch)
2016-10-15 03:29:00.935477: step 38550, loss = 0.70 (716.5 examples/sec; 0.179 sec/batch)
2016-10-15 03:29:02.694550: step 38560, loss = 0.83 (733.3 examples/sec; 0.175 sec/batch)
2016-10-15 03:29:04.466086: step 38570, loss = 0.78 (736.8 examples/sec; 0.174 sec/batch)
2016-10-15 03:29:06.230754: step 38580, loss = 0.79 (694.5 examples/sec; 0.184 sec/batch)
2016-10-15 03:29:07.979648: step 38590, loss = 0.72 (723.7 examples/sec; 0.177 sec/batch)
2016-10-15 03:29:09.734969: step 38600, loss = 0.76 (752.6 examples/sec; 0.170 sec/batch)
2016-10-15 03:29:11.683932: step 38610, loss = 0.73 (728.4 examples/sec; 0.176 sec/batch)
2016-10-15 03:29:13.449375: step 38620, loss = 0.72 (707.4 examples/sec; 0.181 sec/batch)
2016-10-15 03:29:15.206459: step 38630, loss = 0.85 (731.0 examples/sec; 0.175 sec/batch)
2016-10-15 03:29:16.967874: step 38640, loss = 0.68 (727.5 examples/sec; 0.176 sec/batch)
2016-10-15 03:29:18.722424: step 38650, loss = 0.67 (727.4 examples/sec; 0.176 sec/batch)
2016-10-15 03:29:20.474172: step 38660, loss = 0.87 (724.2 examples/sec; 0.177 sec/batch)
2016-10-15 03:29:22.226394: step 38670, loss = 0.86 (734.0 examples/sec; 0.174 sec/batch)
2016-10-15 03:29:23.986554: step 38680, loss = 0.69 (727.4 examples/sec; 0.176 sec/batch)
2016-10-15 03:29:25.750858: step 38690, loss = 0.67 (714.6 examples/sec; 0.179 sec/batch)
2016-10-15 03:29:27.503931: step 38700, loss = 0.65 (761.3 examples/sec; 0.168 sec/batch)
2016-10-15 03:29:29.461233: step 38710, loss = 0.70 (745.0 examples/sec; 0.172 sec/batch)
2016-10-15 03:29:31.227249: step 38720, loss = 0.90 (735.7 examples/sec; 0.174 sec/batch)
2016-10-15 03:29:32.985906: step 38730, loss = 0.61 (735.8 examples/sec; 0.174 sec/batch)
2016-10-15 03:29:34.746318: step 38740, loss = 0.85 (745.6 examples/sec; 0.172 sec/batch)
2016-10-15 03:29:36.507652: step 38750, loss = 0.99 (723.4 examples/sec; 0.177 sec/batch)
2016-10-15 03:29:38.270547: step 38760, loss = 0.71 (706.7 examples/sec; 0.181 sec/batch)
2016-10-15 03:29:40.023281: step 38770, loss = 0.88 (731.5 examples/sec; 0.175 sec/batch)
2016-10-15 03:29:41.780418: step 38780, loss = 0.79 (719.5 examples/sec; 0.178 sec/batch)
2016-10-15 03:29:43.530697: step 38790, loss = 0.64 (748.3 examples/sec; 0.171 sec/batch)
2016-10-15 03:29:45.296610: step 38800, loss = 0.86 (700.6 examples/sec; 0.183 sec/batch)
2016-10-15 03:29:47.241355: step 38810, loss = 0.78 (732.2 examples/sec; 0.175 sec/batch)
2016-10-15 03:29:49.015048: step 38820, loss = 0.78 (745.2 examples/sec; 0.172 sec/batch)
2016-10-15 03:29:50.778050: step 38830, loss = 0.72 (723.9 examples/sec; 0.177 sec/batch)
2016-10-15 03:29:52.540021: step 38840, loss = 0.90 (732.0 examples/sec; 0.175 sec/batch)
2016-10-15 03:29:54.307558: step 38850, loss = 0.70 (738.4 examples/sec; 0.173 sec/batch)
2016-10-15 03:29:56.074929: step 38860, loss = 0.78 (730.5 examples/sec; 0.175 sec/batch)
2016-10-15 03:29:57.834589: step 38870, loss = 0.68 (734.1 examples/sec; 0.174 sec/batch)
2016-10-15 03:29:59.586722: step 38880, loss = 0.66 (741.5 examples/sec; 0.173 sec/batch)
2016-10-15 03:30:01.346381: step 38890, loss = 0.82 (730.0 examples/sec; 0.175 sec/batch)
2016-10-15 03:30:03.132813: step 38900, loss = 0.87 (723.3 examples/sec; 0.177 sec/batch)
2016-10-15 03:30:05.078017: step 38910, loss = 0.69 (733.1 examples/sec; 0.175 sec/batch)
2016-10-15 03:30:06.845406: step 38920, loss = 0.72 (735.8 examples/sec; 0.174 sec/batch)
2016-10-15 03:30:08.618780: step 38930, loss = 0.92 (737.5 examples/sec; 0.174 sec/batch)
2016-10-15 03:30:10.389011: step 38940, loss = 0.69 (723.0 examples/sec; 0.177 sec/batch)
2016-10-15 03:30:12.156572: step 38950, loss = 0.57 (672.2 examples/sec; 0.190 sec/batch)
2016-10-15 03:30:13.909276: step 38960, loss = 0.80 (700.6 examples/sec; 0.183 sec/batch)
2016-10-15 03:30:15.660136: step 38970, loss = 0.90 (741.8 examples/sec; 0.173 sec/batch)
2016-10-15 03:30:17.416095: step 38980, loss = 0.64 (736.3 examples/sec; 0.174 sec/batch)
2016-10-15 03:30:19.174453: step 38990, loss = 0.62 (742.5 examples/sec; 0.172 sec/batch)
2016-10-15 03:30:20.935243: step 39000, loss = 0.85 (730.3 examples/sec; 0.175 sec/batch)
2016-10-15 03:30:23.391937: step 39010, loss = 0.72 (726.7 examples/sec; 0.176 sec/batch)
2016-10-15 03:30:25.159302: step 39020, loss = 0.81 (728.2 examples/sec; 0.176 sec/batch)
2016-10-15 03:30:26.916909: step 39030, loss = 0.77 (728.2 examples/sec; 0.176 sec/batch)
2016-10-15 03:30:28.668867: step 39040, loss = 0.76 (743.4 examples/sec; 0.172 sec/batch)
2016-10-15 03:30:30.433918: step 39050, loss = 0.82 (690.8 examples/sec; 0.185 sec/batch)
2016-10-15 03:30:32.191040: step 39060, loss = 0.81 (710.0 examples/sec; 0.180 sec/batch)
2016-10-15 03:30:33.954804: step 39070, loss = 0.77 (710.5 examples/sec; 0.180 sec/batch)
2016-10-15 03:30:35.704942: step 39080, loss = 0.62 (745.2 examples/sec; 0.172 sec/batch)
2016-10-15 03:30:37.460300: step 39090, loss = 0.79 (740.9 examples/sec; 0.173 sec/batch)
2016-10-15 03:30:39.219360: step 39100, loss = 0.74 (723.4 examples/sec; 0.177 sec/batch)
2016-10-15 03:30:41.155419: step 39110, loss = 0.70 (715.0 examples/sec; 0.179 sec/batch)
2016-10-15 03:30:42.912758: step 39120, loss = 0.66 (719.1 examples/sec; 0.178 sec/batch)
2016-10-15 03:30:44.667686: step 39130, loss = 0.85 (739.5 examples/sec; 0.173 sec/batch)
2016-10-15 03:30:46.423776: step 39140, loss = 0.76 (727.9 examples/sec; 0.176 sec/batch)
2016-10-15 03:30:48.177285: step 39150, loss = 0.70 (736.8 examples/sec; 0.174 sec/batch)
2016-10-15 03:30:49.924155: step 39160, loss = 0.65 (726.3 examples/sec; 0.176 sec/batch)
2016-10-15 03:30:51.682106: step 39170, loss = 0.62 (729.7 examples/sec; 0.175 sec/batch)
2016-10-15 03:30:53.438923: step 39180, loss = 0.61 (719.6 examples/sec; 0.178 sec/batch)
2016-10-15 03:30:55.203395: step 39190, loss = 0.82 (704.1 examples/sec; 0.182 sec/batch)
2016-10-15 03:30:56.957808: step 39200, loss = 0.73 (735.7 examples/sec; 0.174 sec/batch)
2016-10-15 03:30:58.934433: step 39210, loss = 0.66 (727.5 examples/sec; 0.176 sec/batch)
2016-10-15 03:31:00.688173: step 39220, loss = 0.72 (704.3 examples/sec; 0.182 sec/batch)
2016-10-15 03:31:02.448144: step 39230, loss = 0.69 (757.7 examples/sec; 0.169 sec/batch)
2016-10-15 03:31:04.214576: step 39240, loss = 0.70 (740.2 examples/sec; 0.173 sec/batch)
2016-10-15 03:31:05.974612: step 39250, loss = 0.65 (726.7 examples/sec; 0.176 sec/batch)
2016-10-15 03:31:07.736542: step 39260, loss = 0.78 (722.0 examples/sec; 0.177 sec/batch)
2016-10-15 03:31:09.495320: step 39270, loss = 0.83 (729.4 examples/sec; 0.175 sec/batch)
2016-10-15 03:31:11.258500: step 39280, loss = 0.72 (702.2 examples/sec; 0.182 sec/batch)
2016-10-15 03:31:13.025521: step 39290, loss = 0.73 (726.3 examples/sec; 0.176 sec/batch)
2016-10-15 03:31:14.786414: step 39300, loss = 0.83 (750.4 examples/sec; 0.171 sec/batch)
2016-10-15 03:31:16.736480: step 39310, loss = 0.61 (734.4 examples/sec; 0.174 sec/batch)
2016-10-15 03:31:18.492073: step 39320, loss = 0.84 (761.3 examples/sec; 0.168 sec/batch)
2016-10-15 03:31:20.256578: step 39330, loss = 0.75 (744.5 examples/sec; 0.172 sec/batch)
2016-10-15 03:31:22.017127: step 39340, loss = 0.92 (730.2 examples/sec; 0.175 sec/batch)
2016-10-15 03:31:23.778407: step 39350, loss = 0.83 (735.5 examples/sec; 0.174 sec/batch)
2016-10-15 03:31:25.539193: step 39360, loss = 0.68 (713.0 examples/sec; 0.180 sec/batch)
2016-10-15 03:31:27.291521: step 39370, loss = 0.77 (750.1 examples/sec; 0.171 sec/batch)
2016-10-15 03:31:29.056470: step 39380, loss = 0.81 (727.9 examples/sec; 0.176 sec/batch)
2016-10-15 03:31:30.816028: step 39390, loss = 0.69 (747.2 examples/sec; 0.171 sec/batch)
2016-10-15 03:31:32.574380: step 39400, loss = 0.72 (749.9 examples/sec; 0.171 sec/batch)
2016-10-15 03:31:34.526394: step 39410, loss = 0.82 (726.4 examples/sec; 0.176 sec/batch)
2016-10-15 03:31:36.301325: step 39420, loss = 0.88 (735.7 examples/sec; 0.174 sec/batch)
2016-10-15 03:31:38.057823: step 39430, loss = 0.69 (747.8 examples/sec; 0.171 sec/batch)
2016-10-15 03:31:39.813302: step 39440, loss = 0.90 (734.7 examples/sec; 0.174 sec/batch)
2016-10-15 03:31:41.571410: step 39450, loss = 0.76 (732.9 examples/sec; 0.175 sec/batch)
2016-10-15 03:31:43.327170: step 39460, loss = 0.89 (723.2 examples/sec; 0.177 sec/batch)
2016-10-15 03:31:45.090804: step 39470, loss = 0.72 (742.9 examples/sec; 0.172 sec/batch)
2016-10-15 03:31:46.850494: step 39480, loss = 0.84 (769.8 examples/sec; 0.166 sec/batch)
2016-10-15 03:31:48.614978: step 39490, loss = 0.70 (753.6 examples/sec; 0.170 sec/batch)
2016-10-15 03:31:50.374889: step 39500, loss = 0.80 (781.3 examples/sec; 0.164 sec/batch)
2016-10-15 03:31:52.315536: step 39510, loss = 0.81 (733.5 examples/sec; 0.174 sec/batch)
2016-10-15 03:31:54.079832: step 39520, loss = 0.76 (715.8 examples/sec; 0.179 sec/batch)
2016-10-15 03:31:55.822929: step 39530, loss = 0.72 (737.5 examples/sec; 0.174 sec/batch)
2016-10-15 03:31:57.596107: step 39540, loss = 0.69 (729.4 examples/sec; 0.175 sec/batch)
2016-10-15 03:31:59.354499: step 39550, loss = 0.66 (727.1 examples/sec; 0.176 sec/batch)
2016-10-15 03:32:01.107937: step 39560, loss = 0.75 (750.8 examples/sec; 0.170 sec/batch)
2016-10-15 03:32:02.867012: step 39570, loss = 0.77 (718.8 examples/sec; 0.178 sec/batch)
2016-10-15 03:32:04.636027: step 39580, loss = 0.79 (737.5 examples/sec; 0.174 sec/batch)
2016-10-15 03:32:06.401890: step 39590, loss = 0.68 (743.7 examples/sec; 0.172 sec/batch)
2016-10-15 03:32:08.170268: step 39600, loss = 0.63 (721.0 examples/sec; 0.178 sec/batch)
2016-10-15 03:32:10.098818: step 39610, loss = 0.67 (725.8 examples/sec; 0.176 sec/batch)
2016-10-15 03:32:11.859092: step 39620, loss = 0.86 (727.9 examples/sec; 0.176 sec/batch)
2016-10-15 03:32:13.616544: step 39630, loss = 0.75 (721.7 examples/sec; 0.177 sec/batch)
2016-10-15 03:32:15.372971: step 39640, loss = 0.61 (748.0 examples/sec; 0.171 sec/batch)
2016-10-15 03:32:17.129314: step 39650, loss = 0.64 (750.0 examples/sec; 0.171 sec/batch)
2016-10-15 03:32:18.883029: step 39660, loss = 0.69 (730.3 examples/sec; 0.175 sec/batch)
2016-10-15 03:32:20.640815: step 39670, loss = 0.69 (732.2 examples/sec; 0.175 sec/batch)
2016-10-15 03:32:22.413264: step 39680, loss = 0.58 (717.0 examples/sec; 0.179 sec/batch)
2016-10-15 03:32:24.177474: step 39690, loss = 0.66 (715.1 examples/sec; 0.179 sec/batch)
2016-10-15 03:32:25.940745: step 39700, loss = 0.67 (728.3 examples/sec; 0.176 sec/batch)
2016-10-15 03:32:27.898807: step 39710, loss = 0.84 (724.2 examples/sec; 0.177 sec/batch)
2016-10-15 03:32:29.655312: step 39720, loss = 0.69 (768.5 examples/sec; 0.167 sec/batch)
2016-10-15 03:32:31.428151: step 39730, loss = 0.69 (729.0 examples/sec; 0.176 sec/batch)
2016-10-15 03:32:33.193093: step 39740, loss = 0.71 (733.5 examples/sec; 0.175 sec/batch)
2016-10-15 03:32:34.962099: step 39750, loss = 0.80 (755.9 examples/sec; 0.169 sec/batch)
2016-10-15 03:32:36.726889: step 39760, loss = 0.77 (714.3 examples/sec; 0.179 sec/batch)
2016-10-15 03:32:38.497558: step 39770, loss = 0.76 (704.9 examples/sec; 0.182 sec/batch)
2016-10-15 03:32:40.265413: step 39780, loss = 0.60 (719.3 examples/sec; 0.178 sec/batch)
2016-10-15 03:32:42.029264: step 39790, loss = 0.74 (715.2 examples/sec; 0.179 sec/batch)
2016-10-15 03:32:43.784233: step 39800, loss = 0.86 (717.0 examples/sec; 0.179 sec/batch)
2016-10-15 03:32:45.735409: step 39810, loss = 0.73 (727.1 examples/sec; 0.176 sec/batch)
2016-10-15 03:32:47.494729: step 39820, loss = 0.74 (773.5 examples/sec; 0.165 sec/batch)
2016-10-15 03:32:49.262067: step 39830, loss = 0.69 (734.1 examples/sec; 0.174 sec/batch)
2016-10-15 03:32:51.019265: step 39840, loss = 0.86 (743.4 examples/sec; 0.172 sec/batch)
2016-10-15 03:32:52.782800: step 39850, loss = 0.78 (722.5 examples/sec; 0.177 sec/batch)
2016-10-15 03:32:54.546485: step 39860, loss = 0.81 (743.1 examples/sec; 0.172 sec/batch)
2016-10-15 03:32:56.307075: step 39870, loss = 0.83 (736.8 examples/sec; 0.174 sec/batch)
2016-10-15 03:32:58.069292: step 39880, loss = 0.85 (723.6 examples/sec; 0.177 sec/batch)
2016-10-15 03:32:59.838764: step 39890, loss = 0.62 (717.3 examples/sec; 0.178 sec/batch)
2016-10-15 03:33:01.606947: step 39900, loss = 0.76 (715.9 examples/sec; 0.179 sec/batch)
2016-10-15 03:33:03.555311: step 39910, loss = 0.70 (741.0 examples/sec; 0.173 sec/batch)
2016-10-15 03:33:05.319797: step 39920, loss = 0.85 (714.8 examples/sec; 0.179 sec/batch)
2016-10-15 03:33:07.071297: step 39930, loss = 0.96 (726.8 examples/sec; 0.176 sec/batch)
2016-10-15 03:33:08.831901: step 39940, loss = 0.76 (713.6 examples/sec; 0.179 sec/batch)
2016-10-15 03:33:10.585229: step 39950, loss = 0.71 (744.4 examples/sec; 0.172 sec/batch)
2016-10-15 03:33:12.340602: step 39960, loss = 0.85 (729.3 examples/sec; 0.176 sec/batch)
2016-10-15 03:33:14.097494: step 39970, loss = 0.69 (766.4 examples/sec; 0.167 sec/batch)
2016-10-15 03:33:15.848621: step 39980, loss = 0.82 (733.6 examples/sec; 0.174 sec/batch)
2016-10-15 03:33:17.604212: step 39990, loss = 0.72 (737.1 examples/sec; 0.174 sec/batch)
2016-10-15 03:33:19.363208: step 40000, loss = 0.74 (727.7 examples/sec; 0.176 sec/batch)
2016-10-15 03:33:21.886931: step 40010, loss = 0.68 (734.2 examples/sec; 0.174 sec/batch)
2016-10-15 03:33:23.653245: step 40020, loss = 0.70 (743.6 examples/sec; 0.172 sec/batch)
2016-10-15 03:33:25.415722: step 40030, loss = 0.78 (712.3 examples/sec; 0.180 sec/batch)
2016-10-15 03:33:27.171477: step 40040, loss = 0.71 (705.1 examples/sec; 0.182 sec/batch)
2016-10-15 03:33:28.922709: step 40050, loss = 0.79 (722.8 examples/sec; 0.177 sec/batch)
2016-10-15 03:33:30.686178: step 40060, loss = 0.70 (736.0 examples/sec; 0.174 sec/batch)
2016-10-15 03:33:32.442888: step 40070, loss = 0.76 (716.7 examples/sec; 0.179 sec/batch)
2016-10-15 03:33:34.210318: step 40080, loss = 0.79 (702.9 examples/sec; 0.182 sec/batch)
2016-10-15 03:33:35.959579: step 40090, loss = 0.72 (739.1 examples/sec; 0.173 sec/batch)
2016-10-15 03:33:37.721520: step 40100, loss = 0.78 (730.6 examples/sec; 0.175 sec/batch)
2016-10-15 03:33:39.659909: step 40110, loss = 0.76 (732.6 examples/sec; 0.175 sec/batch)
2016-10-15 03:33:41.424693: step 40120, loss = 0.82 (718.1 examples/sec; 0.178 sec/batch)
2016-10-15 03:33:43.179142: step 40130, loss = 0.71 (730.6 examples/sec; 0.175 sec/batch)
2016-10-15 03:33:44.950790: step 40140, loss = 0.69 (734.2 examples/sec; 0.174 sec/batch)
2016-10-15 03:33:46.696369: step 40150, loss = 0.75 (717.6 examples/sec; 0.178 sec/batch)
2016-10-15 03:33:48.456562: step 40160, loss = 0.77 (735.7 examples/sec; 0.174 sec/batch)
2016-10-15 03:33:50.208039: step 40170, loss = 0.70 (728.2 examples/sec; 0.176 sec/batch)
2016-10-15 03:33:51.965008: step 40180, loss = 0.77 (727.5 examples/sec; 0.176 sec/batch)
2016-10-15 03:33:53.721079: step 40190, loss = 0.72 (722.9 examples/sec; 0.177 sec/batch)
2016-10-15 03:33:55.477878: step 40200, loss = 0.73 (718.3 examples/sec; 0.178 sec/batch)
2016-10-15 03:33:57.432722: step 40210, loss = 0.86 (744.6 examples/sec; 0.172 sec/batch)
2016-10-15 03:33:59.199642: step 40220, loss = 0.80 (728.2 examples/sec; 0.176 sec/batch)
2016-10-15 03:34:00.958974: step 40230, loss = 0.72 (735.4 examples/sec; 0.174 sec/batch)
2016-10-15 03:34:02.736372: step 40240, loss = 0.62 (732.9 examples/sec; 0.175 sec/batch)
2016-10-15 03:34:04.495006: step 40250, loss = 0.82 (741.2 examples/sec; 0.173 sec/batch)
2016-10-15 03:34:06.259489: step 40260, loss = 0.78 (728.0 examples/sec; 0.176 sec/batch)
2016-10-15 03:34:08.009932: step 40270, loss = 0.82 (725.0 examples/sec; 0.177 sec/batch)
2016-10-15 03:34:09.766849: step 40280, loss = 0.95 (751.6 examples/sec; 0.170 sec/batch)
2016-10-15 03:34:11.533347: step 40290, loss = 0.77 (724.4 examples/sec; 0.177 sec/batch)
2016-10-15 03:34:13.290825: step 40300, loss = 0.74 (753.0 examples/sec; 0.170 sec/batch)
2016-10-15 03:34:15.248450: step 40310, loss = 0.65 (679.4 examples/sec; 0.188 sec/batch)
2016-10-15 03:34:16.999378: step 40320, loss = 0.76 (721.0 examples/sec; 0.178 sec/batch)
2016-10-15 03:34:18.751421: step 40330, loss = 0.79 (743.6 examples/sec; 0.172 sec/batch)
2016-10-15 03:34:20.515271: step 40340, loss = 0.88 (768.6 examples/sec; 0.167 sec/batch)
2016-10-15 03:34:22.274162: step 40350, loss = 0.87 (732.7 examples/sec; 0.175 sec/batch)
2016-10-15 03:34:24.035294: step 40360, loss = 0.72 (708.9 examples/sec; 0.181 sec/batch)
2016-10-15 03:34:25.786223: step 40370, loss = 0.84 (724.9 examples/sec; 0.177 sec/batch)
2016-10-15 03:34:27.539519: step 40380, loss = 0.77 (731.7 examples/sec; 0.175 sec/batch)
2016-10-15 03:34:29.294974: step 40390, loss = 0.82 (733.7 examples/sec; 0.174 sec/batch)
2016-10-15 03:34:31.065026: step 40400, loss = 0.72 (721.2 examples/sec; 0.177 sec/batch)
2016-10-15 03:34:33.018668: step 40410, loss = 0.83 (721.2 examples/sec; 0.177 sec/batch)
2016-10-15 03:34:34.776622: step 40420, loss = 0.88 (723.8 examples/sec; 0.177 sec/batch)
2016-10-15 03:34:36.536487: step 40430, loss = 0.74 (720.4 examples/sec; 0.178 sec/batch)
2016-10-15 03:34:38.305747: step 40440, loss = 0.56 (723.1 examples/sec; 0.177 sec/batch)
2016-10-15 03:34:40.066925: step 40450, loss = 0.75 (732.2 examples/sec; 0.175 sec/batch)
2016-10-15 03:34:41.825534: step 40460, loss = 0.96 (730.0 examples/sec; 0.175 sec/batch)
2016-10-15 03:34:43.593221: step 40470, loss = 0.88 (710.4 examples/sec; 0.180 sec/batch)
2016-10-15 03:34:45.365742: step 40480, loss = 0.79 (723.9 examples/sec; 0.177 sec/batch)
2016-10-15 03:34:47.124487: step 40490, loss = 0.75 (712.2 examples/sec; 0.180 sec/batch)
2016-10-15 03:34:48.885572: step 40500, loss = 0.72 (709.5 examples/sec; 0.180 sec/batch)
2016-10-15 03:34:50.834946: step 40510, loss = 0.64 (768.8 examples/sec; 0.166 sec/batch)
2016-10-15 03:34:52.616157: step 40520, loss = 0.66 (707.3 examples/sec; 0.181 sec/batch)
2016-10-15 03:34:54.371769: step 40530, loss = 0.72 (728.9 examples/sec; 0.176 sec/batch)
2016-10-15 03:34:56.144251: step 40540, loss = 0.68 (702.7 examples/sec; 0.182 sec/batch)
2016-10-15 03:34:57.907861: step 40550, loss = 0.86 (722.4 examples/sec; 0.177 sec/batch)
2016-10-15 03:34:59.670050: step 40560, loss = 0.61 (751.1 examples/sec; 0.170 sec/batch)
2016-10-15 03:35:01.443094: step 40570, loss = 0.65 (686.5 examples/sec; 0.186 sec/batch)
2016-10-15 03:35:03.210448: step 40580, loss = 0.72 (748.2 examples/sec; 0.171 sec/batch)
2016-10-15 03:35:04.984844: step 40590, loss = 0.65 (697.7 examples/sec; 0.183 sec/batch)
2016-10-15 03:35:06.739303: step 40600, loss = 0.61 (737.1 examples/sec; 0.174 sec/batch)
2016-10-15 03:35:08.689172: step 40610, loss = 0.67 (702.8 examples/sec; 0.182 sec/batch)
2016-10-15 03:35:10.454160: step 40620, loss = 0.77 (739.6 examples/sec; 0.173 sec/batch)
2016-10-15 03:35:12.203446: step 40630, loss = 0.86 (739.9 examples/sec; 0.173 sec/batch)
2016-10-15 03:35:13.969254: step 40640, loss = 0.84 (729.3 examples/sec; 0.176 sec/batch)
2016-10-15 03:35:15.737808: step 40650, loss = 0.67 (715.0 examples/sec; 0.179 sec/batch)
2016-10-15 03:35:17.503744: step 40660, loss = 0.77 (730.4 examples/sec; 0.175 sec/batch)
2016-10-15 03:35:19.263616: step 40670, loss = 0.81 (734.3 examples/sec; 0.174 sec/batch)
2016-10-15 03:35:21.034613: step 40680, loss = 0.85 (713.4 examples/sec; 0.179 sec/batch)
2016-10-15 03:35:22.793164: step 40690, loss = 0.58 (737.4 examples/sec; 0.174 sec/batch)
2016-10-15 03:35:24.560832: step 40700, loss = 0.87 (732.1 examples/sec; 0.175 sec/batch)
2016-10-15 03:35:26.506307: step 40710, loss = 0.77 (752.1 examples/sec; 0.170 sec/batch)
2016-10-15 03:35:28.275439: step 40720, loss = 0.70 (706.3 examples/sec; 0.181 sec/batch)
2016-10-15 03:35:30.033921: step 40730, loss = 0.65 (725.2 examples/sec; 0.177 sec/batch)
2016-10-15 03:35:31.797952: step 40740, loss = 0.89 (735.2 examples/sec; 0.174 sec/batch)
2016-10-15 03:35:33.561724: step 40750, loss = 0.71 (742.1 examples/sec; 0.172 sec/batch)
2016-10-15 03:35:35.325524: step 40760, loss = 0.81 (730.5 examples/sec; 0.175 sec/batch)
2016-10-15 03:35:37.086521: step 40770, loss = 0.76 (722.5 examples/sec; 0.177 sec/batch)
2016-10-15 03:35:38.847900: step 40780, loss = 0.70 (726.2 examples/sec; 0.176 sec/batch)
2016-10-15 03:35:40.603186: step 40790, loss = 0.83 (736.9 examples/sec; 0.174 sec/batch)
2016-10-15 03:35:42.360486: step 40800, loss = 0.93 (723.2 examples/sec; 0.177 sec/batch)
2016-10-15 03:35:44.315109: step 40810, loss = 0.78 (727.8 examples/sec; 0.176 sec/batch)
2016-10-15 03:35:46.085010: step 40820, loss = 0.82 (703.6 examples/sec; 0.182 sec/batch)
2016-10-15 03:35:47.835060: step 40830, loss = 0.71 (746.0 examples/sec; 0.172 sec/batch)
2016-10-15 03:35:49.594840: step 40840, loss = 0.74 (721.3 examples/sec; 0.177 sec/batch)
2016-10-15 03:35:51.355462: step 40850, loss = 0.77 (713.5 examples/sec; 0.179 sec/batch)
2016-10-15 03:35:53.117474: step 40860, loss = 0.95 (717.7 examples/sec; 0.178 sec/batch)
2016-10-15 03:35:54.869289: step 40870, loss = 0.65 (724.3 examples/sec; 0.177 sec/batch)
2016-10-15 03:35:56.637858: step 40880, loss = 0.71 (713.5 examples/sec; 0.179 sec/batch)
2016-10-15 03:35:58.390718: step 40890, loss = 0.76 (752.5 examples/sec; 0.170 sec/batch)
2016-10-15 03:36:00.153217: step 40900, loss = 0.69 (748.9 examples/sec; 0.171 sec/batch)
2016-10-15 03:36:02.106173: step 40910, loss = 0.87 (721.7 examples/sec; 0.177 sec/batch)
2016-10-15 03:36:03.868313: step 40920, loss = 0.80 (739.0 examples/sec; 0.173 sec/batch)
2016-10-15 03:36:05.637244: step 40930, loss = 0.76 (703.4 examples/sec; 0.182 sec/batch)
2016-10-15 03:36:07.400568: step 40940, loss = 0.69 (703.3 examples/sec; 0.182 sec/batch)
2016-10-15 03:36:09.160463: step 40950, loss = 0.88 (747.3 examples/sec; 0.171 sec/batch)
2016-10-15 03:36:10.911354: step 40960, loss = 0.97 (762.4 examples/sec; 0.168 sec/batch)
2016-10-15 03:36:12.673006: step 40970, loss = 0.95 (717.5 examples/sec; 0.178 sec/batch)
2016-10-15 03:36:14.441655: step 40980, loss = 0.75 (714.8 examples/sec; 0.179 sec/batch)
2016-10-15 03:36:16.197239: step 40990, loss = 0.82 (720.3 examples/sec; 0.178 sec/batch)
2016-10-15 03:36:17.956673: step 41000, loss = 0.71 (707.4 examples/sec; 0.181 sec/batch)
2016-10-15 03:36:20.408837: step 41010, loss = 0.69 (709.9 examples/sec; 0.180 sec/batch)
2016-10-15 03:36:22.167011: step 41020, loss = 0.77 (721.9 examples/sec; 0.177 sec/batch)
2016-10-15 03:36:23.932924: step 41030, loss = 0.81 (696.0 examples/sec; 0.184 sec/batch)
2016-10-15 03:36:25.681445: step 41040, loss = 0.81 (744.8 examples/sec; 0.172 sec/batch)
2016-10-15 03:36:27.438797: step 41050, loss = 0.77 (740.1 examples/sec; 0.173 sec/batch)
2016-10-15 03:36:29.191793: step 41060, loss = 0.63 (741.3 examples/sec; 0.173 sec/batch)
2016-10-15 03:36:30.945152: step 41070, loss = 0.66 (750.3 examples/sec; 0.171 sec/batch)
2016-10-15 03:36:32.708126: step 41080, loss = 0.74 (730.8 examples/sec; 0.175 sec/batch)
2016-10-15 03:36:34.470698: step 41090, loss = 0.76 (705.8 examples/sec; 0.181 sec/batch)
2016-10-15 03:36:36.228335: step 41100, loss = 0.66 (722.2 examples/sec; 0.177 sec/batch)
2016-10-15 03:36:38.167547: step 41110, loss = 0.62 (744.2 examples/sec; 0.172 sec/batch)
2016-10-15 03:36:39.932051: step 41120, loss = 0.75 (714.3 examples/sec; 0.179 sec/batch)
2016-10-15 03:36:41.697357: step 41130, loss = 0.77 (733.3 examples/sec; 0.175 sec/batch)
2016-10-15 03:36:43.458344: step 41140, loss = 0.77 (721.3 examples/sec; 0.177 sec/batch)
2016-10-15 03:36:45.217592: step 41150, loss = 0.87 (721.6 examples/sec; 0.177 sec/batch)
2016-10-15 03:36:46.984730: step 41160, loss = 0.63 (704.1 examples/sec; 0.182 sec/batch)
2016-10-15 03:36:48.743456: step 41170, loss = 0.87 (725.6 examples/sec; 0.176 sec/batch)
2016-10-15 03:36:50.506583: step 41180, loss = 0.89 (741.9 examples/sec; 0.173 sec/batch)
2016-10-15 03:36:52.269290: step 41190, loss = 0.67 (712.2 examples/sec; 0.180 sec/batch)
2016-10-15 03:36:54.031171: step 41200, loss = 0.78 (702.0 examples/sec; 0.182 sec/batch)
2016-10-15 03:36:55.987357: step 41210, loss = 0.81 (694.2 examples/sec; 0.184 sec/batch)
2016-10-15 03:36:57.735233: step 41220, loss = 0.91 (711.2 examples/sec; 0.180 sec/batch)
2016-10-15 03:36:59.491816: step 41230, loss = 0.78 (718.9 examples/sec; 0.178 sec/batch)
2016-10-15 03:37:01.251614: step 41240, loss = 0.60 (734.4 examples/sec; 0.174 sec/batch)
2016-10-15 03:37:03.030169: step 41250, loss = 0.95 (702.5 examples/sec; 0.182 sec/batch)
2016-10-15 03:37:04.785010: step 41260, loss = 0.78 (734.1 examples/sec; 0.174 sec/batch)
2016-10-15 03:37:06.544465: step 41270, loss = 0.65 (728.7 examples/sec; 0.176 sec/batch)
2016-10-15 03:37:08.311225: step 41280, loss = 0.64 (716.5 examples/sec; 0.179 sec/batch)
2016-10-15 03:37:10.067582: step 41290, loss = 0.69 (732.4 examples/sec; 0.175 sec/batch)
2016-10-15 03:37:11.823201: step 41300, loss = 0.79 (757.8 examples/sec; 0.169 sec/batch)
2016-10-15 03:37:13.772455: step 41310, loss = 0.77 (717.3 examples/sec; 0.178 sec/batch)
2016-10-15 03:37:15.521863: step 41320, loss = 0.75 (729.0 examples/sec; 0.176 sec/batch)
2016-10-15 03:37:17.286675: step 41330, loss = 0.84 (722.4 examples/sec; 0.177 sec/batch)
2016-10-15 03:37:19.050415: step 41340, loss = 0.67 (724.1 examples/sec; 0.177 sec/batch)
2016-10-15 03:37:20.802944: step 41350, loss = 0.74 (720.2 examples/sec; 0.178 sec/batch)
2016-10-15 03:37:22.562236: step 41360, loss = 0.79 (714.7 examples/sec; 0.179 sec/batch)
2016-10-15 03:37:24.315316: step 41370, loss = 0.70 (745.1 examples/sec; 0.172 sec/batch)
2016-10-15 03:37:26.076089: step 41380, loss = 0.71 (724.1 examples/sec; 0.177 sec/batch)
2016-10-15 03:37:27.832345: step 41390, loss = 0.84 (738.9 examples/sec; 0.173 sec/batch)
2016-10-15 03:37:29.591165: step 41400, loss = 0.78 (729.8 examples/sec; 0.175 sec/batch)
2016-10-15 03:37:31.536018: step 41410, loss = 0.67 (737.7 examples/sec; 0.174 sec/batch)
2016-10-15 03:37:33.304625: step 41420, loss = 0.78 (715.1 examples/sec; 0.179 sec/batch)
2016-10-15 03:37:35.061881: step 41430, loss = 0.90 (730.0 examples/sec; 0.175 sec/batch)
2016-10-15 03:37:36.823412: step 41440, loss = 0.81 (735.0 examples/sec; 0.174 sec/batch)
2016-10-15 03:37:38.585198: step 41450, loss = 0.68 (723.6 examples/sec; 0.177 sec/batch)
2016-10-15 03:37:40.341428: step 41460, loss = 0.57 (714.8 examples/sec; 0.179 sec/batch)
2016-10-15 03:37:42.102320: step 41470, loss = 0.78 (718.7 examples/sec; 0.178 sec/batch)
2016-10-15 03:37:43.859521: step 41480, loss = 0.59 (724.8 examples/sec; 0.177 sec/batch)
2016-10-15 03:37:45.626073: step 41490, loss = 0.73 (730.0 examples/sec; 0.175 sec/batch)
2016-10-15 03:37:47.385065: step 41500, loss = 0.82 (742.8 examples/sec; 0.172 sec/batch)
2016-10-15 03:37:49.322738: step 41510, loss = 0.76 (729.6 examples/sec; 0.175 sec/batch)
2016-10-15 03:37:51.078861: step 41520, loss = 0.74 (733.7 examples/sec; 0.174 sec/batch)
2016-10-15 03:37:52.845495: step 41530, loss = 0.82 (710.0 examples/sec; 0.180 sec/batch)
2016-10-15 03:37:54.605253: step 41540, loss = 0.93 (727.8 examples/sec; 0.176 sec/batch)
2016-10-15 03:37:56.368203: step 41550, loss = 0.82 (721.2 examples/sec; 0.177 sec/batch)
2016-10-15 03:37:58.123434: step 41560, loss = 0.68 (720.8 examples/sec; 0.178 sec/batch)
2016-10-15 03:37:59.889410: step 41570, loss = 0.86 (715.1 examples/sec; 0.179 sec/batch)
2016-10-15 03:38:01.657508: step 41580, loss = 0.77 (707.7 examples/sec; 0.181 sec/batch)
2016-10-15 03:38:03.423371: step 41590, loss = 0.68 (733.7 examples/sec; 0.174 sec/batch)
2016-10-15 03:38:05.189342: step 41600, loss = 0.76 (760.5 examples/sec; 0.168 sec/batch)
2016-10-15 03:38:07.137486: step 41610, loss = 0.78 (747.2 examples/sec; 0.171 sec/batch)
2016-10-15 03:38:08.890968: step 41620, loss = 0.73 (723.8 examples/sec; 0.177 sec/batch)
2016-10-15 03:38:10.653151: step 41630, loss = 0.71 (725.2 examples/sec; 0.176 sec/batch)
2016-10-15 03:38:12.419419: step 41640, loss = 0.83 (732.0 examples/sec; 0.175 sec/batch)
2016-10-15 03:38:14.190349: step 41650, loss = 0.67 (722.3 examples/sec; 0.177 sec/batch)
2016-10-15 03:38:15.949119: step 41660, loss = 0.64 (756.8 examples/sec; 0.169 sec/batch)
2016-10-15 03:38:17.711307: step 41670, loss = 0.84 (718.0 examples/sec; 0.178 sec/batch)
2016-10-15 03:38:19.478757: step 41680, loss = 0.81 (690.1 examples/sec; 0.185 sec/batch)
2016-10-15 03:38:21.229853: step 41690, loss = 0.73 (729.1 examples/sec; 0.176 sec/batch)
2016-10-15 03:38:23.000327: step 41700, loss = 0.67 (725.8 examples/sec; 0.176 sec/batch)
2016-10-15 03:38:24.959750: step 41710, loss = 0.83 (723.6 examples/sec; 0.177 sec/batch)
2016-10-15 03:38:26.718184: step 41720, loss = 0.65 (729.9 examples/sec; 0.175 sec/batch)
2016-10-15 03:38:28.479286: step 41730, loss = 0.84 (751.1 examples/sec; 0.170 sec/batch)
2016-10-15 03:38:30.245577: step 41740, loss = 0.61 (729.9 examples/sec; 0.175 sec/batch)
2016-10-15 03:38:32.015489: step 41750, loss = 0.78 (712.4 examples/sec; 0.180 sec/batch)
2016-10-15 03:38:33.775413: step 41760, loss = 0.89 (707.1 examples/sec; 0.181 sec/batch)
2016-10-15 03:38:35.534363: step 41770, loss = 0.72 (740.8 examples/sec; 0.173 sec/batch)
2016-10-15 03:38:37.296536: step 41780, loss = 0.94 (750.2 examples/sec; 0.171 sec/batch)
2016-10-15 03:38:39.059707: step 41790, loss = 0.78 (731.7 examples/sec; 0.175 sec/batch)
2016-10-15 03:38:40.833990: step 41800, loss = 0.75 (707.6 examples/sec; 0.181 sec/batch)
2016-10-15 03:38:42.781704: step 41810, loss = 0.80 (720.4 examples/sec; 0.178 sec/batch)
2016-10-15 03:38:44.549546: step 41820, loss = 0.75 (734.1 examples/sec; 0.174 sec/batch)
2016-10-15 03:38:46.320903: step 41830, loss = 0.59 (727.4 examples/sec; 0.176 sec/batch)
2016-10-15 03:38:48.085356: step 41840, loss = 0.75 (739.9 examples/sec; 0.173 sec/batch)
2016-10-15 03:38:49.847206: step 41850, loss = 0.78 (725.0 examples/sec; 0.177 sec/batch)
2016-10-15 03:38:51.600012: step 41860, loss = 0.79 (747.1 examples/sec; 0.171 sec/batch)
2016-10-15 03:38:53.361087: step 41870, loss = 0.73 (715.7 examples/sec; 0.179 sec/batch)
2016-10-15 03:38:55.115871: step 41880, loss = 0.83 (721.8 examples/sec; 0.177 sec/batch)
2016-10-15 03:38:56.879265: step 41890, loss = 0.79 (725.5 examples/sec; 0.176 sec/batch)
2016-10-15 03:38:58.646974: step 41900, loss = 0.69 (695.8 examples/sec; 0.184 sec/batch)
2016-10-15 03:39:00.588838: step 41910, loss = 0.82 (722.7 examples/sec; 0.177 sec/batch)
2016-10-15 03:39:02.352636: step 41920, loss = 0.74 (747.9 examples/sec; 0.171 sec/batch)
2016-10-15 03:39:04.121035: step 41930, loss = 0.68 (719.4 examples/sec; 0.178 sec/batch)
2016-10-15 03:39:05.891041: step 41940, loss = 0.75 (722.8 examples/sec; 0.177 sec/batch)
2016-10-15 03:39:07.649211: step 41950, loss = 0.62 (758.4 examples/sec; 0.169 sec/batch)
2016-10-15 03:39:09.423110: step 41960, loss = 0.69 (737.4 examples/sec; 0.174 sec/batch)
2016-10-15 03:39:11.174645: step 41970, loss = 0.70 (737.1 examples/sec; 0.174 sec/batch)
2016-10-15 03:39:12.942174: step 41980, loss = 0.79 (723.3 examples/sec; 0.177 sec/batch)
2016-10-15 03:39:14.704603: step 41990, loss = 0.76 (720.2 examples/sec; 0.178 sec/batch)
2016-10-15 03:39:16.473397: step 42000, loss = 0.67 (729.4 examples/sec; 0.175 sec/batch)
2016-10-15 03:39:18.999752: step 42010, loss = 0.78 (729.0 examples/sec; 0.176 sec/batch)
2016-10-15 03:39:20.788764: step 42020, loss = 0.75 (693.2 examples/sec; 0.185 sec/batch)
2016-10-15 03:39:22.540483: step 42030, loss = 0.87 (721.2 examples/sec; 0.177 sec/batch)
2016-10-15 03:39:24.314934: step 42040, loss = 0.88 (706.3 examples/sec; 0.181 sec/batch)
2016-10-15 03:39:26.077330: step 42050, loss = 0.78 (719.2 examples/sec; 0.178 sec/batch)
2016-10-15 03:39:27.829531: step 42060, loss = 0.73 (718.3 examples/sec; 0.178 sec/batch)
2016-10-15 03:39:29.590810: step 42070, loss = 0.67 (725.0 examples/sec; 0.177 sec/batch)
2016-10-15 03:39:31.352409: step 42080, loss = 0.76 (720.0 examples/sec; 0.178 sec/batch)
2016-10-15 03:39:33.120449: step 42090, loss = 0.71 (716.4 examples/sec; 0.179 sec/batch)
2016-10-15 03:39:34.887230: step 42100, loss = 0.71 (752.8 examples/sec; 0.170 sec/batch)
2016-10-15 03:39:36.855038: step 42110, loss = 0.81 (738.7 examples/sec; 0.173 sec/batch)
2016-10-15 03:39:38.617478: step 42120, loss = 0.72 (745.1 examples/sec; 0.172 sec/batch)
2016-10-15 03:39:40.392859: step 42130, loss = 0.63 (716.9 examples/sec; 0.179 sec/batch)
2016-10-15 03:39:42.166063: step 42140, loss = 0.69 (723.9 examples/sec; 0.177 sec/batch)
2016-10-15 03:39:43.927076: step 42150, loss = 0.77 (713.8 examples/sec; 0.179 sec/batch)
2016-10-15 03:39:45.701924: step 42160, loss = 0.66 (719.4 examples/sec; 0.178 sec/batch)
2016-10-15 03:39:47.460482: step 42170, loss = 0.99 (718.2 examples/sec; 0.178 sec/batch)
2016-10-15 03:39:49.229978: step 42180, loss = 0.80 (699.4 examples/sec; 0.183 sec/batch)
2016-10-15 03:39:50.987449: step 42190, loss = 0.76 (739.3 examples/sec; 0.173 sec/batch)
2016-10-15 03:39:52.748446: step 42200, loss = 0.76 (730.3 examples/sec; 0.175 sec/batch)
2016-10-15 03:39:54.692384: step 42210, loss = 0.66 (737.6 examples/sec; 0.174 sec/batch)
2016-10-15 03:39:56.461172: step 42220, loss = 0.66 (724.8 examples/sec; 0.177 sec/batch)
2016-10-15 03:39:58.226831: step 42230, loss = 0.79 (710.1 examples/sec; 0.180 sec/batch)
2016-10-15 03:39:59.991042: step 42240, loss = 0.70 (737.8 examples/sec; 0.173 sec/batch)
2016-10-15 03:40:01.747674: step 42250, loss = 0.62 (726.5 examples/sec; 0.176 sec/batch)
2016-10-15 03:40:03.527395: step 42260, loss = 0.69 (702.3 examples/sec; 0.182 sec/batch)
2016-10-15 03:40:05.296883: step 42270, loss = 0.63 (729.6 examples/sec; 0.175 sec/batch)
2016-10-15 03:40:07.059092: step 42280, loss = 0.75 (727.9 examples/sec; 0.176 sec/batch)
2016-10-15 03:40:08.824531: step 42290, loss = 0.78 (728.4 examples/sec; 0.176 sec/batch)
2016-10-15 03:40:10.590410: step 42300, loss = 0.80 (731.0 examples/sec; 0.175 sec/batch)
2016-10-15 03:40:12.543473: step 42310, loss = 0.74 (712.7 examples/sec; 0.180 sec/batch)
2016-10-15 03:40:14.310735: step 42320, loss = 0.84 (689.2 examples/sec; 0.186 sec/batch)
2016-10-15 03:40:16.055885: step 42330, loss = 0.75 (736.6 examples/sec; 0.174 sec/batch)
2016-10-15 03:40:17.816440: step 42340, loss = 0.72 (732.3 examples/sec; 0.175 sec/batch)
2016-10-15 03:40:19.576589: step 42350, loss = 0.72 (719.1 examples/sec; 0.178 sec/batch)
2016-10-15 03:40:21.338986: step 42360, loss = 0.79 (717.9 examples/sec; 0.178 sec/batch)
2016-10-15 03:40:23.106676: step 42370, loss = 0.68 (710.3 examples/sec; 0.180 sec/batch)
2016-10-15 03:40:24.859288: step 42380, loss = 0.66 (725.9 examples/sec; 0.176 sec/batch)
2016-10-15 03:40:26.621984: step 42390, loss = 1.03 (729.3 examples/sec; 0.176 sec/batch)
2016-10-15 03:40:28.392403: step 42400, loss = 0.79 (706.6 examples/sec; 0.181 sec/batch)
2016-10-15 03:40:30.335135: step 42410, loss = 0.75 (723.9 examples/sec; 0.177 sec/batch)
2016-10-15 03:40:32.105236: step 42420, loss = 0.85 (722.4 examples/sec; 0.177 sec/batch)
2016-10-15 03:40:33.878990: step 42430, loss = 0.86 (763.5 examples/sec; 0.168 sec/batch)
2016-10-15 03:40:35.638159: step 42440, loss = 0.84 (734.1 examples/sec; 0.174 sec/batch)
2016-10-15 03:40:37.399143: step 42450, loss = 0.76 (738.3 examples/sec; 0.173 sec/batch)
2016-10-15 03:40:39.154900: step 42460, loss = 0.69 (735.5 examples/sec; 0.174 sec/batch)
2016-10-15 03:40:40.915586: step 42470, loss = 0.58 (741.6 examples/sec; 0.173 sec/batch)
2016-10-15 03:40:42.668851: step 42480, loss = 0.63 (745.3 examples/sec; 0.172 sec/batch)
2016-10-15 03:40:44.428978: step 42490, loss = 0.83 (720.1 examples/sec; 0.178 sec/batch)
2016-10-15 03:40:46.184823: step 42500, loss = 0.67 (747.3 examples/sec; 0.171 sec/batch)
2016-10-15 03:40:48.120061: step 42510, loss = 0.70 (730.8 examples/sec; 0.175 sec/batch)
2016-10-15 03:40:49.885553: step 42520, loss = 0.76 (713.4 examples/sec; 0.179 sec/batch)
2016-10-15 03:40:51.648322: step 42530, loss = 0.73 (737.8 examples/sec; 0.173 sec/batch)
2016-10-15 03:40:53.416321: step 42540, loss = 0.88 (725.2 examples/sec; 0.176 sec/batch)
2016-10-15 03:40:55.171816: step 42550, loss = 0.78 (743.5 examples/sec; 0.172 sec/batch)
2016-10-15 03:40:56.932108: step 42560, loss = 0.70 (740.8 examples/sec; 0.173 sec/batch)
2016-10-15 03:40:58.696408: step 42570, loss = 0.71 (749.1 examples/sec; 0.171 sec/batch)
2016-10-15 03:41:00.455836: step 42580, loss = 0.66 (752.9 examples/sec; 0.170 sec/batch)
2016-10-15 03:41:02.223270: step 42590, loss = 0.77 (703.4 examples/sec; 0.182 sec/batch)
2016-10-15 03:41:03.986730: step 42600, loss = 0.77 (705.9 examples/sec; 0.181 sec/batch)
2016-10-15 03:41:05.936562: step 42610, loss = 0.80 (749.1 examples/sec; 0.171 sec/batch)
2016-10-15 03:41:07.703813: step 42620, loss = 0.82 (728.9 examples/sec; 0.176 sec/batch)
2016-10-15 03:41:09.467407: step 42630, loss = 0.82 (726.8 examples/sec; 0.176 sec/batch)
2016-10-15 03:41:11.230604: step 42640, loss = 0.65 (732.4 examples/sec; 0.175 sec/batch)
2016-10-15 03:41:12.995273: step 42650, loss = 0.89 (719.5 examples/sec; 0.178 sec/batch)
2016-10-15 03:41:14.757759: step 42660, loss = 0.82 (726.1 examples/sec; 0.176 sec/batch)
2016-10-15 03:41:16.515037: step 42670, loss = 0.71 (737.7 examples/sec; 0.174 sec/batch)
2016-10-15 03:41:18.271861: step 42680, loss = 0.69 (723.2 examples/sec; 0.177 sec/batch)
2016-10-15 03:41:20.036923: step 42690, loss = 0.65 (691.9 examples/sec; 0.185 sec/batch)
2016-10-15 03:41:21.800283: step 42700, loss = 0.79 (725.5 examples/sec; 0.176 sec/batch)
2016-10-15 03:41:23.748981: step 42710, loss = 0.74 (734.9 examples/sec; 0.174 sec/batch)
2016-10-15 03:41:25.510757: step 42720, loss = 0.72 (735.9 examples/sec; 0.174 sec/batch)
2016-10-15 03:41:27.278130: step 42730, loss = 0.86 (736.2 examples/sec; 0.174 sec/batch)
2016-10-15 03:41:29.037424: step 42740, loss = 0.74 (714.3 examples/sec; 0.179 sec/batch)
2016-10-15 03:41:30.794000: step 42750, loss = 0.77 (751.2 examples/sec; 0.170 sec/batch)
2016-10-15 03:41:32.563247: step 42760, loss = 0.93 (729.8 examples/sec; 0.175 sec/batch)
2016-10-15 03:41:34.328642: step 42770, loss = 0.81 (695.1 examples/sec; 0.184 sec/batch)
2016-10-15 03:41:36.083096: step 42780, loss = 0.74 (716.1 examples/sec; 0.179 sec/batch)
2016-10-15 03:41:37.846215: step 42790, loss = 0.73 (727.8 examples/sec; 0.176 sec/batch)
2016-10-15 03:41:39.608665: step 42800, loss = 0.70 (750.8 examples/sec; 0.170 sec/batch)
2016-10-15 03:41:41.562282: step 42810, loss = 0.69 (736.3 examples/sec; 0.174 sec/batch)
2016-10-15 03:41:43.335003: step 42820, loss = 0.72 (712.2 examples/sec; 0.180 sec/batch)
2016-10-15 03:41:45.105347: step 42830, loss = 0.92 (712.4 examples/sec; 0.180 sec/batch)
2016-10-15 03:41:46.876131: step 42840, loss = 0.68 (705.3 examples/sec; 0.181 sec/batch)
2016-10-15 03:41:48.635866: step 42850, loss = 0.64 (731.5 examples/sec; 0.175 sec/batch)
2016-10-15 03:41:50.405278: step 42860, loss = 0.69 (706.8 examples/sec; 0.181 sec/batch)
2016-10-15 03:41:52.164184: step 42870, loss = 0.76 (719.2 examples/sec; 0.178 sec/batch)
2016-10-15 03:41:53.922086: step 42880, loss = 0.80 (742.8 examples/sec; 0.172 sec/batch)
2016-10-15 03:41:55.691618: step 42890, loss = 0.74 (717.2 examples/sec; 0.178 sec/batch)
2016-10-15 03:41:57.455615: step 42900, loss = 0.79 (723.4 examples/sec; 0.177 sec/batch)
2016-10-15 03:41:59.396902: step 42910, loss = 0.68 (747.1 examples/sec; 0.171 sec/batch)
2016-10-15 03:42:01.164142: step 42920, loss = 0.87 (723.6 examples/sec; 0.177 sec/batch)
2016-10-15 03:42:02.925600: step 42930, loss = 0.80 (723.7 examples/sec; 0.177 sec/batch)
2016-10-15 03:42:04.685665: step 42940, loss = 0.85 (744.0 examples/sec; 0.172 sec/batch)
2016-10-15 03:42:06.444893: step 42950, loss = 0.80 (740.6 examples/sec; 0.173 sec/batch)
2016-10-15 03:42:08.208881: step 42960, loss = 1.02 (715.2 examples/sec; 0.179 sec/batch)
2016-10-15 03:42:09.974054: step 42970, loss = 0.78 (737.1 examples/sec; 0.174 sec/batch)
2016-10-15 03:42:11.739637: step 42980, loss = 0.85 (717.2 examples/sec; 0.178 sec/batch)
2016-10-15 03:42:13.507991: step 42990, loss = 0.61 (705.1 examples/sec; 0.182 sec/batch)
2016-10-15 03:42:15.263866: step 43000, loss = 0.77 (708.2 examples/sec; 0.181 sec/batch)
2016-10-15 03:42:17.781001: step 43010, loss = 0.74 (739.4 examples/sec; 0.173 sec/batch)
2016-10-15 03:42:19.544873: step 43020, loss = 0.76 (738.8 examples/sec; 0.173 sec/batch)
2016-10-15 03:42:21.310424: step 43030, loss = 0.64 (710.8 examples/sec; 0.180 sec/batch)
2016-10-15 03:42:23.071104: step 43040, loss = 0.68 (718.4 examples/sec; 0.178 sec/batch)
2016-10-15 03:42:24.836989: step 43050, loss = 0.77 (691.9 examples/sec; 0.185 sec/batch)
2016-10-15 03:42:26.591749: step 43060, loss = 0.70 (732.7 examples/sec; 0.175 sec/batch)
2016-10-15 03:42:28.349975: step 43070, loss = 0.78 (734.8 examples/sec; 0.174 sec/batch)
2016-10-15 03:42:30.111199: step 43080, loss = 0.63 (729.6 examples/sec; 0.175 sec/batch)
2016-10-15 03:42:31.866267: step 43090, loss = 0.99 (732.8 examples/sec; 0.175 sec/batch)
2016-10-15 03:42:33.618844: step 43100, loss = 0.80 (740.2 examples/sec; 0.173 sec/batch)
2016-10-15 03:42:35.569188: step 43110, loss = 0.84 (745.2 examples/sec; 0.172 sec/batch)
2016-10-15 03:42:37.329095: step 43120, loss = 0.66 (724.2 examples/sec; 0.177 sec/batch)
2016-10-15 03:42:39.079444: step 43130, loss = 0.92 (753.9 examples/sec; 0.170 sec/batch)
2016-10-15 03:42:40.846879: step 43140, loss = 0.74 (724.7 examples/sec; 0.177 sec/batch)
2016-10-15 03:42:42.603914: step 43150, loss = 0.82 (731.3 examples/sec; 0.175 sec/batch)
2016-10-15 03:42:44.366004: step 43160, loss = 0.82 (721.3 examples/sec; 0.177 sec/batch)
2016-10-15 03:42:46.118944: step 43170, loss = 0.67 (736.8 examples/sec; 0.174 sec/batch)
2016-10-15 03:42:47.877872: step 43180, loss = 0.81 (722.0 examples/sec; 0.177 sec/batch)
2016-10-15 03:42:49.634482: step 43190, loss = 0.79 (714.7 examples/sec; 0.179 sec/batch)
2016-10-15 03:42:51.393428: step 43200, loss = 0.66 (736.2 examples/sec; 0.174 sec/batch)
2016-10-15 03:42:53.346557: step 43210, loss = 0.73 (748.2 examples/sec; 0.171 sec/batch)
2016-10-15 03:42:55.102526: step 43220, loss = 0.85 (748.2 examples/sec; 0.171 sec/batch)
2016-10-15 03:42:56.876369: step 43230, loss = 0.68 (689.2 examples/sec; 0.186 sec/batch)
2016-10-15 03:42:58.636579: step 43240, loss = 0.78 (718.6 examples/sec; 0.178 sec/batch)
2016-10-15 03:43:00.396388: step 43250, loss = 0.76 (729.9 examples/sec; 0.175 sec/batch)
2016-10-15 03:43:02.151775: step 43260, loss = 0.86 (733.8 examples/sec; 0.174 sec/batch)
2016-10-15 03:43:03.909582: step 43270, loss = 0.74 (721.7 examples/sec; 0.177 sec/batch)
2016-10-15 03:43:05.667632: step 43280, loss = 0.75 (722.3 examples/sec; 0.177 sec/batch)
2016-10-15 03:43:07.437812: step 43290, loss = 0.71 (705.9 examples/sec; 0.181 sec/batch)
2016-10-15 03:43:09.194763: step 43300, loss = 0.72 (736.5 examples/sec; 0.174 sec/batch)
2016-10-15 03:43:11.138873: step 43310, loss = 0.78 (767.7 examples/sec; 0.167 sec/batch)
2016-10-15 03:43:12.898652: step 43320, loss = 0.84 (731.9 examples/sec; 0.175 sec/batch)
2016-10-15 03:43:14.655949: step 43330, loss = 0.84 (776.3 examples/sec; 0.165 sec/batch)